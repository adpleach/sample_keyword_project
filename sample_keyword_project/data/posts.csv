,id,author,url,title,selftext,domain
0,ptfb2d,redditthrowaway0315,https://www.reddit.com/r/dataengineering/comments/ptfb2d/does_airflow_provide_a_functionality_to_remove/,Does Airflow provide a functionality to remove historical data created by previous runs of a DAG?,"Hi friends,  I'm pretty sure this is a common requirement so I figured there must be some elegant resolution.  Scenario: I have a DAG that runs an INSERT query daily for 100 days. Now I modified the code and client requests me to re-run it for the last 60 days.  Somehow I couldn't find a way to do it elegantly. I can ""clear"" previous runs and Airflow will schedule re-runs but the re-runs do NOT remove historical data. It looks like the only way to do it properly is to:  * Pause DAG in Airflow UI * Manually DELETE all data of previous 60 days * Manually run the new script and INSERT data for the past 60 days * Unpause the DAG  However this is error-prone and can be tricky to implement if say I want to re-run the previous 48 hour data for an hourly process (so that I really need to be careful where to cut the data).  What I think could be a nice solution is that we are allowed to script the ""Clear"" DAG run action. If everytime I cleared a run, Airflow can run a script in background (e.g. I can put up a DELETE query using the same datetime paramaters as the INSEERT one uses), that would be very convenient.   Is there a way to do this in Airflow?",self.dataengineering
1,ptf61u,average_ukpf_user,https://www.reddit.com/r/dataengineering/comments/ptf61u/api_issues_as_a_relatively_new_de_calling/,"API issues as a relatively new DE - calling endpoint with query params works fine, looping over pages results in incomplete dataset?","Hello all,  I've been about a DE for around 6 months now and have come across my first big hurdle.  Thought I'd come here because I've spent the weekend, the past few days, and past few nights trying to solve this problem and feel like I'm losing hope.  We're currently trying to create a DB from a REST API.  I'm using Azure and Data Factory to pull down the data.  Most of the endpoints work, one which is giving me trouble has the following behaviour:  * It's a paginated endpoint with no `['next']` keys in the JSON.  There's also no way of knowing which page you're on.  * You have to loop through using `start` and `limit` e.g. `https://website.com/name?start=0&amp;limit=50` will give 50 records (0-49).  I'm looping over this and pulling new records every time by adding to `start` e.g. `https://website.com/name?start=50&amp;limit=50` gives me 50-99.  My number of records goes up, and there are little duplicates.  This would give me all data from the `name` endpoint.  * The JSON includes a `total` key showing number of records.  The pipeline keeps iterating, adding to `start` until there are no new records.  The number of records in my DB is the same as the `total` key.  So I collect all the records, however, if I call the endpoint with a specific query e.g. `https://website.com/name?location=USA`, it gives me results which are not in my DB.  This leads me to believe this is a problem with the API side because, as per the example, my record number is exactly the same as the `total`, and I called the endpoint correctly with 0 bad requests, and it pulls 50 records each time which I have monitored via ADF's output. I have tried sorting from the lowest record ascending and going up, however, this functionality does not work and the way the API returns records when looping appears honestly random.  That being said, I'm not really sure what I can do apart from speaking to the people who have made the API.  My manager seems to think I'm doing something wrong which is totally okay, I'm just not sure what.  I really want to make sure I can't do anything else before speaking to the API people.  If there are any suggestions, that'd be great. Thanks all.",self.dataengineering
2,ptdg0g,diegoelmestre,https://www.reddit.com/r/dataengineering/comments/ptdg0g/improvements_to_my_first_de_project_streaming/,Improvements to my first DE (?) project (Streaming Datawarehouse). What's next?,"Hi guys, hope you all are well.  I started my career in DE a little bit by accident. Despite having a strong SQL knowledge from previous jobs and having studied DW during my master's I never had thought about DE. But mostly I was a backend/database developer.  Fast forward to 15 months ago, I was challenged to build a real-time (or near-realtime) analytics model for my company, which I gladly accepted. Honestly, the project was a success, it was delivered what was expected by the management but some things concern me about if it was the right decision.     Finished the project, it is in maintenance mode, now and then there are new requirements but nothing new. Because of that, I decided to apply to Data Engineer jobs where I can be around people more experienced in this area than me. The good news is that I'll start a new job in the coming weeks.  The reason I opened this thread is to show you how I designed the architecture and ask you where do you think I can improve it. The goal is eventually to learn one or more technologies during the few weeks I have before beginning the new job. I'll mention the points of the architecture of I'm not happy about.  We adopted the Apache Kafka ecosystem and Bigquery for DWH, MongoDB is our operational database. Below is a summary of how the pipeline works and a simplified diagram.     1 - We have a Kafka Connect running that grabs the changes from our MongoDB collections.  2 - The connector stores the data in a Kafka topic  3 - We are also running ksqlDB to make most of the transformations and a few aggregations.   4 - Transformed data is sent to the new Kafka Topic  \-- My biggest uncertainty about all systems starts here.  5 - A BigQuery connector reads data from topics and sends it to BigQuery. The data is stored in a dataset named Staging.  6 - We have another dataset named Reporting which contains the tables in the final format (dimensions, facts, etc). My biggest problem in this was the SCD data where I need to update existing rows (e.g. mark as inactive) and add new ones.  7 - To overcome this problem, we created several Scheduled Queries (one for each entity business group) that are executed every 15 minutes that runs some Procedures that are responsible to manage the SCD data.  My main issue is in the scheduled queries that I created. They running every 15 minutes gave me two headaches. This is far from being real-time, I would be satisfied with a 30 to 60 seconds delay (or am I being optimistic about this) and another problem was because the scripts were always running the costs per execution started to be more than expected.  But in all honesty, for a first project, I'm super happy about the final result. It delivers what I've been asked, but again, I feel it can be improved.  So, if you have a few minutes, I would appreciate some input on how I could improve this system. Be free to suggest new techs (Sparks, Airflows, or even other GCP components (dataflow p.e.), I'm willing to learn it  &amp;#x200B;  &amp;#x200B;  https://preview.redd.it/lyap4qggf3p71.png?width=735&amp;format=png&amp;auto=webp&amp;s=fedf64446f22a27f195f172e653c7160a3295da6  Feel free to ask for more information",self.dataengineering
3,ptd508,suddilonesuddi,https://www.reddit.com/r/dataengineering/comments/ptd508/help_copying_huge_table_from_hana_to_data_lake/,Help. Copying huge table from hana to data lake - Azure,"Hi all, I am trying to copy a huge table data (~3.5TB) to data lake. The connection happens through self hosted IR which has four nodes. Now the issue is this table doesn’t have physical partitions or a proper column which I can use for dynamic partition. Currently from a date column I have created date ranges and I am using those ranges in for loop which triggers copy data activity. The batch count is set to 4. The problem with this is date ranges have huge data skew. For each query I am getting ~6.5MB/s as throughput.   Has anyone done such load? If so let me know if you have any suggestions for me. 6.5 MB/s throughput seems very low to me. Can anyone confirm this?.   Few points…. 1. Both source and data lake are in the same region. 2. The data is copied into a csv file.",self.dataengineering
4,ptcnp5,ZookeepergamePure467,https://www.reddit.com/r/dataengineering/comments/ptcnp5/need_help_to_advance_in_career/,Need help to advance in career,"Hello all. I have 1 YOE in data engineering in an service based MNC. I have mostly worked on SQL and spark and have a pretty good knowledge of Azure technologies like Databricks, data factory, data lake storage, etc. I have not done competitive since an year and am rusty on DSA. I am planning to switching jobs. My salary right now is 10 lpa. I need advice on what to study and learn additionally to get a better paying salary as I am planning to switch jobs. Also if someone could advice me about the companies I could apply to for a job who look for these skills and how to prepare for them, then I would be really grateful.",self.dataengineering
5,ptceyv,kristiclimbs,https://www.reddit.com/r/dataengineering/comments/ptceyv/airflow_externaltasksensor_proper_way_to_use_for/,Airflow - ExternalTaskSensor-&gt; proper way to use for subdag task,"Hello,  I'm having difficulty figuring out how to use the ExternalTaskSensor for Airflow. I've asked on stackoverflow [here](https://stackoverflow.com/questions/69213527/using-externaltasksensor-on-a-subdag-task) and the Airflow slack channel but I haven't received a response.       waiting_for_page_tokens = ExternalTaskSensor(         task_id='waiting_for_page_tokens_from_daily_metrics_dashboard',         external_dag_id='fbk.daily_metrics_dashboard',         external_task_id='facebook_load_page_info_and_tokens ',         start_date=datetime(2020, 1, 20),         timeout=3600,     )  I even tried by changing the start\_date to match the DAG's start date (which is the same start date as this task since its in the same DAG) but it keeps saying:   **Poking for fbkdaily\_metrics\_dashboard.facebook\_load\_page\_info\_and\_tokens  on 2021-09-21T09:00:00+00:00**  The above message is looking for the correct date, I checked to see that this date/time matches the most recent successful run, but I think it has to do with the subdag, perhaps I don't have the external\_dag\_id correct?",self.dataengineering
6,ptbw3n,thedemonkey,https://www.reddit.com/r/dataengineering/comments/ptbw3n/whos_responsible_for_sql/,Who's responsible for SQL?,[removed],self.dataengineering
7,ptazl0,The_small_print,https://www.reddit.com/r/dataengineering/comments/ptazl0/corise_learning_platform_dbt_analytics/,Co:Rise Learning Platform &amp; dbt Analytics Engineering Course,"Does anybody have any experience with the Co:Rise learning platform? I've heard of the usual Udacity, edx, etc. but not Co:Rise.  They recently announced an Analytics Engineering course that looks to use dbt for various projects over a few weeks. Cost is $400.  Anybody have insight into the topics covered by the course structure?   Any thoughts on if this would be worth it?  [Here's a link to the course in quesiton](https://corise.com/course/analytics-engineering-with-dbt?utm_source=emilyh)",self.dataengineering
8,pt9mrs,CommissionFar3525,https://www.reddit.com/r/dataengineering/comments/pt9mrs/master_data_mgmt_how_do_you_set_up_organizational/,MASTER DATA MGMT: How do you set up organizational data,"I work for a complex organization with a bunch of legacy systems (Healthcare).  We are trying to tidy up the data pipeline for organizational data by going from several systems : hr, accounting, identity holding their own org (with a ton of systems with dependency on these structures) to a MDm-centric solution with source systerns - Mdm - consuming systems.  Any experience or the with what your source systems should be in such a setup?  How to handle conflicting org in MDM?  How to complement org in MDM due to missing data from source system?  I am thinking of leveraging the org in accounting to build a business org and build from that. Marrying the he leg with it in Mdm and so forth...  Happy to hear thoughts and experiences with this",self.dataengineering
9,pt90kg,ArunMu,https://www.reddit.com/r/dataengineering/comments/pt90kg/clickhouse_and_apache_pinot/,ClickHouse and Apache Pinot,I had posted my findings about both ClickHouse and Apache Pinot at [https://www.reddit.com/r/bigdata/comments/pse4gb/clickhouse\_and\_apache\_pinot/](https://www.reddit.com/r/bigdata/comments/pse4gb/clickhouse_and_apache_pinot/) in the hope of getting more inputs and correcting my false assumptions.  Posting it here to get more inputs. Thanks!,self.dataengineering
10,pt88mq,vektor888,https://www.reddit.com/r/dataengineering/comments/pt88mq/any_des_working_in_a_company_implementing_the/,Any DEs working in a company implementing the Spotify model?,"Hi,   as the title might suggest, I've been working in a company implementing some flavour of the [Spotify model](https://www.atlassian.com/agile/agile-at-scale/spotify) for 4 months now, and I am finding terribly difficult to adapt and integrate in this company.  &amp;#x200B;  We are 6 DEs in total, and there are two main tribes, each getting 3 DEs. Finally, each tribe has 3 squads, and a Data Engineer is assigned to each squad.  I am finding this way of working quite complicated because:  * There is no possibility to have tech initiatives, since formally we don't have a data team, but 6 squads with 1 DE each * Sharing standards and best practices is almost impossible * Pairing is very difficult because other DEs have different calendars and it's very hard to find a free slot, plus all of them work on completely different activities related to their squad * Product developers in the squad don't seem to care much about what I do * Compared to other devs, DE have their meeting doubled, because we have to follow the product squad meetings and the ""data people"" meetings, where we have to say what we're working on but we have no power to start new tech initiatives  The result of this is that I'm feeling terribly isolated, and working from home doesn't help, because there isn't a team composed by DEs, and I am the only DE in the squad.   &amp;#x200B;  Should I leave this company as soon as possible, or should I see the situation from a different perspective? I am not at the point where I hate this job yet, but let's say I need to understand the benefits of working this way very soon 😅",self.dataengineering
11,pt6lvo,Marksfik,https://www.ververica.com/blog/the-apache-flink-story-at-pinterest-flink-forward-global-2021?utm_campaign=Flink%20Forward%20Global%202021&amp;utm_source=reddit&amp;utm_medium=social&amp;utm_term=subreddits,The Apache Flink Story at Pinterest - Flink Forward Global 2021,,ververica.com
12,pt5lhm,gurmanavfc14,https://www.reddit.com/r/dataengineering/comments/pt5lhm/using_python_for_writing_etl_jobs_enough_for_a/,using python for writing ETL jobs enough for a data engineering job.,[removed],self.dataengineering
13,pt4t5v,Delicious_Attempt_99,https://www.reddit.com/r/dataengineering/comments/pt4t5v/scala_or_python_for_data_engineer/,Scala or Python for Data engineer?,"Hi All,  I’m a scala developer, not to a pro level, but I know scala with spark very well.  I’m looking for a change, most of the companies are expecting python. I’m confused whether to enhance my knowledge in scala like functional programming and be a pro in it or learn python well. I know basics of python as it was part of trainings.  Thanks in Advance :)",self.dataengineering
14,pt4hl8,ai_jobs,https://www.reddit.com/r/dataengineering/comments/pt4hl8/a_first_update_on_our_aimlbig_data_salary_survey/,A first update on our AI/ML/Big Data salary survey,"🎉 We have a little update on our salary survey which we launched roughly [three months ago](https://insights.ai-jobs.net/share-your-salary-and-see-what-everyone-else-is-making-in-the-ai-ml-and-big-data-sphere/) (check out [https://salaries.ai-jobs.net/](https://salaries.ai-jobs.net/) if you haven’t yet) and needless to say we’re still pretty excited about it.  About four weeks after the launch we enabled the [download](https://salaries.ai-jobs.net/download/)   feature on the site so everyone can get the latest dataset in JSON and   CSV format. Furthermore there’s now a weekly sync of these results to a   dedicated [github repo](https://github.com/foorilla/ai-jobs-net-salaries) as well.  As initially announced, but not yet implemented during that time, we built our own [FX data API](https://fxdata.foorilla.com/)   to provide free and public currency data (yes, you can use it as well   if you like!) for the Forex calculations taking place on the dataset in   the **salary\_in\_usd**  column. This is because we allow  people to fill in their annual salary  in their home or actually paid out  currency and then do the work for  you to translate that into its  corresponding USD amount (yearly  average) for better  comparability/reference, with data provided by the [Bank for International Settlements](https://www.bis.org/) (🏦 the bank for the central banks, basically).  Well,  it’s always fascinating how much effort can go into something   seemingly simple like a salary survey (hint: way more than you   anticipated). But still, it looks like it’s worth the effort.  We also put in some more descriptive information on the [download page](https://salaries.ai-jobs.net/download/)   about what each column in the dataset represents or how to interpret   it. Should be pretty straight forward by now, and hopefull very easy to   work with.  Now the plan is to  keep this site up there indefinitely for the  future to collect remote  work salary information year by year on an  ongoing basis. With this in  mind it should be a good reason now to share  this with your colleagues  and friends if you haven’t done so yet. 😉  It’ll be very interesting to see how much data we can gather in the long term, and also keep in mind that all this is in the [public domain](https://creativecommons.org/publicdomain/zero/1.0/)   (though mentioning the data came from us would be nice and also   increases the amount of data available to share). Meaning it’s free to   use by anyone for anything. 🙂  Last but not least: **Many thanks to all of you who filled out the survey form and shared the site with others**. That’s pretty awesome! 💪  *Original article:* [*https://insights.ai-jobs.net/a-first-update-on-our-salary-survey/*](https://insights.ai-jobs.net/a-first-update-on-our-salary-survey/)",self.dataengineering
15,pt3z6s,sweetaskate,https://www.reddit.com/r/dataengineering/comments/pt3z6s/vector_database_for_unstructured_data_processing/,Vector database for unstructured data processing,"Hi guys,  I'm Kate, from the Milvus project.   I'd like to share with you a video talking about the increasing demands and challenges for unstructured data processing and how Milvus aims to solve the problem. Hope you find it useful!  Video URL: [https://www.youtube.com/watch?v=\_x8RTSaiT50&amp;t=927s&amp;ab\_channel=TheApacheFoundation](https://www.youtube.com/watch?v=_x8RTSaiT50&amp;t=927s&amp;ab_channel=TheApacheFoundation)",self.dataengineering
16,pt2k31,growth_man,https://www.reddit.com/r/dataengineering/comments/pt2k31/here_is_a_short_summary_of_the_etl_article_by/,Here is a short summary of the ETL article by AirbyteHQ. Take a look!,[removed],self.dataengineering
17,pt0vhk,pineapplesoda1e-3,https://www.reddit.com/r/dataengineering/comments/pt0vhk/parquet_files/,parquet files,"Hey, i have parquet files where each column is a pixel value of a image,i want to feed my pytorch model,should i directly extract each row as image from parquet file?I wanted to make each row a pickle file which i will feed to model. Im just curios what is fastest way.Thanks.Files are about 1.2gb,its not a memory issue,im just curios for best pratice.",self.dataengineering
18,pt0ggc,incognitoRed23,https://www.reddit.com/r/dataengineering/comments/pt0ggc/how_to_estimate_data_engineer_salary_in_india/,How to estimate data engineer salary in India given such sparse statistics?,"I poorly negotiated my salary while joining and now i feel i deserve more. But how to get a good estimate given the tech stack in working, experience i have, industry I'm working in, etc?  Glassdoor and Ambition Box salary estimates seem poor given that there are very few, if any, data engineer salaries for India... If they do exist, they're for FAANG level organisations. Even then, for Google, Glassdoor says 10lpa average for 1-3 yoe! Ambition Box says 17lpa for 5-8yoe! Sounds unreal to me.  I have 3 yoe with 1 year in big data projects, working at an established fintech.  Please help me find out the salary figure i deserve given the current market?",self.dataengineering
19,psz5mx,american-roast,https://www.reddit.com/r/dataengineering/comments/psz5mx/should_i_push_the_issue_of_getting_my_title/,Should I push the issue of getting my title changed?,"Hey guys,  I’ve been in a data engineering role for just about a year now; however, my official title according to HR reads something else (totally non-data related).  My boss told me to just put “Data Engineer” on my email signature and such, and we discussed making these titles “official” for the whole team.  I don’t care about the title to stroke my ego, but I do care if a year from now I take a job with another company (where I will have “data engineer” listed on my resume and LinkedIn), and they call my current company’s HR department who just goes “yeah, he worked here as a $UNRELATED_TITLE.”  Is this a valid concern?  We discussed the title change a few months ago and I don’t know if it would be appropriate to push the issue or how I could do so without raising flags.",self.dataengineering
20,psw2ux,clqi,https://www.reddit.com/r/dataengineering/comments/psw2ux/any_resourcessites_with_sample_projects/,Any resources/sites with sample projects specifically for Data Engineering?,"For a little more context, I'm working on my first data engineering project of creating an ETL pipeline in Python. And I am reaching intermediate level of coding in Python. Although it's running smoothly, I was wondering where I can find other projects similar to the scope I am at to make the code more efficient.  So anywhere I can find sample Python codes that include the libraries, functions, etc. used? Or if you have any tips you'd like to just comment would also really be greatly appreciated! Thanks in advance.",self.dataengineering
21,psv1wj,Vorskl,https://www.reddit.com/r/dataengineering/comments/psv1wj/any_opinions_on_akretzs_learndataengineeringcom/,Any opinions on A.Kretz's learndataengineering.com ?,"Hi,  please share your opinion (especially if you taken a course on [https://learndataengineering.com/](https://learndataengineering.com/)   The courses look interesting, but I have some reservations that for example, Spark is covered in only 3 hrs ([https://learndataengineering.com/p/learning-apache-spark-fundamentals](https://learndataengineering.com/p/learning-apache-spark-fundamentals))",self.dataengineering
22,pssfyi,sb2nov,https://www.reddit.com/r/dataengineering/comments/pssfyi/analytics_engineering_with_dbt_course_looking_for/,Analytics engineering with dbt Course - Looking for feedback,"I’m Sourabh, I was a data infrastructure lead at Coursera and then lead one of the core Tensorflow teams at Google Brain. Emily Hawkins, Data Engineering Lead at Drizly/Uber and I are leading a live, cohort based course on dbt starting November 15th. [https://corise.com/course/analytics-engineering-with-dbt](https://corise.com/course/analytics-engineering-with-dbt).   We wanted to share what we’ve learned in data engineering over the years. You can join the first run of the course (capped at about 30 students) below. If you’re open to giving feedback on the class on how we can do better, happy to give a discount.",self.dataengineering
23,pss6oo,nfrankel,https://developers.redhat.com/articles/2021/09/21/distributed-transaction-patterns-microservices-compared,Distributed transaction patterns for microservices compared,,developers.redhat.com
24,pss5zf,antonhp,https://www.reddit.com/r/dataengineering/comments/pss5zf/switching_from_fulltime_de_to_consulting_worth_it/,Switching from full-time DE to consulting. Worth it?,"   Hi everyone,  I've been at my current company for a while, a few years as BI, and the last 2 as DE. I'm in Greater Toronto Area.  With all the current DE hype, I've landed an interview with Deloitte, had a technical and a case study interviews with them. They went pretty well, and I'm guessing they will go ahead with an offer. I'm also guessing they will offer me about 20% more than what I'm currently making. Other benefits look similar.  Does anyone have experience of making a similar switch? Is it worth it for a 20% raise? I've heard some badmouthing about consulting, and I definitely don't want to work long hours.  Thanks in advance!",self.dataengineering
25,psryjw,Assi6,https://www.reddit.com/r/dataengineering/comments/psryjw/tips_and_useful_websitecourses_to_get_into_data/,Tips and useful website/courses to get into data engineering?,"Hey guys,  I've been working as a data scientist for the past 2 years, but recently I got a job as a data engineer. My SQL is on a good level, however my python is not that much good. I know a bit of Airflow but zero knowledge about cloud things (AWS for example).  I started doing the data engineering path on datacamp and decided to try awesomedataengineering.com for more info.  However, I'm looking for some more tips that you all would share with me :) I dont mine doing other courses as well!",self.dataengineering
26,psrpww,potaTO_9844,https://www.reddit.com/r/dataengineering/comments/psrpww/switching_from_fulltime_de_to_consultant_worth_it/,Switching from full-time DE to consultant. Worth it?,"Hi everyone,  I've been at my current company for a while, a few years as BI, and last 2 as DE. I'm in Greater Toronto Area.  With all the current DE hype, I've landed an interview with Deloitte, had a technical and a case study interviews with them. They went pretty well, and I'm guessing they will go ahead with an offer. I'm also guessing they will offer me about 20% more than what I'm currently making. Other benefits look similar.  Does anyone have experience of making a similar switch? Is it worth it for a 20% raise? I've heard some badmouthing about consulting, and I definitely don't want to work long hours.  Thanks in advance!",self.dataengineering
27,psro4u,Reasonable_Banana297,https://www.reddit.com/r/dataengineering/comments/psro4u/facebook_amazon_de_onsite_rounds_preparations/,Facebook Amazon DE Onsite rounds preparations,Any specific resources for  Data Modeling System Design Data warehouse Design for DE Onsite rounds ?,self.dataengineering
28,psntnd,honorchan1,https://www.reddit.com/r/dataengineering/comments/psntnd/tutorial_monitoring_airflow_with_prometheus/,"Tutorial: Monitoring Airflow with Prometheus, StatsD and Grafana","Sharing a post that [Databand.ai](https://Databand.ai) Senior Software Engineer, Vladimir Rozhkov (Vova), wrote a while ago that our data engineer community continues to enjoy. In this post, Vova guides you through how to build an open-source Airflow monitoring solution that helps you visualize Airflow cluster metrics. If you've ever wanted to configure an [**open source data observability dashboard**](https://databand.ai/blog/everyday-data-engineering-monitoring-airflow-with-prometheus-statsd-and-grafana/?utm_source=forum&amp;utm_medium=r&amp;utm_group=de), we think you'll find this useful.  Don't take our word for it though. Have a look and let us know what you think. :)",self.dataengineering
29,psnohg,PaulSandwich,https://www.reddit.com/r/dataengineering/comments/psnohg/when_taking_a_table_through_the_bronzesilvergold/,"When taking a table through the Bronze/Silver/Gold (raw/stage/curated) model, does Silver validation, mapping, cleanup, etc. all result in a single file?","Obviously there are usecase where one file can yield multiple grains of output; I'm not talking about that.      I'm curious if a raw csv file is written to silver with invalid records removed, and then that file is replaced with one with proper column naming conventions, and then that file is replaced with one enriched with dimension data, so that there is one singular `table_cleaned.csv` at the end of it all.       Or is Silver typically several files: `table_validated.csv`, `table_mapped.csv`, `table_enriched.csv`",self.dataengineering
30,psmw4l,JY-DataMechanics,https://www.datamechanics.co/blog-post/how-the-united-nations-modernized-their-maritime-traffic-data-exploration-while-cutting-costs-by-70-percent,How the United Nations Modernized their Maritime Traffic Data Exploration while cutting costs by 70% - Data Mechanics Blog,,datamechanics.co
31,psmjme,PlentyOfFreeTime,https://www.reddit.com/r/dataengineering/comments/psmjme/traditional_dwhetl_developer_to_data_engineer/,Traditional DWH/ETL Developer to Data Engineer,"Dear all,     Title says it all pretty much. I am looking into diving in the Data Engineering world and I would like to know if you could share some kind of learning path.     I am really good at Data Warehouse concepts, and how it all words (in a traditional sense) and also have experience with SQL (PL-SQL and T-SQL too), various ETL tools such as ODI, Talend, DataStage and Informatica. I know a little bit of Python and am currently viewing some courses in order to learn more about it.      I do not have any preference regarding tech Stack (except for Python maybe) but I am in desperate need to have some kind of path on what to learn in order to become a Data Engineer.",self.dataengineering
32,pskxx5,mdl003,https://www.reddit.com/r/dataengineering/comments/pskxx5/best_tools_for_metadata_capture_indexing_for/,Best tools for metadata capture + indexing for delta/parquet tables?,"What tools are out there that can automate column value testing, metadata capture, and indexing at scale for columns in delta/parquet tables? I've checked out [Amundsen.io](https://Amundsen.io) and dbt but doesn't look like either fits our use case very well on first read. Details of our team's current ETL process and our homegrown metadata capture/indexing solution below if that helps spark discussion.  Our team manages data pipelines for around 200 applications. Raw data is passed by the application to various Kafka topics where it's streamed into s3 buckets. From there we transform the raw data and insert it into delta tables using Databricks jobs orchestrated by Airflow. Due to the size of our data lake (10+PB, around 5-10TB/day) and lack of a common schema the transformations we perform on the raw data are usually limited to extracting high-use keys to separate columns and leaving the rest of the data as is (typically MapType() cols).  If you're wondering why we don't enforce a CIM on ingest...we're working on it but unlikely we'd be able to enforce it anytime soon due to internal politics.  At any rate, in lieu of a common information model/schema, we have created a process that flattens out a stratified sample of each batch of data that goes into each table and does a combination of regex testing + join matching on internal data sources to test for things like ip addresses, hostnames, and userids. We tag the columns with a high match rate for a particular test and use that as a criteria to pass all unique values for each batch into an index.  This process does give us some insight as to what types of data are contained in most important columns as well as making partitioned searches a lot quicker (we can search the index for matches and only scan relevant partitions), however I'm thinking there's gotta be a tool out there that's going to do what we're doing more efficiently.",self.dataengineering
33,pskt02,insanetech_,https://www.lunaticai.com/2021/09/data-engineering-on-azure-manning-pdf.html,Data Engineering on Azure,,lunaticai.com
34,pskh7b,shittyfuckdick,https://www.reddit.com/r/dataengineering/comments/pskh7b/is_freelance_data_engineering_a_thing/,Is Freelance Data Engineering a Thing?,I really want to escape the 9-5 jobs and start my own company. I'm not really sure how to leverage my skills as a DE to do that.  My idea might be to create my own website advertising my skills and charging some smaller companies a monthly fee to build data pipelines for them. The idea being these companies may not want to pay for a full time DE when they only need some simple pipelines built and be done with it.  Would this be a feasible/profitable business? Anyone have any similar experience?,self.dataengineering
35,psiwv4,Practical-East1161,https://www.reddit.com/r/dataengineering/comments/psiwv4/looking_for_use_cases_to_use_ai_to_build_de/,Looking for use cases to use AI to build DE pipelines,"Hi All, I am looking for some use cases to improve / optimise data pipelines using AI. Now this is a very big topic. I am looking for some advice from where I can begin my research. Any suggestion will be helpful. Thanks",self.dataengineering
36,psh63n,pineapplesoda1e-3,https://www.reddit.com/r/dataengineering/comments/psh63n/parquet_files/,Parquet files,What is the fastest way to read parquet file form S3 Bucket using Python?Should i stream file?Should i just use pandas?,self.dataengineering
37,psgi1i,SQLPipe,https://sqlpipe.com/getting-started,"Check out SQLPipe, my project to make ETL easier! It is a tool that transfers the result of a query from one database to another. It is still a prototype, so constructive feedback and bug reports are welcome.",,sqlpipe.com
38,psfqgv,onechamp27,https://www.reddit.com/r/dataengineering/comments/psfqgv/technical_python_and_sql_test_graduate_i_must/,Technical Python and SQL test [graduate] I must complete by today. Any advice?,"Hey guys, I've applied to a Data engineering graduate training program.  &amp;#x200B;  I'm through to the final stage, with one more final hurdle of completing a technical task consisting of Python and SQL (2 hours). I'm being assessed on my accuracy and efficiency.  &amp;#x200B;  Coming from a physics background I don't have much experience in data structures and algorithms.   Could I have some advice before I start the task?  &amp;#x200B;  Very much appreciated guys &lt;3",self.dataengineering
39,pscnk0,raghukveer,https://www.reddit.com/r/dataengineering/comments/pscnk0/university_of_helsinki_is_providing_big_data/,University of Helsinki is providing Big Data Platforms Mooc (https://big-data-platforms-21.mooc.fi/),"Main topics are:  * distributed computing, * Warehouse-Scale Computers, * fault tolerance in distributed systems, * distributed file systems, * distributed batch processing with the MapReduce and the Apache Spark (PySpark) computing frameworks, and * distributed cloud-based databases.  The course material will consist of lecture materials and exercises provided by the lecturer.",self.dataengineering
40,pscgtm,RayisImayev,https://datanrg.blogspot.com/2021/09/lambda-architecture-in-data-systems-and.html,Blog post: Lambda Architecture in data systems and possible meaning of this name,,datanrg.blogspot.com
41,psbm6b,Ok-Message1053,https://www.reddit.com/r/dataengineering/comments/psbm6b/help_with_dataformgh_issue/,help with dataform/GH issue,"I'm running dataform in GH workflows and suddenly run into an error:  'FATAL ERROR: Ineffective mark-compacts near heap limit Allocation failed - JavaScript heap out of memory'.  I've read that this error has to do with node so I should use --max-old-space-size to increase the memory limit but that doesn't seem to do anything. The issue only occurs inside GH workflows though, it runs fine locally, so I'm not sure if it's actually a node problem or if that error could also mean its a GH thing? I don't know what's changed since yesterday though - it's been working fine up until today and I've changed nothing to my repo since then. Any ideas? Thanks",self.dataengineering
42,ps90r5,CapitalistZ,https://www.reddit.com/r/dataengineering/comments/ps90r5/extraction_time_rest_api_aws_lambda/,Extraction Time - REST API + AWS Lambda,"Hi, I have built a Python script that grabs the data from our web-based CRM via API request. Currently it requests all the data and takes about 20 minutes to complete.  Is 20 minutes a long time? The reason I ask is I was considering using AWS Lambda to host the script but that only allows for 15 minutes of use.",self.dataengineering
43,ps7sto,First-Ingenuity-1929,https://www.reddit.com/r/dataengineering/comments/ps7sto/1_tb_a_month_through_neo4j/,1 tb a month through Neo4j?,Does anyone fellows has managed this or more?,self.dataengineering
44,ps702d,third_dude,https://www.reddit.com/r/dataengineering/comments/ps702d/should_you_flatten_json_from_apis_as_you_are/,Should you flatten json from apis as you are loading it into staging tables or afterwards with dbt?,Some of our apis have a lot of nesting with json arrays. I have noticed that some singer targets allow you to de-nest json even when its in an array by making several tables in the case of arrays or underscore separated names in the case of dictionaries.   If we are loading api results into snowflake for instance or bigquery is it better to just dump the thing in that nested structure and dbt everything out of it later? Or to de-nest it?,self.dataengineering
45,ps5hi3,enginerd298,https://www.reddit.com/r/dataengineering/comments/ps5hi3/jupyterhub_for_teams/,JupyterHub for teams,Has anyone used JupyterHub to manage teams? What tips do you have to manage kernels/dependencies/config files for users?,self.dataengineering
46,ps4wzd,jemccarty,https://www.reddit.com/r/dataengineering/comments/ps4wzd/derisk_your_data_to_accelerate_your_cloud_journey/,De-Risk Your Data to Accelerate Your Cloud Journey,[removed],self.dataengineering
47,ps4tga,fearthemonstar,https://www.reddit.com/r/dataengineering/comments/ps4tga/derisk_your_data_to_accelerate_your_cloud_journey/,De-Risk Your Data to Accelerate Your Cloud Journey,"Hey there, sorta shameless self-promotion here. I work for Google and frequent/contribute to the DE subreddit often, I love the community so interested in your feedback (good and bad) on my 3 part blog series I just wrapped up. This would be specifically for those in large regulated industries (financial services, health care, stuff like that). If this is you, would love a follow on Medium or feedback here.  Or if you just want to talk data engineering, Google, or whatever, that works too.  [Part 1: How Did We Get Here](https://ericmccarty.medium.com/de-risk-your-data-to-accelerate-your-cloud-journey-part-1-how-did-we-get-here-68eeab52762c)  [Part 2: Design and Potential Pitfalls](https://ericmccarty.medium.com/de-risk-your-data-to-accelerate-your-cloud-journey-part-2-design-and-potential-pitfalls-2538592fc281)  [Part 3: Turning Design into Reality](https://ericmccarty.medium.com/de-risk-your-data-to-accelerate-your-cloud-journey-part-3-turning-design-into-reality-363fd6c21e41)  If you only have time for one: [Bonus: Intro to Data Strategies for Highly-Regulated and Risk-Averse Companies](https://ericmccarty.medium.com/intro-to-data-strategies-in-highly-regulated-and-risk-adverse-companies-3cd6a632f2fe)",self.dataengineering
48,ps33uv,the_travelo_,https://www.reddit.com/r/dataengineering/comments/ps33uv/cicd_architecture_for_infrastructure_etl_jobs/,CI/CD architecture for Infrastructure &amp; ETL jobs,"Is there any framework that allows for infrastructure tests?  I have to create multiple Glue ETL jobs using CloudFormation. I want to run 2 set of tests:  1. That the jobs and associated resources are correctly created.  2. That my ETL pipeline produces the correct results.  I'm setting up my CI/CD pipeline for this but I've got some questions about what the process might look like  I have three environments, Dev/Test/Prod so my questions are:  1. Where do developers test their infra code? Do they have a sandbox environment linked to their feature branch?  2. How to do unit tests on the CF stacks  3. How are Glue Jobs tested? All I do today are unit tests using Pytest but I don't know how to test and end to end pipeline when a new data source is ingested.  4. How can I include Data Quality tests?  I'm using AWS products (CodeCommit, Code  pipeline, etc)  Any resources or pointers are appreciated",self.dataengineering
49,ps2o0u,tmanipra,https://www.reddit.com/r/dataengineering/comments/ps2o0u/python_codility/,Python Codility,"Hey Ppl,  Please suggest me in prepping for Python Codility test in next two days. I have basic programming knowledge on Python and I'm zero in DSA. Will it be harder to crack?  Appreciate your responses. Thanks",self.dataengineering
50,ps0p9j,TheMightySilverback,https://www.reddit.com/r/dataengineering/comments/ps0p9j/interview_tomorrow_for_de_position/,Interview Tomorrow for DE position,"Hi y'all,  I've got my first ever DE interview tomorrow and there will be a SQL Test. And I want to know, how it would be best to prepare. They say there are only a few questions bc it's only 30 mins. What do you all think?",self.dataengineering
51,ps0h44,scraper01,https://www.reddit.com/r/dataengineering/comments/ps0h44/is_anyone_here_still_using_apache_storm/,Is anyone here still using Apache Storm?,"Framework seems interesting, altought these days there doesen't seem to have a lot going on in the form of videotutorials and general discussion. Is this because of it's age? Wondering if anyone here prefers Storm above other stream frameworks like Flink and why.",self.dataengineering
52,ps008q,w_savage,https://www.reddit.com/r/dataengineering/comments/ps008q/looking_for_advice_on_what_i_should_learn/,Looking for advice on what I should learn.,"I'm slowly transitioning to a full time DE, currently doing half data engineering and Analyst work. I'm just about done building my first pipeline at my company. This has made me realize I have a lot to learn still haha!  I'm decent in python and still a beginner in AWS. What other topics should I look into learning? I'm not really sure where to start. Terms I hear thrown around a lot are ""streaming"", ""batches"", etc. Anyone know a good guide to get a handle on common DE terms/best practices/use cases. My background is not in computer science, I'm completely self taught.  TIA.",self.dataengineering
53,pryp6w,KimStacks,https://www.reddit.com/r/dataengineering/comments/pryp6w/less_than_1tb_of_data_what_tools_should_i_get/,Less than 1TB of data what tools should I get better at?,"I read https://docs.dask.org/en/latest/spark.html and in the very last line it said and I quote  &gt; you are looking to manage a terabyte or less of tabular CSV or JSON data, then you should forget both Spark and Dask and use Postgres or MongoDB.  I’m a software developer using mostly django and Postgres  And wants to slowly improve my data engineering skills in tandem with my day to day.  So far there’s no chance I hit 1 TB data anytime soon.  My questions are:  Is this 1TB good rule of thumb? If I deal less than 1 TB data then what other tools do I need to be good at?",self.dataengineering
54,prygvc,honorchan1,https://www.reddit.com/r/dataengineering/comments/prygvc/a_data_observability_model_for_data_engineers/,A Data Observability Model for Data Engineers,"Hello friends!   It's exciting to see data observability platforms grow in relevance to data teams. Our team at [Databand.ai](https://Databand.ai)  wrote a blog post a while ago to help teams shop for the right data observability platform. Like all things, one-size doesn't fit all so if your team is exploring options, [**here's a guide to help you make a selection**](https://databand.ai/blog/a-data-observability-model-for-data-engineers/?utm_source=forum&amp;utm_medium=r&amp;utm_group=de).",self.dataengineering
55,pryban,regreddit,https://www.reddit.com/r/dataengineering/comments/pryban/metadata_dictionary_software_for_small_teams/,Metadata Dictionary software for small teams?,"I've installed and semi-configured LinkedIn's Datahub and Lyfts Amundsen, and both were pretty complex to setup, and data discovery and ingestion is a very involved process in Amundsen, and only slightly easier in datahub. Can anyone recommend an alternative for small teams? Amundsen literally requires almost a dedicated team to keep it running and any ingestion of metadata requires deep knowledge of the platform, which is very sparsely documented.   Our main goals are data discovery, lineage, traceability, and documentation. Datahub would be perfect except it leaves a lot of components to the end user, like auth.",self.dataengineering
56,prvlqs,vy52,https://www.vcloudata.in/2021/09/microsoft-azure-data-factory.html,Microsoft Azure Data Factory,,vcloudata.in
57,pru6r1,tfyz,https://cal-data-eng.github.io/,"Data Engineering course - UC Berkeley, Spring 2021",,cal-data-eng.github.io
58,prr2r7,Aaron-SWE,https://www.reddit.com/r/dataengineering/comments/prr2r7/how_likely_is_it_to_land_a_london_based_de_job/,How likely is it to land a London based DE job with 100k+ salary working fully remote?,"I'm seeing a few DE jobs pop up in my emails with big London salaries (fully-remote).  I'm just wondering if anyone has any experience landing these roles?   I'm also curious if DE salaries are likely to change in the coming years.  I'm still after junior positions. But I'm already thinking about where I could be in 5, 10 or 20 years. Ideally I'd eventually be on £100k - but maybe I'm just expecting too much from UK tech salaries...  Although I do like DE, money is still important to me.",self.dataengineering
59,prqrc7,yddadpoj,https://www.reddit.com/r/dataengineering/comments/prqrc7/snowflake_architecturemapping/,Snowflake architecture/mapping,Hello everyone  Hoping I could perhaps get some help with this.  My team is inheriting a datawarehouse on Snowflake - as you can imagine there is not much accompanying documentation. Is there any tool that can scan/download the various schemas and tables to display the architecture/relationships at all?  Just trying to save myself a manual background task as &amp; when I get the time to commit to it.  Many thanks and have a great week.,self.dataengineering
60,prp2nd,Ok-Message1053,https://www.reddit.com/r/dataengineering/comments/prp2nd/how_can_i_write_my_pipeline_to_execute_only/,how can I write my pipeline to execute only certain tasks?,"I currently have a python script, [main.py](https://main.py), as my entry point to my docker container. [Main.py](https://Main.py) combines functions for separate processes, not dependent on each other. Sometimes only one process will fail and I might want to rerun that, but currently I am just reruning the entire [main.py](https://main.py) script. I'm sure there is a better way to design my pipeline so that I can manually run my docker container and input something to say 'only run task x this time'. I'm thinking there's probably a lot of ways to do it...  like setting an env variable when I run my container and run specific scripts based on what that env variable is. I know Airflow exists, but I'm not sure if that would work for me because I am only creating a VM and starting my container on command, so I don't have an environment for Airflow to constantly be running in. Any ideas?  For some additional detail, [main.py](https://main.py) looks something just like this:      def task_1():       try:         function_1()       except:         logger.warning('Exception raised in task 1')          def task_2():       try:         function_2()       except:         logger.warning('Exception raised in task 2')          def task_3():     ...          if __name__ == '__main__':       task_1()       task_2()       task_3()  and this is automated to run every day at the same time. However, in case one task fails, I may want to rerun just task\_2() manually. My first thought is to do something like this:      if __name__ == '__main__':       if environ['TASK'] == 'All':         task_1()         task_2()         task_3()       elif environ['TASK'] == '1':         task_1()       elif environ['TASK'] == '2':         task_2()...  My default TASK variable will be all, but if I need to manually rerun, I can set this to something else. Good idea?? Thanks",self.dataengineering
61,prneam,Kickass_Wizard,https://youtu.be/ZZr9oE4Oa5U,The Six Stages of Data Pipeline Maturity,,youtu.be
62,prlo9q,vananth22,https://www.dataengineeringweekly.com/p/data-engineering-weekly-56,"Benn Stancil's data OS, UCBerkely's data engineering spring 2021 course, Airbnb's Automating Data Protection at Scale, Uber's YAML Generator for Funnel YAML Files, Intuit's A Paved Road for Data Pipelines, Pinterest's Faster Flink adoption with self-service diagnosis tool",,dataengineeringweekly.com
63,prlh97,Kickass_Wizard,https://i.redd.it/zrj5sut5gko71.png,"Big Data Pipelines on AWS, Azure &amp; GCP (associated blog in comments)",,i.redd.it
64,prityf,TheOneWhoDidntCum,https://www.reddit.com/r/dataengineering/comments/prityf/data_lake_engineer_salary_goldman_sachs/,Data lake engineer salary Goldman Sachs,What is the pay like and what are working conditions ?  I’m Ontario if possible but anywhere else is fine as well.,self.dataengineering
65,priirc,TattyRoom,https://www.reddit.com/r/dataengineering/comments/priirc/why_is_the_word_dump_so_prevalent_in_the_data/,"Why is the word ""Dump"" so prevalent in the data industry","I hear it all the time. Considering the superlatives used for the value of data (gold, oil etc) - why do so many insiders refer to it a something that's synonymous with bad smells and watering eyes?",self.dataengineering
66,prh9by,HighlyIllogicall,https://www.reddit.com/r/dataengineering/comments/prh9by/data_engineering_training/,data engineering training,what do you recommend as a training for someone who is interested in expanding their knowledge in data engineering and streaming?,self.dataengineering
67,prfn3w,Zorkol02,https://www.reddit.com/r/dataengineering/comments/prfn3w/de_certifications/,DE certifications,"Hey everyone!  I’ve been looking on the internet for certificafions, beginner DE, and I’ve came across the Cloud Digital Leader Certification. Is this one good for beginners? Or should I just study hard for the GCP Data Engineer one? Since the cloud digital leader focuses on all the products GCP offers and not database and storage.   Also I’ve been thinking of doing data engineer course from datacamp, is that one good or you recommend something else?",self.dataengineering
68,preqiw,m_usamahameed,https://www.reddit.com/r/dataengineering/comments/preqiw/cdc_implementation_in_airflow/,CDC implementation in Airflow.,How can we implement CDC in Airflow using Mysql or Python Operator. :thinking_face:   Can anyone share helping source or thoughts. :smiley:,self.dataengineering
69,prdxfb,if155,https://www.reddit.com/r/dataengineering/comments/prdxfb/do_you_use_math_and_stats_as_a_data_engineer/,Do you use math and stats as a data engineer?,I currently work as a junior software engineer with previous experience as a data analyst. I'm looking to make the transition into data engineering because I enjoy the convergence of the two fields (data analytics and software development). My question is whether math and stats is used in your job like data scientist or ML engineers?,self.dataengineering
70,prdgxd,Relevant_take_2,https://www.reddit.com/r/dataengineering/comments/prdgxd/managing_a_data_warehouse_project_as_a_nonde_and/,"Managing a data warehouse project as a non-DE, and non-PM","TL;DR: Company needs a data warehouse project starting all the way from defining data and systems. I want to grow my career, but have no DE or PM experience. Is this a good fit?  Background:   I work for a smallish company in the process industry. The company has 10-30 computer systems that may or may not be connected in some ways. Think process control, sensors, financial reporting, customer interface, and so on. And I say may or may not be connected, because I don't believe there's any one person in the company who knows exactly what's going on and where. I am not responsible for any of these systems.  So this is the starting point. New leadership understands it's not OK and wants to make changes. Among these (several) changes is implementing a data warehouse to enable automation for processes that happen between the systems. These would be mainly of the reporting type at this point.  My situation:   I'm an analyst with 4 years of programming experience designing analytical code in the field (think Pandas, OOP etc.), although not on the process side. I understand the basics of databases and SQL from working with them. I have very little project management or project planning experience. But I'm wanting to add to my responsibilities and grow my role and visibility within the organization, plus I'm interested to work more with data.  At this moment in the company, I am (on the tech side) the most qualified person interested in being the project manager for the data warehouse development project. And I have a good chance of getting it if I want. This would be a multi-year project/continuation that would start from creating a vision, facilitating teams to make data catalogs of their systems, doing interviews, prioritizing business cases, and then connecting the systems one by one into the warehouse. (The time frame is to ensure planned use of key personnell. Even I would be only  around 10% on this!)   I would, of course, have a budget that I could use to hire actual professionals to build the data warehouse and advise me on any aspects of the project. I would never do anything technical directly, although I would be involved in the design.   You can perhaps see from the size of the endeavor that this is a make-or-break type of situation, as however the warehouse progresses, that is the way I will be seen by much of the management. But if it does go well, there's significant room for career (and CV) growth.  My question:   So, as professionals, can you tell me if I'm headed for a train wreck and should avoid this project at all costs... Or should I embrace the chaos and start reading books? I would appreciate any insight into the decision, or sources to look at for this type of work case.   And if the project sounds infeasible no matter the person, please let me know how it SHOULD be done so I can appear smart when discussing it with my superiors.",self.dataengineering
71,prcz3g,shaon_9941,https://www.reddit.com/r/dataengineering/comments/prcz3g/how_can_i_become_a_self_taught_data_engineer/,How can I become a self taught Data Engineer?,"Hello all,  I want to become a self taught data engineer to get a remote position. For this I have learn Python, SQL, Data Cleaning ,pandas and some Data structure and algorithm.  My question is :  1. What are the must have skills before applying as junior data engineer ? 2. what are the road map to become a self taught data engineer? 3. how can prepare my portfolio to apply for remote DE position?  if some one can help will be very thankful",self.dataengineering
72,pr96sa,vinsanity1603,https://www.reddit.com/r/dataengineering/comments/pr96sa/pyspark_etl_aws_glue_joining_on_nearest_timestamp/,Pyspark ETL (AWS Glue) Joining on Nearest Timestamp,"Hi, I have an ETL job that involves handling two types of json data (one is generated every minute, and one is generated every 5mins). The goal is to join them using the object\_id, and the timestamp. However,  their timestamp are not exactly the same. How can I join the every-5-min data to the nearest timestamp in the every-1-min data using Pyspark.      Has anyone experienced creating an ETL job for the same problem? Any help is appreciated.",self.dataengineering
73,pr9227,nobel-001,https://www.reddit.com/r/dataengineering/comments/pr9227/data_teams_structure/,Data teams structure,"What is the structure of your data team, including data engineers, data scientists, data analysts, ...?  Do you have dedicated team for data engineering or both data scientists and data engineers are working in one team?  and do you have different teams / sub-teams for different business domains?",self.dataengineering
74,pr84te,Successful-Aide3077,https://youtube.com/watch?v=rLF-56tXwC8&amp;feature=share,How to use Function App to automate Data Ingestion Pipeline,,youtube.com
75,pr7gmz,AmDprimer,https://www.reddit.com/r/dataengineering/comments/pr7gmz/machine_learning_or_data_engineering/,Machine Learning or Data Engineering,"I have offers from both Machine learning and Data Engineering role, I have a very common doubt on which one should i choose as they are very new to me, what’s the right path. I completed doing a PG diploma in ML &amp; AI. However, I was not getting offers at that time, so i started with looking out for Data Engineering role. Now I have offers from both the roles and I can’t decide due to no industry experience in these areas. Can anyone help me out here?  Thanks much!",self.dataengineering
76,pr6xg5,onelostsoul115,https://www.reddit.com/r/dataengineering/comments/pr6xg5/python_code_test_interview_general_career_advice/,Python Code Test (Interview) + general career advice,"Hi  This question is 3 pronged:  1. What sort of challenge could I get in a 30 minute Python code test (live)? 2. What is the best way to brush up on my Python? 3. Based on the below info, do you think I should go for the job, or wait a few more months until I have more exposure to the stack?  Info:  I have a 30 minute code test in Python (options were Python or Scala) coming up as part of an interview for a mid / senior data engineering role, and wondered if anyone can give me any tips as to the types of questions I might be asked? This is the second stage of the interview, after a 30 minute phone call with the hiring manager.  The role is basically exactly what I want to do and would allow me to get experience with much larger volumes of data and distributed systems. I'm halfway through reading Designing Data Intensive Applications and it's opened my eyes as to what I really want to be doing. Their tech stack is as follows:  SQL, Python, Clojure, Scala, AWS:Glue, Athena, Redshift, Apache Airflow, Kafka, Spark, Terraform  I'm a bit worried about the Python code test as my day to day and expertise is still mainly in pl/sql and pl/pgsql, so I need to brush up on my Python. I have used Python in the past for some basic things like a small file ingestion project with some basic Pandas, but I rarely use it (this is something I intend to change) and need to go back to basics even of just the syntax to remind myself. I also have exposure to things like Ruby, C#, JS, Java but just to a basic level, and I haven't looked at any of these for years.  I've evolved from \~5 years of hardcore database work (Oracle, Postgres) both OLTP and Warehouses to more of a data engineering role, but currently it is mainly research and establishing principles for future work, and there is no one senior to learn from. My current role offers loads of freedom and flexibility, and the chance to build a team, but I just don't think it will ever get to the technical level I want, I think it will end up more being a research and make PoC applications in various technologies, then assist other teams in using them.  I have also done \~1500 hours of freelance work (built many great relationships and have the highest rating on the main platform I use) and recently have been pushing myself more towards more DE related activities, e.g. file ingestion from S3 via Glue with some very basic spark transformations, very basic lamdbas etc. building a warehouse in Redshift, release automation pipelines, basic Kubernetes, messaging queues, lots of other databases.  I also have a side business with a friend and we want to get into more big scale / DE projects.  Currently for that I have been focussing on the database side of things and he does anything Python related (e.g. we subcontracted on a project for a company that owns many of the worlds largest brands, to build their ETL orchestration layer, he did the Python wrapper that the user interacted with and I created all of the PL/SQL packages to dynamically create the schemas and call all of the ETL procedures (written in PL/SQL).  Basically, I'm an SQL expert but a lot of the other things are things that I am aware of but haven't used. That said, I'm a quick learner and love a challenge, so I want to go for it, I just find myself in a position where I'm unsure of what to expect or how I will do, where as normally when I do an interview I'm very confident I will smash it.  I'm not sure if I should just wait 6 months or a year and learn as much as possible in my current role / more freelancing out with my comfort zone, then look again, or try now.  The other thing to think about I suppose is salary, the recruiter said base salary range was around 50 - 95k DOE, which is obviously huge.  I had said my expectations were around 85-90 pre speaking to him, and now I think that is likely unrealistic based on my experience.  All comments appreciated, thanks.",self.dataengineering
77,pr1yw9,omkarkonnur,https://www.reddit.com/r/dataengineering/comments/pr1yw9/data_modeling_in_rdbms_with_complex_data_types/,Data Modeling in RDBMS with complex data types?,"Most RDBMS support complex data types like XML, JSON, etc. Some support querying external object stores as well.  Although technically, they work, I was curious if anyone has worked on a specific use case where these feature was helpful?  Any pointers on how the data was modeled in the database while using these complex data types?  \*\* I do understand ELT has some advantages, just looking for specific use case with complex data types. i.e Would storing a API response in JSON column be better than parsing the fields and storing them in a table. Does the complex data type solution scale with volume?",self.dataengineering
78,pqypla,MaxGanzII,https://www.reddit.com/r/dataengineering/comments/pqypla/amazon_redshift_research_project_autovacuum_white/,"Amazon Redshift Research Project : Auto-Vacuum (white paper, PDF)","https://www.amazonredshiftresearchproject.org  https://www.amazonredshiftresearchproject.org/white_papers/downloads/auto_vacuum.pdf  Auto-vacuum is a black box and so it is impossible to devise a test method which is known to be fair. Given this limitation, the test method used found that auto-vacuum ran so infrequently as to be of negligible consequence. Of the auto-vacuums which were seen, only delete vacuums were observed; sort vacuums were never observed.",self.dataengineering
79,pquy43,dataEng_questions,https://www.reddit.com/r/dataengineering/comments/pquy43/data_engineering_career_guidance/,Data Engineering Career Guidance,[removed],self.dataengineering
80,pquth8,mcfab8,https://www.reddit.com/r/dataengineering/comments/pquth8/mobile_management_platform/,Mobile Management Platform,"In your opinion, as a data engineer, which third party software is the easiest to integrate with? I want one that acts like a data provider with excellent documentation. No red tape around getting access to raw data. A secure share or direct copy using ODBC would be ideal, but if it's a REST API that's fine too.",self.dataengineering
81,pqty5b,scraper01,https://www.reddit.com/r/dataengineering/comments/pqty5b/stream_processing_tool_to_pair_with_kafka/,Stream processing tool to pair with Kafka,"I've been trying to learn for the last months or so a data engineering stack. So far i've learnt Spark for big data ETL, a bit of Big Query for ELT, Apache Airflow for small data ETL,  and at last Kafka as a message passing queue. What i think i'm missing here is a stream processing tool. Between spark streaming, apache flink and apache storm, what do you think sinergizes the best with Kafka? Any suggestions?",self.dataengineering
82,pqt16n,knlph,https://www.reddit.com/r/dataengineering/comments/pqt16n/at_what_point_do_companies_start_looking_at/,At what point do companies start looking at metadata management?,"Disclaimer: I am fairly new to understanding this space.  I have been reading about the shifts happening in the data ecosystem, how diverse data teams function, this community has been very helpful along the process (thankyou).  My question is on ""metadata management"". I recently read about Gartner's announcement of scrapping the magic quadrant for metadata management and introduced marketing guide for ""metadata management"" (https://towardsdatascience.com/the-gartner-magic-quadrant-for-metadata-management-was-just-scrapped-d84b2543f989).   This brings a big shift in how the companies may start looking at metadata management. A lot of this can be attributed to need of building a strong foundational data culture.  I'd love to hear about any resources/experiences to understand this better. If you can, I'd also love to see what kind of/at what point do companies start thinking about actively managing metadata?",self.dataengineering
83,pqqqjl,Pragyanbo,https://www.theclickreader.com/mlflow-for-machine-learning-pipelines/,MLflow For Machine Learning Pipelines [Ultimate Guide],,theclickreader.com
84,pqo4uk,cieloskyg,https://www.reddit.com/r/dataengineering/comments/pqo4uk/data_structures_internals/,Data structures internals??,As a data engineer are we supposed to know the internals of python data structures ? Have you guys used arrays over list in python?(only relevant for folks with python background but feel free to chime  ),self.dataengineering
85,pql3xk,shubham7stark,https://www.linkedin.com/pulse/data-story-powerplay-part-1-shubham-goyal/,I am co-founder &amp; CTO at Powerplay. I have documented my journey to build data engineering from initial days. This can be super helpful for early stage startups.,,linkedin.com
86,pqjums,saik2363,https://www.dasca.org/newsroom/mit-to-host-first-citizen-data-science-summit-on-september-20#,MIT to Host First Citizen Data Science Summit on September 20 | Register For Free,,dasca.org
87,pqi9pw,Key_Base8254,https://www.reddit.com/r/dataengineering/comments/pqi9pw/how_to_create_olap_fact_table/,How to create olap fact table,"Hello guy, i wanna ask how create olap fact table from this table   presidential_national_toplines_2020.csv   https://github.com/fivethirtyeight/data/tree/master/election-forecasts-2020  I still cant understand about fact table and dimensiOn table,  there is article that said  if fact table contains  primary keys",self.dataengineering
88,pqi47e,throwawayDE99,https://www.reddit.com/r/dataengineering/comments/pqi47e/30_min_python_live_code_test_tips/,30 Min Python Live Code Test - Tips,[removed],self.dataengineering
89,pqh9un,techexpertsforall,https://techbiason.com/programming-languages-for-data-science/,Best Programming Languages for Data Science,,techbiason.com
90,pqeipn,QuaternionHam,https://www.reddit.com/r/dataengineering/comments/pqeipn/standardbest_ways_of_moving_data_between_databases/,Standard/Best ways of moving data between Databases,"What are the standard ways to move data from an OLTP database to an OLAP database in general? (for example, Postgre to Clickhouse).  &amp;#x200B;  We are currently just dumping data from Postgre to a CSV then copying that file using scp to the host where our Clickhouse instance is running (everything on-premises), and finally inserting from that file into our DW. All of this using Airflow as orchestrator.  &amp;#x200B;  The problem with this way is that when a lot of our pipelines run in parallel, we end up having some spikes in our usage of disk space.  &amp;#x200B;  I thought of creating a custom python function that gets data from Postgre (using the correspondent airflow hook) with fetchmany (currently the way that's implemented is fetchall) setting a batch size so that i can iterate through those batchs (for example 10k rows) saving them in a variable (therefore using memory) and then loading them into our DW using ClickHouse Hook.  &amp;#x200B;  The problem with that is that I'm going to increase the load that each worker is going to have (since I'm saving the data in memory). So maybe there is a better way that I'm not aware of, we don't have a mature data team so I'm not being able to consult this issue with a senior engineer.  &amp;#x200B;  Thanks!",self.dataengineering
91,pqb7jl,knlph,https://www.reddit.com/r/dataengineering/comments/pqb7jl/what_is_the_modern_data_stack_at_your_company/,"What is the ""modern data stack"" at your company?","Hi  I am trying to understand this space – the Modern Data Stack. There are so many open-source tools,  and so many new tools entering the market almost across the board.  - How big is your company? - What is it that you currently use? - If possible, how do you evaluate tools? Do you prefer flexibility over ease of operating (in paid solutions)?  Is there anything even missing now in the stack?",self.dataengineering
92,pqanfs,cyrilou242,https://medium.com/@cdecatheu/data-quality-anomaly-detections-that-work-with-thirdeye-8fbce5d2f34a,Data Quality: Anomaly Detections that Work with ThirdEye,,medium.com
93,pqadz5,DrSnakee95,https://www.reddit.com/r/dataengineering/comments/pqadz5/interview_assignment_too_big/,Interview assignment too big ?,"I was given one week to do an assignment as part of an interview and I was wondering if they were just asking me for disguised work.. Here is what I'm supposed to do :  - Extract data from an API  - Clean the data, add KPIs - Explain how I would model the data (with full documentation) - Include testing and error handling - Contenerize the code I have written in a docker containter  This feels a bit overboard doesn't it ?",self.dataengineering
94,pq9f8b,noNSFWcontent,https://www.reddit.com/r/dataengineering/comments/pq9f8b/i_just_had_an_interview_with_a_senior_data/,"I just had an interview with a Senior Data Engineer and the head of the Data Science department of a company and they said, in the next interview they'd see how I think.","We had a good conversation about my work experience which was not quite of a data engineer and my personal experience working with AWS tools such as EC2, Kinesis, Lambda, DynamoDB and S3. Then towards the end of the interview they mentioned, for the next interview they'd like to see how I think.   I don't know why I didn't ask what they mean by that but now that I think about it, I become less and less sure of what it could mean and what I should prepare for.   Honestly, I've never gotten this far in a data engineering interview and don't know what they mean by ""how I think"" .   From what I can think of it could mean, how I approach programming problems and how I approach writing SQL queries.   Any other things it could mean that I should prepare for?",self.dataengineering
95,pq7owa,DataGeek0,https://www.reddit.com/r/dataengineering/comments/pq7owa/how_ups_makes_data_driven_decisions_for_ai/,"How UPS makes Data Driven Decisions for AI Innovation with Laura Patel, Advanced Analytics and Gregory Brown, VP R&amp;D - Thoughts?","Hey guys -  I  found this webinar that looks interesting and were wondering your thoughts? I'm planning to attend. Here's the description from the website:  \------------------  **Featured  Guest Speakers: Gregory Brown, Vice President of  Strategy and R&amp;D,  Advanced Technology Group at UPS and Laura Patel,  Principal Data  Scientist at UPS!**  Global  transportation and logistics leader UPS delivers more than 24  million  packages and documents a day for customers in more than 220  countries  and territories. To provide reliable service with a high level  of  visibility, the company seamlessly collects and crunches petabytes  of  information with sophisticated data management systems and highly   specialized data analysts. In this talk we will share insights from the   company’s deep understanding of how data is collected, engineered, and   analyzed to make scalable, data-driven decisions that support AI for innovation.  **About Data for AI:**  The  Data for AI Community is geared toward innovative companies  pushing  the boundaries of what’s possible with Artificial Intelligence  and  cognitive technologies. This community is focused on the data side  of  AI including: Data Engineering, Data Preparation, Data Labeling &amp;   Annotation, Sourcing and Generating Data, and All Other Topics   Data-Related for AI. Join us at this monthly event for high-quality   content with compelling &amp; informative speakers and opportunities to   network and connect with fellow like-minded individuals. ​  **Agenda:**  * 11:30-12:30pm: Featured Presentation * 12:30-13:00pm: Your Q&amp;A and interaction  \-----------------  Free Registration: [https://events.cognilytica.com/CLNTc3MHwyNA](https://events.cognilytica.com/CLNTc3MHwyNA)",self.dataengineering
96,pq790o,jekapats,https://github.com/cloudquery/cloudquery,CloudQuery: SQL for Cloud Infrastructure,,github.com
97,pq78ss,kristiclimbs,https://www.reddit.com/r/dataengineering/comments/pq78ss/pandas_change_row_to_column_and_extract_tried/,"Pandas change row to column and extract. Tried pivot but see Index contains duplicate entries, cannot reshape","I have the below df:  &amp;#x200B;  |name|values|id| |:-|:-|:-| |page\_fans|{'value': 111, 'end\_time': '2021-09-13T07:00:00+0000'}|247111| |page\_fans|{'value': 233, 'end\_time': '2021-09-14T07:00:00+0000'}|247111| |page\_fans|{'value': 551, 'end\_time': '2021-09-15T07:00:00+0000'}|247111|  &amp;#x200B;  but I'm trying to do this:  &amp;#x200B;  &amp;#x200B;  |page\_fans|end\_time|id| |:-|:-|:-| |111|'2021-09-13T07:00:00+0000'|247111| |233|'2021-09-14T07:00:00+0000'|247111| |551|'2021-09-15T07:00:00+0000'|247111|  &amp;#x200B;  The below is what I've done so far to create / clean up the df:           row =  {'name': 'page_fans', 'period': 'day', 'values': [{'value': 111, 'end_time': '2021-09-13T07:00:00+0000'}, {'value': 233, 'end_time': '2021-09-14T07:00:00+0000'}, {'value': 551, 'end_time': '2021-09-15T07:00:00+0000'}], 'title': 'Lifetime Total Likes', 'description': 'Lifetime: The total number of people who have liked your Page. (Unique Users)', 'id': '247111/insights/page_fans/day'}          pat_id = r'(\d+)'     df = pd.io.json.json_normalize(row)      df['id'] = (df['id'].astype(str).str.extract(pat_id))     df = df.explode('values')   I've tried to use df.transpose() and      pivoted = df.pivot(columns='name').reset_index()  but I see the error:  &amp;#x200B;  &gt;Index contains duplicate entries, cannot reshape",self.dataengineering
98,pq50oj,Aromatic-Pay-3540,https://www.reddit.com/r/dataengineering/comments/pq50oj/what_are_the_innovations_changing_and_shaping/,What are the innovations changing and shaping data engineering right now?,"I'm getting into the field and I'm curious about what innovations are either just on the horizon, or have like recently affected the field. Any answers would be greatly appreciated.",self.dataengineering
99,pq4dma,britishbanana,https://www.reddit.com/r/dataengineering/comments/pq4dma/how_do_you_handle_data_versioning_for/,How do you handle data versioning for reproducibility?,"Hey all - my team is starting to discuss tackling our dataset versioning in a more elegant way than just copying tables to a new location versioned name each time we rebuild our analytical datasets.  We are heavily based on Spark and we only recalculate our datasets a few times a year as new data comes in (lambda architecture), so this versioning strategy has been fine if suboptimal.  We usually don't update / append to datasets often, we just recalculate them when we have new data.   We now have a need to start caching results from analyses on our analytical datasets in order to reduce the amount of some more expensive operations, but in order to reliably read from a cache (by cache I just mean a Delta table on S3) like that the reader needs to be able to identify the version the data is on. Once we start thinking about what makes up a dataset version, things start to seem kind of complicated - for our purposes a particular dataset version is a function of the versions of each input dataset as well as the writer application version but could possibly things like system configuration, operating system, etc.    The initial idea that comes to mind (and which is vaguely described in the Delta Lakehouse paper) is to have a metadata layer on top of the Delta lake.  Delta tables seem to provide some basic functionality for tracking versioning, and I think by leveraging the userMetadata we may be able to achieve some of the more complex versioning relationships I described above.  While I really like that the metadata could be atomically updated with the data, I'm a bit afraid though that the userMetadata may prove a bit too limited to be able to scale with the version complexity.  Alternatively I've thought about augmenting the Delta versioning / metadata with an external metadata store of some kind (another S3 file, or some persistent database), although the lack of transaction control across the metadata store and the data itself would make me sad.  There would be other benefits to having a metadata store though so it's still something to consider.  So my question to you all is, does anyone have suggestions, tips, or recommended tools to help with this problem?  I've seen some tools such as Pachyderm that attempt to address this, but have no idea how well they work in practice - and haven't seen anything built more for a Spark context.  I know it's a very common problem, particularly in regulated industries, so I'd be curious to hear any experiences of things that have worked or didn't work for you in the past.  Thanks!",self.dataengineering
100,pq3f29,International-Life17,https://www.reddit.com/r/dataengineering/comments/pq3f29/help_in_selecting_an_offer_from_a_german_org_from/,Help in selecting an offer from a German org from India for data engineer position,"Originally posted this in blind. I didnt get much response.   Thought I could share here and get some advice from you people. Please provide your suggestions. Highly appreciate it.   Received an offer of 75k euros from a German Organisation for a mid level data engineer role (not senior role). I have 9+ years of total experience. Last 4 years were into data engineering   I asked why I wasn’t given a senior role they mentioned it’s because of my performance in the interview.  Total comp offered  75k euros Relocation bonus 7.5k euros No RSUs since it’s a mid level role.  Current comp in India 36 LPA 32 fixed and 4 LPA as bonus. Which is like 45k € Current title Senior data engineer  The question I have   1. I think if I try a little harder I can get a much better TC in India, been receiving lot of calls lately for ranges from 40 to 50 LPA (60k €) but haven’t cracked any yet. Not sure if it’s worth to move to Germany in terms of savings perspective?  2. Is it easy for an introvert and single guy to survive in Germany without getting depressed and how easy it is to make friends? Btw I really don’t know the culture there got this impression from a bunch of vloggers in YouTube.  3. The people whom I interacted with be it Germans or Indians were very nice and had a positive mindset during the interview. So I guess the culture is great. But not sure of wlb  4. If I have intentions to travel back to India having worked for 2 to 3 years in Germany, how easy it is to get a job in India?  5. Is this an opportunity not to be missed? Meaning will I get jobs like this in future so I can prepare myself better for good TC and roles. Not sure what’s the job market is like in Germany.  Highly appreciate your suggestions. Tia location Berlin",self.dataengineering
101,pq3dda,m_bii,https://www.youtube.com/watch?v=CD565M8Eta8&amp;list=PLa7VYi0yPIH1B0i7mhzVi78TIkKSd-0vE&amp;index=21,"Zhamak Dehghani Explains What is Data Mesh, and How does it work?",,youtube.com
102,pq1qh5,Phantazein,https://www.reddit.com/r/dataengineering/comments/pq1qh5/datawarehouse_best_practice/,Datawarehouse Best Practice?,What is the best practice for storing source data in your data warehouse? At my current work we basically copy tables directly from the transactional system to identical tables in our datawarehouse.   I'm not currently a data engineer but I'm trying to break into the field and I thought I read somewhere that copying the source system 1-to-1 is not what you want to do?   &amp;#x200B;  Thoughts?,self.dataengineering
103,pq1oc8,sa1amandra,https://www.reddit.com/r/dataengineering/comments/pq1oc8/quantumblack_data_engineering/,QuantumBlack Data Engineering?,"Hi /r/dataengineering,  I was shopping around for job postings and saw QuantumBlack (owned by the consulting firm McKinsey) were firing Data Engineers.  Was wondering if anyone in the sub has experience with this company, what it's like, the type of work, and would you recommend applying here?",self.dataengineering
104,pq0su2,saik2363,https://www.dasca.org/world-of-big-data/article/the-simple-5-step-process-for-creating-a-winning-data-pipeline,The Simple 5-Step Process for Creating a Winning Data Pipeline,,dasca.org
105,pq0nc1,stackedhats,https://www.reddit.com/r/dataengineering/comments/pq0nc1/suggestion_thread_for_ted_talks/,Suggestion: Thread for TED Talks,"I recently watched a TED talk called ""Stop Using Classes"" (or something to that effect) about some of the negative design patterns that result from overusing OOP (the joke was that if your class has two methods and one is init, you don't need a class).  I feel like it would make sense to build a thread or wiki page to compile useful talks, videos and podcasts about data engineering (Or more general SWE).  &amp;#x200B;  Now, as there is a larger degree of personal taste and subjectivity when it comes to talks than say, ""The Data Warehouse Toolkit"" we'll probably need to hash out some ground rules.  &amp;#x200B;  For example, since watching a TED talk is free and not a huge time commitment, I think it's reasonable to prohibit people asking something like ""Is X talk worth watching.""  Discussion is great, but maybe we have people do that in separate threads so we avoid the 50+ deep nested comments that take up 2 pages?  Possibly organize a quarterly poll based on suggested videos so people can vote on the ones they thought were particularly good?  &amp;#x200B;  Thoughts?",self.dataengineering
106,pq08hw,nonkeymn,https://www.youtube.com/watch?v=6RiA_Qur2yo,Being A Data Engineer: Expectations vs Reality,,youtube.com
107,ppytpk,Illustrious_Role_304,https://www.reddit.com/r/dataengineering/comments/ppytpk/india_data_engineering_salary_check/,India - data engineering salary check,Lets gets some insights of current DE market in india .  Post below info-  Position title - Org name - your wish City name - Tech stack - Current ctc with breakup,self.dataengineering
108,ppxmqx,boggle_thy_mind,https://www.reddit.com/r/dataengineering/comments/ppxmqx/why_an_sme_should_invest_in_data/,Why an SME should invest in data?,"**In case of Small and Medium sized companies, what would be your sales pitch/value proposition why they should invest in data? Should they?**  I was talking recently to a manager of a boutique online marketing company and he brought up that he was considering to centralize reporting - currently they do it manually by pulling numbers from 3rd party tool providers, insert into excel, then into a ppt presentation that is sent to a client (it's annoying but doable). The data is mostly from 3rd party providers and can be retrieved via API calls, but pretty much all of them have interfaces, so the data is accessible. I was asked to give an estimate of how long it would take to centralize it and build some reporting on top of it (with other costs), I gave an estimate after which he said that he needs to think about the business case (I guess it was more than he expected).   **I somewhat take it for granted and assume having data inhouse is always good uncritically**, but in this case I didn't know what to say, partly because I'm unfamiliar with the domain and it seems that the people already have access to the data and the scale is not something that they can't handle at the moment. I'm unfamiliar with the tools so can't speak if they lack any functionality.  **Should they invest in data infrastructure or keep as is and stick to 3rd party interfaces until scale grows?**  As I mentioned I don't have experience with the domain, meaning I haven't seen how other companies do it (**would appreciate your input if you have**). Some thoughts I had so far but I don't feel strongly about any of them:   * Trend Analysis? My guess the tools already provide it.  * Operational Efficiency? Mostly holds for larger companies, for a small company, manual processing is not prohibitive, so the cost preassure is not that great yet.  * Marketing, Positioning and Client Satisfaction? Provide better, more frequent and richer reporting to your clients?  * AB Testing? I was told that the tools provide the interface and data themselves.   * Predictive modeling? Of what?  * Combining Data from different suppliers into new views. Can't really tell upfront what useful combinations are possible.  * Better/New reporting to drive decision making? What kind, what is there that they can't access through their tools.",self.dataengineering
109,ppv549,camelCaseGuy,https://www.reddit.com/r/dataengineering/comments/ppv549/data_engineering_at_revolut/,Data Engineering at Revolut,"Hi all! I'm being recruited by Revolut for their DE team and I would love to know what kind of company they are.  I mean, it seems like a very decent job, with great salary and all, but the work-life balance seems to be a bit off for what I've read in Glassdoor. So I would love to understand if this is true, was true but isn't anymore, or is plainly false.  Does anyone work there or has a friend that works there that could enlighten me?  Cheers!",self.dataengineering
110,ppuh5c,hatchikyu,https://www.reddit.com/r/dataengineering/comments/ppuh5c/if_i_had_to_explain_data_engineering_work_to_a/,If I had to explain data engineering work to a business colleague...,"*Let's build a common framework of how we'd explain (or more likely ""sell"") DE to non-engineering folk*  This is just a start of how I'd explain data engineering...  **Supports the following kind of work (that business people understand):**  * Machine learning * Data Science * Deep Learning  **Most common jargon**  ETL  - extract-transform-load — tooling to move data from place to place  **Scope of work**  * Enormous amounts of data means heavy lifting around **scalability** * Responsible for deciding what data is useful and what is superfluous * Today’s data engineers are essentially specialist software engineers  **Capability areas**  * SDLC best practice - version, control, release management, automated pipelines, implementing open-source tools like Spark and Tensorflow * InfoSec - cloud security best practice, data handling, data privacy, GDPR, OSS security etc. * Data architecture - distributed databases, traceability, data model definition * Business domain knowledge - know-how of industry working within, communicate findings with the audience at their level and intent of understanding  **10 years ago vs now**  * Ten years ago, data engineering was data warehousing, business intelligence and ETL * There was limited development of pipelines for analytical models * Very fast space now with a lot of directions you can go within the same org * ""Throwing myself in as an example, I'm now going into my 8th year in data, but still lack knowledge in K8s, Kafka beyond the surface level, massive-scale migration projects""  **End up working in orgs with varying levels of DE maturity**  1. **Starter** — No data science or data engineering capabilities, very limited infrastructure for analytics 2. **Siloed** — Pockets of analytics capabilities, but uncoordinated and with messy infrastructure 3. **Potential** — Has a functioning analytics centre of excellence (CoE) but team needs to build more capability or is missing process 4. **North Star** — Mature data organisation, working analytics platform",self.dataengineering
111,ppt375,Global-Goat-2949,https://www.reddit.com/r/dataengineering/comments/ppt375/algorithm_leetcode_in_data_engineer_interview/,Algorithm leetcode in Data Engineer Interview,"I have interview in coming and I know there will have Question about algorithms similar in leetcode.. In most case can this be Easy, Medium, Hard?.. And will require concept such as BFS and DFS, link list, or simple string ,array, dict question.?..  Thank you ~",self.dataengineering
112,pps31t,j_joaa,https://www.reddit.com/r/dataengineering/comments/pps31t/job_interview_help/,Job Interview Help,"I recently got an interview with a blockchain company through a current employee referral. I sent him my resume for a position called Data Engineer. Upon actually receiving the interview invitation, I realized that I do not fully satisfy the skills to become a data engineer. I actually do not know why I was scheduled for this specific job. I guess I made the mistake of choosing a position I was not fully qualified for.    I have satisfied the requirements on the job description    \- SQL and Programming   \- Tableau   \- Having knowledge of blockchain    I have not satisfied the requirements on the job description   \- Data ETL (Redshift)   \- Amazon Web Services   \- Snowflake    Learning Amazon Web Services can take a month or more. I feel it would be insane to cram all of the last 3 tools to learn within a 2-3 week timeframe. On the other hand, I would love to explore the role of Data Engineer.  What should I do?",self.dataengineering
113,ppnv8j,mgalarny,https://www.anyscale.com/blog/the-third-generation-of-production-ml-architectures,The Third Generation of Production ML Architectures,,anyscale.com
114,ppmc48,getafterit123,https://www.reddit.com/r/dataengineering/comments/ppmc48/spark_and_s3_integration/,Spark and S3 integration,"Has anyone been able to connect spark 3.0+ to S3 object store using s3 connector or only s3a? I heard somewhere that s3 connector was being depreciated in favor of s3a only in 3.0 and beyond due to s3a being more performant and supporting larger object sizes. Would it be possible to used s3a on the Hadoop configuration on the client side but still connect to a s3://endpoint on the object storage side?  Am I looking at this wrong, does it even matter on the object storage side?",self.dataengineering
115,ppkgeu,Impressive_Path2037,https://www.youtube.com/watch?v=_YPScrckx28,Support Vector Machines (SVM) in 2 minutes (Python code included),,youtube.com
116,ppjtk8,Cloakie,https://www.reddit.com/r/dataengineering/comments/ppjtk8/amundsen_vs_datahub_vs/,Amundsen vs DataHub vs ???,"Just curious if anyone has researched open-sourced data discovery platforms and ultimately decided on integrating one. If so, what was the reason you picked that platform over the other?     I see that Amundsen has a long laundry list of adopters, but I find their documentation to be a little spotty and hard to read-through in comparison to DataHub. Hoping to understand why some companies prefer one vs the other.",self.dataengineering
117,ppig6g,Ok_Profession_745,https://www.reddit.com/r/dataengineering/comments/ppig6g/free_coupon_the_python_programming_for_everyone/,Free Coupon: The Python Programming For Everyone Immersive Training,[removed],self.dataengineering
118,ppiful,Ok_Profession_745,https://www.reddit.com/r/dataengineering/comments/ppiful/free_coupon_the_python_programming_comprehensive/,Free Coupon: The Python Programming Comprehensive Bootcamp,[removed],self.dataengineering
119,ppifhi,Ok_Profession_745,https://www.reddit.com/r/dataengineering/comments/ppifhi/free_coupon_python_programming_beyond_the_basics/,Free Coupon: Python Programming Beyond The Basics &amp; Intermediate Training,[removed],self.dataengineering
120,ppif7m,Ok_Profession_745,https://www.reddit.com/r/dataengineering/comments/ppif7m/free_coupon_advanced_foundations_of_python/,Free Coupon: Advanced Foundations of Python Programming | 2021 Training,[removed],self.dataengineering
121,pphvff,TravellingBeard,https://www.reddit.com/r/dataengineering/comments/pphvff/any_success_pulling_newrelic_data_to_onprem/,Any success pulling NewRelic data TO on-prem database for analysis?,"NewRelic is a great tool for site monitoring.  But we have different dashboards/metrics that don't lend themselves well to being combined on the NewRelic side.  I think they have an API to pull data, and looking into the process, plus I have access to the queries under these dashboards.  I would like to pull disparate sources of data, linked purely by timestamp of event, to more clearly see correlations.  That could easily be done in a local database server if I could find a way to pull the data in, and wondering how others have had success with it, or if they approached this problem a completely different way than I'm thinking, for a similar result?",self.dataengineering
122,ppho3g,danielwbean,https://mixpanel.com/blog/a-modular-approach-for-integrating-an-analytics-platform-like-mixpanel-into-your-ios-app/,A modular approach for integrating an analytics platform into your iOS app,,mixpanel.com
123,pphbrg,NotACardassian,https://www.reddit.com/r/dataengineering/comments/pphbrg/docker_environments_and_best_practices_in/,Docker environments and best practices in Industry question,"I’m really struggling to wrap my head around industry best practice for an airflow workflow. Though I’m setting all this up for a personal project, the goal of the project is to learn so I want to do it the way people would in a production ready industry accepted way.   So say I have a docker container that runs airflow. This is going to schedule my workflows. Most workflows are just pythons scripts that call various rest apis, normalize data, and upload to a database.   Is the docker container my “whole” environment or just my scheduler? Should any additional packages I might use in my airflow script be installed in the python virtual environment inside the container? Or would I setup a second virtual environment inside or outside the docker container so airflow dependencies don’t mess with etl script dependencies where I may want something like the latest version of pandas?   What if I’m also planning to setup a website to report the data? I was thinking a second docker container running a dash server, again, so dependencies don’t get mixed up but is this the correct approach? Maybe the reporting component belongs on another machine entirely?  Last question: im doing this experimentation with digital ocean servers but would AWS be better if I want to use this project as something to talk about in an interview?  Thank you to anyone who bothered to read my long questions!",self.dataengineering
124,ppgu8y,Southern_Region_3967,https://www.reddit.com/r/dataengineering/comments/ppgu8y/any_data_engineers_at_snapchat_how_do_they_pay/,Any data engineers at Snapchat ? How do they pay against FAANG? Levels says TC is 193k for L3 SWE - do you think DE would be the same ?,"Are there any ways to see data engineer salaries at big tech firms , because the common sites usually report swe and Glassdoor and payscale are wrong",self.dataengineering
125,ppgq7z,stephenrajdavid,https://www.reddit.com/r/dataengineering/comments/ppgq7z/provenance_data_on_neo4j/,Provenance data on Neo4j?,we have provenance data of assets(~5m records) and stored in elasticsearch currently. Maintaining relationships between records has become pain(relationships are maintained by adding parent and child record IDs in each and every record).  Thinking of moving to Neo4j. What are your opinions?,self.dataengineering
126,ppfbvm,Solid-Exchange-8447,https://www.reddit.com/r/dataengineering/comments/ppfbvm/looking_for_a_project_idea_to_impress_an_employer/,Looking for a project idea to impress an employer for Jnr DE position?,"Wise man says ""Nothing beats a portfolio"". So here I am....asking for opinions/ideas on a project idea would generally impress an employer for a jnr DE position.   Briefly context: I've just recently switched to DE from a non-background. So basically I'm learning on my own. So far, I can code (in Python/SQL), have a little experience in Database design (not so much confident though) and Snowflake. It looks like every material I touch is all worth learning. I do have a LONG LIST of what needs to learn (from Reddit, thank you.) in order to be qualified for SE since lots of folks advise that you must become a SE to become a competent DE. Therefore, it'd take me a long time to get a hang of it. So I think it's the best strategy for me to take an approach ""learning by doing"".  I'd welcome and appreciate all sorts of ideas that can give me some sort of directions.  THANK YOU.",self.dataengineering
127,ppdgdm,castor-metadata,https://www.reddit.com/r/dataengineering/comments/ppdgdm/how_to_build_your_data_team/,How to build your data team?,"When building a data analytics team, heads of data typically grapple with the following questions:  * How big should this team be? * How many data engineers, data analysts, data scientists? * How does the team interact with the rest of the organization? * Which structure for the data team? Centralized or Embedded?  They rightly do so; having a strong data team is not a luxury anymore, but essential to the very survival of a company today. Here's a [good breakdown](https://www.castordoc.com/blog/how-to-build-your-data-team) of what to check when building a data team.     I'd love to know your answers to these questions in the comments.",self.dataengineering
128,pp9ogz,vinsanity1603,https://www.reddit.com/r/dataengineering/comments/pp9ogz/convert_do_code_block_to_postgres_function/,Convert DO code block to Postgres Function,"Hi, I've been trying to create a dynamic view where the number of columns can change depends on the unique values to be used in the pivot. I found this stackoverflow [https://stackoverflow.com/questions/12988575/crosstab-with-a-large-or-undefined-number-of-categories/12989297#12989297](https://stackoverflow.com/questions/12988575/crosstab-with-a-large-or-undefined-number-of-categories/12989297#12989297) solution where it uses a crosstab inside a DO code block and made a table out it, and it worked like charm.     However, I wanted to make it a function that returns a view so that when I call the function, it will give me right away the results. Can anyone help me or guide me? Thanks.",self.dataengineering
129,pp9o1t,sonalg,https://www.reddit.com/r/dataengineering/comments/pp9o1t/zingg_open_source_data_reconciliation_and/,Zingg : Open source data reconciliation and deduplication using ML and Spark,We often talk about data silos and the need to build data warehouses and lakehouses. One common need post getting the data in one place is the need to establish relations in the data - linking records of the same entity together for analytics and compliance. Happy to open source Zingg - an ML based tool that can reconcile and deduplicate records. Very keen to hear feedback and comments.   [https://github.com/zinggAI/zingg](https://github.com/zinggAI/zingg),self.dataengineering
130,pp96o9,Rare_Mud7490,https://www.reddit.com/r/dataengineering/comments/pp96o9/career_guidance/,Career guidance,"I'm in my bachelor's from India and I found my passion for data engineering. But there are not many resources (courses, tutorials) online for learning DE. Can someone please share any courses/tutorials which covers it entirely. As this is a specialised field , what will the job prospects after my graduation ?",self.dataengineering
131,pp2y15,_swizzlemmk_,https://www.reddit.com/r/dataengineering/comments/pp2y15/dataops_life_cycle/,DataOps life cycle,[removed],self.dataengineering
132,pp0l7j,SeshuAd,https://www.reddit.com/r/dataengineering/comments/pp0l7j/a_paved_road_for_data_pipelines_at_intuit/,A Paved Road for Data Pipelines at Intuit,"In our first installment of  this Medium blog, we discussed the need for A Paved Road for Data  Pipelines at Intuit, covering various types of pipelines \[ingestion,  curation, ETL (e.g., extract, transform, load) pipelines\], how to manage  these pipelines through a developer portal, and a variety of tools  available for monitoring, managing lineage, etc.     Over the past  year, we’ve made substantial progress. In this second installment, we’ll  explore data processing capabilities and take a deep dive into our  batch processing platform.  [https://medium.com/intuit-engineering/a-paved-road-for-data-pipelines-part-2-e17e5a377e99?source=friends\_link&amp;sk=1ddcf2f6ed4b57fad77c307bfaf773ba](https://medium.com/intuit-engineering/a-paved-road-for-data-pipelines-part-2-e17e5a377e99?source=friends_link&amp;sk=1ddcf2f6ed4b57fad77c307bfaf773ba)",self.dataengineering
133,pp0096,nokia_user,https://www.reddit.com/r/dataengineering/comments/pp0096/is_it_just_me_or_there_are_way_less_h1b_workers/,Is it just me or there are way less H1B workers in DE space compared to SWE?,"Really curious to know if you guys feel the same. It is something I have noticed ever since I switched to DE. Also, undoubtedly, a good thing in terms of market saturation.",self.dataengineering
134,pozvi7,knlph,https://www.reddit.com/r/dataengineering/comments/pozvi7/what_is_your_basic_evaluation_criteriaapproach/,What is your basic evaluation criteria/approach while evaluating a tool for your data stack?,"Hi everyone,  I am interested in learning about your evaluation criteria/questions while choosing a tool for your data stack. This is specifically keeping in mind that most tool choices are made ""for scale"". There is so much happening around the modern data stack. The thought was triggered by this excellent blog from Postman on how they tried multiple ways to solve challenges for their data team and then finally choosing ""Atlan"" as the tool for data discovery.   P.S. The blog is great to understand that journey and how they are building trust within data teams by solving the problem around ""context"".  [https://blog.postman.com/how-postman-fixed-missing-layer-in-our-data-stack/](https://blog.postman.com/how-postman-fixed-missing-layer-in-our-data-stack/)",self.dataengineering
135,pozm2s,moonstormer,https://www.datafold.com/blog/data-quality-management-according-to-lyft-shopify-and-thumbtack,"Data Quality Management According to Lyft, Shopify, and Thumbtack",,datafold.com
136,poz9lz,dc_atoms,https://blog.postman.com/how-postman-fixed-missing-layer-in-our-data-stack/,How Postman Fixed Missing Layer in Their Data Stack,,blog.postman.com
137,powzy9,mirado_classic,https://www.reddit.com/r/dataengineering/comments/powzy9/follow_up_as_a_de_what_is_your_biggest/,"Follow Up: As a DE, what is your biggest people/organizational frustration?","Follow up from the previous poll: [https://www.reddit.com/r/dataengineering/comments/potmyp/as\_a\_data\_engineer\_what\_are\_your\_biggest/](https://www.reddit.com/r/dataengineering/comments/potmyp/as_a_data_engineer_what_are_your_biggest/)  &amp;#x200B;  Many of you said ""people"" so I created a whole new set of people-related frustrations.  Again, please also respond with topics not mentioned in the poll.  [View Poll](https://www.reddit.com/poll/powzy9)",self.dataengineering
138,potmyp,mirado_classic,https://www.reddit.com/r/dataengineering/comments/potmyp/as_a_data_engineer_what_are_your_biggest/,"As a Data Engineer, what are your biggest technical frustrations?","As a data engineer, what technical frustrations drive you the craziest? Feel free to respond with other options.  [View Poll](https://www.reddit.com/poll/potmyp)",self.dataengineering
139,potc9s,m_bii,https://www.youtube.com/watch?v=CD565M8Eta8&amp;list=PLa7VYi0yPIH1B0i7mhzVi78TIkKSd-0vE&amp;index=21,"Zhamak Dehghani Explains What is Data Mesh, and How does it work?",,youtube.com
140,posl4v,Conscious_Agent_5129,https://www.reddit.com/r/dataengineering/comments/posl4v/opensource_data_lineage_for_dwh/,Open-source Data lineage for DWH,"Hi,  My friend and I wrote a tool to visualize dependencies between tables in DWH by only reading the query history.  We would love to get some feedback and learn what we should add and improve, it is really easy to use, you can have your graph in 2 mins:  [https://github.com/elementary-data/elementary-lineage](https://github.com/elementary-data/elementary-lineage)",self.dataengineering
141,porsan,careerhelpthroaway,https://www.reddit.com/r/dataengineering/comments/porsan/data_engineering_in_an_it_consultancy_career/,Data Engineering in an IT consultancy: career advice needed,"Hi all, I seek any advice you may have.  Basically I've been headhunted for a role that is offering a \~40% increment, same amount of annual leave, slightly better medical benefits. The reason why I'm not jumping to accept it is that it's at a tech consultancy that somewhat has a reputation for being an IT bodyshop. The kind that takes plenty of outsourcing shops and have a reputation for being a headache to work with. If you know, you know. I am currently at a product-based company. The roles and responsibilities are roughly the same at both roles, though I can anticipate that the working style will be tremendously different. I say this from my experience working on the client side with one of these IT consultancies in my previous career.  I am in my early 30s now and was a career changer about 2 years ago. Since then, this is my second role. I like my current role and company and was promoted recently. However, compensation, is definitely on the low-side when benchmarked to the industry. My existing plan prior to this offer has been to start looking out for opportunities towards the year end, aiming for a similar 40%ish increase in mind, since I know another promotion at my current firm is probably only gonna do 15-20% at best.  So here are my current considerations and options:  \- Take the offer. I know with 99.9% certainty that I will not want to stay at the tech consultancy (the firm that headhunted me). Reputation of the firm aside, I wish to work at product-based places. The only reason I am considering this is to bump my base salary, and henceforth deal with the HR practice (this is quite prevalent in Singapore, it may not apply to you) of asking for your last 3 months payslip and benchmarking your offer to it. I am hoping that with a better base salary, I won't get lowballed at my next role. If I accept the offer, I plan to start looking for something else in 2months. To what extent is this going to be a red flag to the places I apply?  \- Use the offer to negotiate a raise at my current firm. However, I know my mileage here is extremely limited because i just got promoted a few months ago, and also due the company's existing payscale. But if this is possible, i'd be happy with a 20-25% increment.  \- Does being in a service-based company work against me when I want to apply to a product-based company again or am i being paranoid  \- Growth opportunities skillset wise, I am fairly certain that it's better at my current company  \- Any sharing from folks working at IT consultancies of the like would be welcome too  \- How valid are my concerns about getting lowballed on my next salary offer? It's pretty much the only reason I'm considering the offer. To the best of your knowledge, to what extent does your HR consider the existing salary vs the actual budget for the role when making an offer?  I'd really appreciate any thoughts and comments with regards to my situation. Feel free to point out whatever flaws you think exist in my general train of thought.",self.dataengineering
142,poqthi,Material_Cheetah934,https://www.reddit.com/r/dataengineering/comments/poqthi/documenting_by_parsing_sql_into_diagrams/,Documenting by parsing SQL into diagrams?,"I have heard of DBT, but I have constraints that will prevent me from using it, either that or my understanding is limited.  We are trying to move an existing ETL into 21st century, building on Azure infra. This is the POC that they want me to architect and develop from the ground up.  So I figured I need to document the pipeline. It is about &gt; 100 SAS SQL(essentially SAS SQL dialect with SAS data procs interleaved, yes FML) files each with &gt; 3 tables being transformed and fed into subsequent SAS SQL files.  Of course no docs or anything like that, SAS devs said code is their documentation(&lt;insert samuel jackson meme/&gt;). I have been using graphviz to create documentation on the data flow, but it seems to be taking forever to go through. I have spent upwards of 2 weeks and only 1/3 of the way through the pile of files. Probably because I don't know SAS and nothing is organized, there is 1 folder called ""code"" with all the code in it. The graphviz diagram has grown SO huge that it takes up my entire 43 inch UW screen and still requires zooming in considerably. Rendering the graphviz digram seems to be taking longer and longer now. But I am now starting to see the shitshow that is starting to develop slowly.  Some of my constraints passed down by redtape:  - Cannot hook up a data modeling tool to point to any upstream databases - Cannot **export** data from the SAS environment to take a look at it or work with it, at least not without going through another group to de-identify it(for a charge back). - Cannot requisition a DB server to copy data into. - Cannot make existing changes to already owned DB server with PHI, eg. schema/column/table level changes without going through a db ""admin"" group(for a charge back). - Can only view data being worked on through SAS software - Can query data upstream through SQL developer, but cannot download it locally. - SAS devs not willing to explain everything through...company hasn't been reserved about telling the existing SAS devs that they are on their way out soon if they don't learn other tech, so you can imagine they haven't been cooperating.   Essentially what I need is a place that I can park the semi-SQL(SAS sql dialect) in the existing files that will also diagram the data lineage. I will translate where necessary to a normal SQL dialect. Just trying to reduce any high time draining work which can be done by an existing tool.  Any help would be appreciated to point me to a tool that will essentially parse sql and build a data lineage diagram for me. I just need to get a sense of situational awareness, other than the nagging voice in my head screaming ""WTF"" every time I open one of the SAS files.",self.dataengineering
143,poouhc,WalrusWhich202,https://www.reddit.com/r/dataengineering/comments/poouhc/general_python_library_for_de/,General python library for DE,"Due to some circumstances i am using pho for my whole etl process. However, i understand python, and some library that might be related, which are, sqlalchemy, panda, and numpy. Besides the library i shown is there any important pytjon library that i should understand for my DE career?",self.dataengineering
144,ponzhb,ThunderBlade-,https://www.reddit.com/r/dataengineering/comments/ponzhb/i_have_an_interview_for_data_engineer_role/,"I have an interview for Data Engineer role tomorrow, what all questions should I prepare? In the interview it’s mentioned as Python + SQL.",Please help.,self.dataengineering
145,ponkx1,Gaston154,https://www.reddit.com/r/dataengineering/comments/ponkx1/will_my_nonrelevant_bachelor_be_an_hurdle/,Will my non-relevant bachelor be an hurdle?,"Hello, after a whole year of studying math, stats and CS at a non-university school, I managed after an admission test to get enrolled in a master in Data Science that is particularly heavy on math. I have been told by former students that it's not a bullshit degree, but rather a quite rigorous one based on the theory rather than application.  I have a bachelor in law and used to work in the data protection team. I now intend to pursue the data Engineering route since I would prefer having a more CS type of job rather than a mathematical heavy one right out of my master's.  Will my bachelor in law cause problems when I will start applying for jobs in two years?",self.dataengineering
146,ponhwq,ciskoh3,https://www.reddit.com/r/dataengineering/comments/ponhwq/advice_on_db_for_ml_project/,Advice on DB for ML project,"HI all,  I am preparing a DB for traning  several ML model. The DB will be fairly simple: structured, holding integer and categorical data, with max 5 tables (of max 10 columns each) but with a lot of datapoints (I have calculated that I would like to reach 10¹² datapoints eventually). Once populated, the DB will be accessed by up to 4 connections, that will download data in batches to train different ml models.   I have no idea of how much memory is required for that, and what would be the best solution. Since this is for my startup, money is very tight as well. Also I do not necessarily need it to be cloud (although there are obvious advantages)  Which solution would you pick:  \- Local Postgres SQL (if the whole thing stays below 600 GB)  \- Google cloud SQL with PostGREs server  \-Google BigQuery (not sure if this is needed)  \- Other?....  &amp;#x200B;  Thanks",self.dataengineering
147,pondby,nezamolmolk,https://www.reddit.com/r/dataengineering/comments/pondby/hierarchical_treemap_from_scratch_with_no/,Hierarchical Treemap from scratch with no aggregation,"Hello community, hope you are doing well.  Let me introduce myself, I am a Junior Data Analyst and I allow myself to do this post on your group because I need advices.  The situation is as follows : I have Hierarchical data with more than two levels, and there is no sum from a child level to a parent level, each level is independant from the other (you can see the [csv file](https://drive.google.com/drive/folders/1ujNevGh9GFG8Dz7H0FL_notNoXi599pG?usp=sharing) on the drive I shared).  The idea is therefore to create a Treemap visualization with drill down / up functionality to get an idea of ​​the value of each level / each sub level.  The problem here is that all the treemaps I tested (google Data Studio / Tableau / Python / R) are based on aggregation from a lower level to a higher level and in some situations can support only two levels, (you can see the [HTML file](https://drive.google.com/drive/folders/1ujNevGh9GFG8Dz7H0FL_notNoXi599pG?usp=sharing) on the drive link I shared).  I searched a bit on the internet, and I found that we can define our [own Treemap](https://js.devexpress.com/Demos/WidgetsGallery/Demo/Charts/HierarchicalDataStructure/jQuery/Light/) from scratch,  On the previous example, there is only one level, can you please give me some tutorials / code snippets/indications that can help me build a hierarchical multi-level treemap that is not based on aggregation from a level to another?  I am grateful for the time you give me to understand my problem.  P.S: I am not a pro JS, but I am open to learn because the one who does not try anything has nothing.  Thank you community \^\^.",self.dataengineering
148,pomqcg,dadadawe,https://www.reddit.com/r/dataengineering/comments/pomqcg/dbt_looker_whats_it_all_about/,"DBT, Looker, ... what's it all about?","Heya,  Been working on a DWH for a while (cloud &amp; on prem) but now I see more and more references to DBT &amp; Looker.   They all talk about ""models"", what's up with that? I know what a data model is, but it almost feels like I should capitalize ""Model"".  Aside from flashy interfaces, what would be the difference between me generating a view in PostgreSQL or using a tool like Denodo to virtualize my star schema on top of the storage layer?   Thanks for helping out and saving me hours of read time!",self.dataengineering
149,pomfl0,dazajuandaniel,https://www.reddit.com/r/dataengineering/comments/pomfl0/any_resource_that_shows_a_real_life_cicd/,Any resource that shows a real life CI/CD implementation?,"I'm implementing a CI/CD at my work place but I'm looking to see a real life one to understand the process over it.  My CI/CD will be used to deploy Infrastructure using CloudFormation and ETL Python Jobs  I'm highly interested in understanding testing, failure handling, and human approval process.  Any good resources of real life ones? Most are vague blog posts that cover high level concepts.",self.dataengineering
150,poma73,boxy2121,https://www.reddit.com/r/dataengineering/comments/poma73/hiring_analytics_freelance_writers_remote/,Hiring Analytics Freelance Writers (Remote),"Hi everyone,  &amp;#x200B;  I'm looking to hire freelance data analytics writers to write bog articles on my company's blog. We're a tech company based in Singapore, with 18k readers on our blog.  &amp;#x200B;  If you're interested in applying your analytics expertise and building your portfolio for personal branding, please check out this job description and apply directly on LinkedIn:  &amp;#x200B;  [https://www.linkedin.com/jobs/view/2693130301/](https://www.linkedin.com/jobs/view/2693130301/)  &amp;#x200B;  This role is remote and will be paid by article.  &amp;#x200B;  (PS. Dear admin, if this post isn't allowed, please feel free to take it down)",self.dataengineering
151,poj4u0,[deleted],https://industrywired.com/are-you-looking-for-data-engineer-jobs-then-here-you-go/,Are You Looking for Data Engineer Jobs? Then Here You Go!,[deleted],industrywired.com
152,poixiq,sorenadayo,https://www.reddit.com/r/dataengineering/comments/poixiq/can_a_periodic_snapshot_fact_table_grain_be/,Can a Periodic Snapshot Fact Table grain be segmented?,"Goal: Build tables to support a dashboard that shows top unique view movies, with filters by date and category.     Design:  https://preview.redd.it/dxsh46xffln71.png?width=857&amp;format=png&amp;auto=webp&amp;s=ea58ee0f09591985c4e5b508500ba34c9add2176  Her fact\_movie\_view grain is one row per date per movie. Normally periodic snapshot fact is one row per date.     Is this design wise? Is there a better design?",self.dataengineering
153,poimtb,brownstrom,https://www.reddit.com/r/dataengineering/comments/poimtb/data_profiling_reviews_on_dbtprofiler_or_any/,Data Profiling: Reviews on dbt-profiler or any similar tool?,I work at a small startup and my organization is looking at ways of automating data profiling.   Was wondering what tools other people use to profile large datasets with about billion rows. I came across **dbt**\-**profile** and also **pandas profiler**.   Don't like pandas profiler as it doesn't use Spark so it would be very expensive to convert spark df to pandas for data profiling.   dbt-profiler is still in beta and kind off new to the market. Any suggestions are welcome.,self.dataengineering
154,poi4im,gloriarachel77,https://www.reddit.com/r/dataengineering/comments/poi4im/how_can_i_become_a_data_engineer/,How can I become a data engineer?,"Hi friends! I am a data scientist in the banking industry, and because my team is using Hadoop and Big Data techniques, I want to know whether there are some good resources to let me learn those myself. I focus on data analytics and almost have no idea of data engineering and Big Data. I took several courses regarding Hadoop and PySpark, but still feel a little confused.  If you have any good recommendations for data engineering courses, plz let me know! Many thanks.",self.dataengineering
155,pofn1w,Diawhaties,https://www.reddit.com/r/dataengineering/comments/pofn1w/sql_update_based_on_timestamp_field/,SQL- update based on timestamp field?,"At my new job, the team uses merge statements to update tables in SQL. They’re convenient, but the issue is that rows that don’t need to be updated will be updated.  I’m used to creating separate insert and update scripts. I always keep the timestamp from the source in my target table, and will only update a record in the target table when the timestamps aren’t equal. Is this bad practice or just unnecessary?   Update Table Target Set A=A ,Timestamp=timestamp From Source Where source.ID=Target.ID And Source.timestamp != target.timestamp",self.dataengineering
156,podqsz,Scalar_Mikeman,https://www.reddit.com/r/dataengineering/comments/podqsz/resume_critique_and_advice/,Resume critique and advice,"Looking for feedback on my resume. Even though my job title is Senior Data Analyst, there are only 3 of us on the IT team so it's over 50% user support, hardware installs, Firewall and phone system admining etc. I've been doing courses outside of work constantly for the past 5+ years and I am looking to make the jump to full time Data Engineer.   https://preview.redd.it/09htp6mbtjn71.png?width=628&amp;format=png&amp;auto=webp&amp;s=ad4483c4276232d8c3b0732d0580234fc35b51f4  I need to have a surgery so won't be looking until January. Is there anything that I should REALLY focus on to give me an edge when the time comes to start applying. Also, my total comp package right now is about 130k (so my employer tells me) would it be unreasonable to look for 110k starting salary in a large city? Does anyone know of any good paid Data Engineer mentorship sites/companies?",self.dataengineering
157,poa7e5,TheParanoidPyro,https://www.reddit.com/r/dataengineering/comments/poa7e5/i_have_the_second_interview_to_be_a_jr_data/,I have the second interview to be a Jr data engineer coming up. Stressing out at the potential technical questions.,"I finished a phone interview with the recruitment coordinator for a Jr. Data engineering position. And have a zoom interview with the hiring manager tomorrow, with a possible third in-person interview if that goes well.   I have been applying to positions since I am scheduled to graduate with my bachelor's in November.  The position was written in a way that suggested that it truly was a junior position, as opposed to the ones I've seen that want master's. One part of the expectations is: a working sql knowledge, as well as a working familiarity with a variety of databases.  I made it known in my resume and cover letter, that I'm still in the process of learning and have no real projects or examples to showcase my knowledge. So I shouldn't be worried that I'm not qualified if I was told that ny application was what they were looking for.  The company uses aws, I haven't had the time to learn it, as I have been devoting my time to finishing my bachelor's.   I want to stress that I'm not asking for a list of things to learn or what steps I need to take. I know the steps, I just feel like the dog that caught the car, I didn't expect to get an interview so quickly. And the imposter syndrome is deafening.  What could possibly be expected if I reach a technical interview?   If they wrote and emphasized ""working knowledge"" but expect me to help with building or maintaining the etl pipelines, how technical could the questions be?  Thanks for your time.",self.dataengineering
158,po9ru6,theant97,https://www.reddit.com/r/dataengineering/comments/po9ru6/mind_fish_confused/,Mind fish : confused,"Hello everyone. I’m currently 33 with zero Sql and programming skills with 13+ . Currently leading a small Dev team . Which path should I take and why  1. Complete python, sql and spark courses and become a de  2. Continue to lead and upskill myself in data team manager like scrum , project manager etc or in management side  I love pt1. But due to responsibilities of completing the project deadline makes me to focus on the project but not on learning new. I’m getting project based learning.",self.dataengineering
159,po9qel,Scott-Moore-007,https://www.reddit.com/r/dataengineering/comments/po9qel/data_engineering_in_manufacturing_environments/,Data Engineering in Manufacturing Environments,"Full transparency: I have a consulting / strategy background and have recently begun working in a management / advisory role in the IIoT space. Manufacturing data - the sheer quantity of it and messiness of it - fascinates me.  I'm wondering if any data engineers in this channel work with manufacturing / operational data. What led you to select that industry? Are there systems you prefer working with (e.g., SCADA, MES, historian)? Do you expect the tech stack to change to make it easier to prepare these types of data?  I understand this is more of a macro-oriented thread for this community, but have several other questions if there is any traction here.",self.dataengineering
160,po8y8l,honorchan1,https://www.reddit.com/r/dataengineering/comments/po8y8l/airflows_best_kept_secrets_how_to_track_metadata/,Airflow's Best Kept Secrets: How To Track Metadata With Airflow Cluster Policies &amp; Task Callbacks,"Databand.ai Senior Software Engineer, Jonathan Barda, recently penned a [**tutorial on how to track metadata using Airflow's cluster policies and task callbacks**](https://databand.ai/blog/how-to-track-metadata-with-airflow-cluster-policies-task-callbacks/?utm_source=forum&amp;utm_medium=r&amp;utm_group=de). This post was written for data engineers who want to find a scalable way of maintaining their platform where hundreds of pipelines are involved. The solutions recommended here will help you track and alert on important data quality and pipeline issues.  We hope you'll find this useful. Let us know what you think!",self.dataengineering
161,po8sm8,monistordualist,https://www.reddit.com/r/dataengineering/comments/po8sm8/pyspark_on_kubernetes_a_production_boilerplate/,PySpark on Kubernetes: A Production Boilerplate Framework,"I spent some time looking for an all-in-one template/demo for how to run PySpark jobs on Kubernetes--and couldn't quite find something fully fledged, so I made one myself specifically with data scientists and engineers in mind. I hope some of you find this material useful.  Here's the repo: [https://github.com/Albell-Cloud-Labs/pyspark-k8s-boilerplate](https://github.com/Albell-Cloud-Labs/pyspark-k8s-boilerplate)  And a video demo: [https://www.youtube.com/watch?v=iH2CDQJNiAw](https://www.youtube.com/watch?v=iH2CDQJNiAw)  Any feedback is appreciated :)",self.dataengineering
162,po7imu,stani76,https://www.reddit.com/r/dataengineering/comments/po7imu/is_python_sql_spark_enough_to_enter_de_role/,Is Python + SQL + Spark enough to enter DE role?,"I am coming from database role and wondering what skills I need to learn, thank you in advance",self.dataengineering
163,po6zh0,Nervous-Chain-5301,https://www.reddit.com/r/dataengineering/comments/po6zh0/aws_managed_airflow_or_aws_sam_for_simple_etls/,AWS Managed Airflow or AWS SAM for Simple ETLs,"My company's core data stack is Fivetran + Snowflake + DBT + Looker. Fivetran is our data loading tool, we load raw data into Snowflake, and process using DBT. We are planning on ingesting data sources that do not have a native Fivetran connector, and I am brainstorming the best approach.  &amp;#x200B;  My initial thought was to use AWS managed Airflow...and write simple DAGs to load raw data to Snowflake. The advantages here are the managed infrastructure, and out of the box notification/logging. The only disadvantage I could think of is that its a heavy handed approach for very simple load jobs...and the cost of the managed instance.   &amp;#x200B;  That led me to look into AWS SAM, where I had a framework for writing simple lambdas. The disadvantage here would be more custom coding required for the jobs, and, to my knowledge, not a simple way to allow a historical backfill in the event of any run failures.   &amp;#x200B;  90% of all data loading jobs I foresee would be handled by Fivetran, curious if anyone else was in a similar position where they needed to ingest data not supported by their Data Replication tool.  &amp;#x200B;  i want to avoid writing mishmash Python scripts, and have a solid, reliable, and consistent design for each new data source to integrate.",self.dataengineering
164,po6jpv,SingingNumbers,https://www.reddit.com/r/dataengineering/comments/po6jpv/how_do_i_improve_my_data_pipeline/,How do I improve my data pipeline?,"**TLDR:** I built a pandas/Jupyter/Excel/openpyxl data pipeline for my job. I'm the only person who knows how this pipeline works. I do some regular testing and I'm not using Git. I'm wondering how to improve this pipeline?  &amp;#x200B;  **PIPELINE OVERVIEW**  * **I'm using:**    * pandas to do ETL    * Jupyter as my IDE    * Excel for storing my data    * openpyxl to make formatted Excel reports * **Other notes about my pipeline:**    * This pipeline runs on a monthly cycle    * None of my Excel workbooks are larger than 100MB    * I'm the only person who knows how this pipeline works    * The rest of my team are accountants who mainly use Excel (and don't know Python)    * I've been using Python for a year (I had a bit of coding experience before that) * **My source data is messy/formatted Excel workbooks**    * messy/formatted = data spread out across multiple worksheets, data isn't normalized, there are lots of blank rows/columns for formatting, etc.....    * I use pandas to do ETL on the raw data…...and output clean/normalized tables (in Excel workbook format)    * How I generally use pandas: read\_excel().....then some dataframe transformations…...then to\_excel() * **My pandas/openpyxl codebase is about 20-40 Jupyter Notebooks**    * If I need to see the code's output (e.g. because I might change a filter based on this month's data)........then I do Shift + Enter all the way through the Notebook    * If I don't need to see the code's output.........then I just hit Run All * **I run some tests as part of my pipeline**    * My testing is mainly looking at Excel PivotTables to see if my data looks the way I expect    * I'm not doing any unit testing       * I don't really understand what unit testing is........or how it would be helpful for me * **I'm not using Git at all**    * I can detect pretty quickly whether my code has a bug........and I can usually fix bugs pretty quickly.......so I don't see the need for version control    * I save a backup of my code before I change it  &amp;#x200B;  **SPECIFIC QUESTIONS ABOUT IMPROVING MY PIPELINE**  * **How can I get my pipeline so that someone else could quickly learn how it works?**    * Scenario: my team hires someone else who knows Python........and I need to train that person in 3-6 months before I leave my team * **How can I make my pipeline more automated?**    * I'd like to be able to hit Run All on most of my Jupyter Notebooks.........rather than doing Shift+Enter all the way through a Notebook (so that I can see the output of the code) * **Should I use Airflow?**    * Would Airflow be overkill if none of my Excel workbooks are bigger than 100MB ?    * I can mentally keep track of my monthly process right now.......so do I need Airflow?",self.dataengineering
165,po6ckp,Exact-Status7352,https://www.reddit.com/r/dataengineering/comments/po6ckp/design_data_pipeline_to_ingest_large_json_files/,Design data pipeline to Ingest large Json files,I wanted to see if anyone can help me architect data ingestion. The source files are around 15 gb Json   I get a file daily which isn’t a delta but the full load. So I will have to process this file daily to determine any changes and load.   What is the best way to build out this data pipeline in Azure Data Factory?   Thanks!,self.dataengineering
166,po6b3k,stackedhats,https://www.reddit.com/r/dataengineering/comments/po6b3k/bcp_format_file_autogeneration/,BCP Format File Auto-generation?,"I'm writing a python wrapper for BCP (really I'm adding functionality to my pyodbc wrapper class to handle BCP).  I'm trying to add a timestamp column to the table I'm loading from csv, and have the column set to a default of current\_time so it should auto-generate upon insertion.  However, without using a format file this throws an error because it expects to pull in the timestamp from the actual csv.  So I need a format file to tell it to skip that column during the import.  Here's the question in three parts:  1. Would automatically generating the format file be horrible practice? For this to work you have to operate on the assumptions that any changes made to the destination table were intended and are correct, and that those changes are reflected in the source data generation as well. (This is the main reason I'm posting here and not stackoverflow) 2. Is it possible to automatically generate the format file with the built-in bcp command, without using an interactive command line session? 3. If I can't use the built-in would I be better off just writing an in-house format file generator?  &amp;#x200B;  My google searching makes it seem like most people aren't trying to automate this part of BCP, so if you think I'm just grabbing rope to hang myself with later let me know.",self.dataengineering
167,po5kgw,marcosmarxm,https://www.reddit.com/r/dataengineering/comments/po5kgw/presentation_airbyte_airflow_on_gcp/,Presentation: Airbyte + Airflow on GCP,"Hey there, I'm a User Success Engineer at Airbyte.  Tomorrow I'll go over how to set up Airbyte + Google Composer on GCP and send data from a Postgres Database to BigQuery! If you're curious about how to leverage Airbyte with Airflow this is your chance! You can register for the Livestorm session here: [https://airbyte.io/weekly-office-hours](https://airbyte.io/weekly-office-hours?utm_content=buffer52018&amp;utm_medium=social&amp;utm_source=linkedin.com&amp;utm_campaign=buffer)  I'll discuss when to use Airbyte and Airflow, deploy both in GCP (hope everything works! live code is hard)  &amp;#x200B;  See ya :wave:",self.dataengineering
168,po5i9t,kaartman1,https://www.reddit.com/r/dataengineering/comments/po5i9t/pivot_from_rdms_to_de/,Pivot from RDMS to DE,"Hello everyone, I worked as a Oracle pl/sql developer for 5 years and then transitioned into Oracle ERP(I still use pl/sql on daily basis but nothing complex).   I am familiar with UNIX, XML and learning Python. I want to pivot to DE and work on interesting projects. I understand that I need to learn some kind of no sql database and workflow tool. But, there are so many technology stacks for DE and it is overwhelming.   Can you please suggest a learning track for me? I want to get a break in DE relative faster ( 3-4 months pre time). I am willing to work for less money for 1 year.",self.dataengineering
169,po5b1g,yungplantdad,https://www.reddit.com/r/dataengineering/comments/po5b1g/what_goes_into_a_research_and_technical_design/,What goes into a research and technical design document?,I’ve been tasked with completing a research and technical design document for a new API integration into an existing stack. What needs to be included in those documents? This is my first experience writing.,self.dataengineering
170,po3osh,sarmoho,https://www.reddit.com/r/dataengineering/comments/po3osh/what_is_data_model_knowledge_that_data_engineer/,What is data model knowledge that Data Engineer must know?,"Recently during interview session with several companies, they asked me to design data model using Star schema and slowly changing dimensions. The problem is I have not had much experience on this since my previous company use data lake which did not model data to fact and dimension table.  Here is my questions would like some expert here to help and suggest the resource to study:  1. What is data model knowledge that Data Engineer must know? Is the star schema of Kimball is enough?  2. What about data modelling for data lake? What is the best practice to design database and table?  &amp;#x200B;  Thank you",self.dataengineering
171,po25tz,Historical-Rest-6995,https://www.reddit.com/r/dataengineering/comments/po25tz/is_it_hard_to_find_a_sde_new_graduate_with_a_de/,Is it hard to find a SDE new graduate with a DE intern at Amazon? Should I decline the offer and keep finding a SDE intern?,[removed],self.dataengineering
172,po1np1,FeelsToWaltz,https://www.reddit.com/r/dataengineering/comments/po1np1/starting_my_first_de_role_good_resources_for/,Starting my first DE role - good resources for learning Scala and Spark?,I'm transitioning into a role as a Data Engineer at a company that primarily uses Scala as well as Spark and Hadoop.   I was wondering if someone could recommend a few courses that I can use to build up a good base knowledge of these technologies before I start this role - I'd like to come into it with at lease a small bit of familiarity with each technology.  The company also uses GCP - would it be worth going through a GCP Data Engineering course to get familiar with the products they offer?  Any course/textbook/tutorial suggestions would be greatly appreciated!,self.dataengineering
173,pnz47w,Embarrassed_Cap1673,https://www.reddit.com/r/dataengineering/comments/pnz47w/managed_connector_to_pull_intercom_custom_fields/,Managed connector to pull Intercom custom fields,"Hi all,  I've been playing around with some customer service data from the Intercom platform (Conversations).  It is extracted through a managed connector (Stitch).  Hoever, even if stitch pulls all data, it is missing the custom\_\* fields.  I have looked a bit on another extraction service (Fivetran) and they also do not offer the custom\_\* fields data on their connectors to intercom.  Does anyone know of a managed solution (not custom API calls) that offer pulling the custom fields from Intercom?  &amp;#x200B;  Here a diagram of the data taken from Intercom by fivetran:   [https://fivetran.com/connectors/intercom](https://fivetran.com/connectors/intercom)  Here by stitch:  [https://fivetran.com/connectors/intercom](https://fivetran.com/connectors/intercom)",self.dataengineering
174,pnz3dh,MarcosVeezoo,https://www.reddit.com/r/dataengineering/comments/pnz3dh/should_data_teams_specialize_and_whats_the_future/,Should data teams specialize... And what's the future of data engineering?,[removed],self.dataengineering
175,pnyty7,Se7enEl11ven,https://www.reddit.com/r/dataengineering/comments/pnyty7/how_where_did_you_gain_your_data_engineering/,How / where did you gain your data engineering skills?,"I’m curious how DEs gain their stack. Did you learn everything in college, pickup on the job, are you self taught? Share your story  [View Poll](https://www.reddit.com/poll/pnyty7)",self.dataengineering
176,pny53c,sgtbrecht,https://www.reddit.com/r/dataengineering/comments/pny53c/an_alternative_to_aws_solutions_architect_course/,An alternative to AWS Solutions Architect course on Udemy?,"Would this course be a good alternative to learning the whole AWS Solutions Architect course? [https://www.udemy.com/course/data-engineering-using-aws-analytics-services/#reviews](https://www.udemy.com/course/data-engineering-using-aws-analytics-services/#reviews)      I just transitioned into a Data Analyst role in the company earlier this year. For the past few months, I've been extremely active in self learning. At first, I learned some basic SQL and Tableau for my current job. I then watched a bunch of videos on youtube and udemy on Python, Pandas API and Spark. I managed to build a basic ETL tool that I use at my job to automate some of the things I do. I also managed to streamline the process of cleaning data using Pandas instead of working directly on Excel/Google Sheets.      After that, I've been reading posts on this sub to see what else I should learn. I read the popular DE book called ""Designing Data Intensive Applications"". I've tried reading the Kimball book after on DWH but I found it to be overwhelming as a newbie with zero knowledge on DWH so I took the 5 hour course on data warehousing on udemy instead. At least I have some basic knowledge DWH at this point. Then I started this AWS course, I kept seeing how people recommend the Solutions Architect one on this sub. I found it to be so overwhelming and frankly rather depressing. I found it so painful to go through a single 10 min video while I enjoyed the previous courses I took. I just got to the RDS section on the AWS and at least it's somewhat interesting now (cause database stuff)  but still, I feel like there might be a lot of concepts that I probably don't need to know yet and will likely forget anyway due to the sheer amount of information being thrown at you.  &amp;#x200B;  With all the things that you are supposed to learn to be a DE, my goal was to have some baseline knowledge at least on each of the tooling/technologies. Could anyone point me to the concepts or aspects I should focus on if I were to learn AWS ? Or would the link I posted be a good alternative course? I feel like if I do focus on taking this AWS course, I'm going to miss out on other aspects of DE. I still want to learn stuff like Airflow, Kafka, advanced SQL, advanced DWH...etc but I'm not sure what to focus on at this point. I do feel like learning a cloud platform is important even though it looks like it's role dependent. It looks like other mid level DEs do not even use a cloud platform at their point in their career.   While editing my post, I realize I may have possibly overdone my studying. But I am not sure, as it seems like there's always things to learn and to improve on. Every once in awhile I also read my notes from previous courses so I don't forget the things I learned. Any tips are greatly appreciated, thanks!",self.dataengineering
177,pnxw5u,energizer_87,https://www.reddit.com/r/dataengineering/comments/pnxw5u/rapidminderorange_vs_general_purpose_languages/,Rapidminder/Orange vs general purpose languages and SQL,[removed],self.dataengineering
178,pnxcar,barbapapalone,https://www.reddit.com/r/dataengineering/comments/pnxcar/does_anyone_know_of_de_hackathons/,Does anyone know of DE « hackathons »?,"I am curious to know if there are good online competitions for data engineers, have you ever participated in one?",self.dataengineering
179,pnvfue,Ok-Message1053,https://www.reddit.com/r/dataengineering/comments/pnvfue/why_use_airflow/,why use airflow,"I hear about Airflow all the time so I finally decided to watch a couple tutorials on it, but I'm not really sure I understand why it's so popular. I think sometimes it's hard to see the benefit from basic beginner tutorials. I currently use Google Scheduler to trigger a GH workflow, which starts my Docker container on GCE, which I really like. Can anyone give some thoughts on why Airflow might be preferable to my current method? I'm not sure if Airflow would be replacing these tools, or if it should be used in addition to what I'm using? Thanks",self.dataengineering
180,pnvfig,NotACardassian,https://www.reddit.com/r/dataengineering/comments/pnvfig/a_question_on_setting_up_airflow_docker_and_python/,"A question on setting up airflow, docker, and python",I decided to teach myself airflow and am trying to get a simple etl environment setup on digital ocean. I have installed airflow with docker using the install instructions in their documentation.   I created a dag and was able to see it show up in the web server and run it. Then I created a new venv so that I don’t mess with the docker environment with what would be my etl scripts.   I’ve tried making a dag that calls a bash operator to execute the python script but with the venv like in: $/path/to/venv/python /path/to/script.py but that is failing.   What I’m wondering though is whether this is the right approach at all? Is there a more common practice for setting up your scripting environment and airflow environment? Should I be doing everything in the docker venv?,self.dataengineering
181,pnvekv,sweetaskate,https://www.reddit.com/r/dataengineering/comments/pnvekv/sigmod_21_the_first_research_paper_focused_on/,[Sigmod '21] The first research paper focused on open-source vector database,"It seems that there are more and more companies adopting and developing vector databases from the posts I see from HN, but still not enough people explaining how it works and what it does.   So if you are interested in vector database you can take a look at this- Sigmod ‘21 Research Paper about the open-source Milvus vector database: [https://dl.acm.org/doi/10.1145/3448016.3457550](https://dl.acm.org/doi/10.1145/3448016.3457550)  It explains the design of the system, the technology used, and its applications.",self.dataengineering
182,pntt51,AMGraduate564,https://www.reddit.com/r/dataengineering/comments/pntt51/a_career_move_to_data_architect_from_data_engineer/,A career move to Data Architect from Data Engineer?,"I am working as a Data Engineer and I am about to be fed up with the inefficient ways things are being designed and implemented, not to mention the lack of consideration for governance measures. It seems to me that the Data Architect role is actually doing the System Design of an organization (in Australia) and moving to this role will give me the decision/policy making privilege that is not possible in a Data Engineer role.  I was wondering if it would be a good idea to change my career track to Data Architect from Data Engineer, and what consequence (positive or negative) I might face in the future for doing so?",self.dataengineering
183,pnro38,Se7enEl11ven,https://www.reddit.com/r/dataengineering/comments/pnro38/data_engineers_what_tasks_do_you_do_in_your_daily/,"Data Engineers, what tasks do you do in your daily job and what tools do you use? My job title is DE but I’m doing BI and need advice","Title basically says it. I have been trying to learn more about data engineering so I decided to join an internship program in DE this year. I’m still in uni (majoring in data science). I wasn’t given a lot of detail for this internship, they just told me I could join the data engineering team and they would give me training and I wouldn’t have to worry. So I decided to join as I’m still a student and thought this would look good on my CV and would help me gain practical knowledge. All I did through the program was developing reports in Tableau and other business intelligence tools for the executive team, had to use some SQL but that’s it. It’s been 3 months of doing this (tho the program is about to end). Thought I would be writing code with python, developing pipelines, work with a cloud provider, etc. Was I wrong about what a data engineer does? What do you guys usually do?",self.dataengineering
184,pnol2m,brownstrom,https://www.reddit.com/r/dataengineering/comments/pnol2m/data_profiling_for_a_large_dataset/,Data Profiling for a large dataset,I have a large dataset with about 200 columns and 100 million rows.   How do I do data profiling on something so large to come up with a significant analysis of the data which is useful?,self.dataengineering
185,pnneop,Ehinfo,https://www.reddit.com/r/dataengineering/comments/pnneop/conference_is_chief_data_analytics_officer/,Conference: Is Chief Data &amp; Analytics Officer Exchange Worth It?,[removed],self.dataengineering
186,pnmzao,A-Nit619,https://www.reddit.com/r/dataengineering/comments/pnmzao/import_python_package_to_aws_glue/,Import python package to AWS glue,^(Hey guys. I have written this python code that I wanna import to AWS glue. Is there a way to do that? Kindly advice.),self.dataengineering
187,pnmvkm,treacherous_tim,https://www.reddit.com/r/dataengineering/comments/pnmvkm/upskilling_in_api_service_development_as_a_data/,Upskilling in API / Service development as a data engineer,"I've worked for a while as a data engineer building out pipelines using Airflow, Spark, etc.. to produce datasets that feed applications and data science workflows. I have not gotten much experience in service development, but my team is going to be starting a project that heavily relies on taking data we're producing and exposing it to consumers via a REST API instead of just plopping some data out on a file store.  I can definitely go out a google for this, but I'm sure others in this community have had to upskill on this area, so wanted to see if there are any recommendations from others with a pipelining / data modelling background. I'll say I'm more interested in architecture / patterns as opposed to hands on development as I'm a lead, but will likely still get my hands dirty a bit. TIA!",self.dataengineering
188,pnlgxj,Away-Suggestion-5845,https://www.reddit.com/r/dataengineering/comments/pnlgxj/importing_data_question_kafka_vs_dwh/,Importing data question Kafka vs DWH,"Hi everyone, someone asked me a technical question about Kafka.    There is a platform which uses relational Database and also Kafka to communicate and receive order data(and updates) of the customers. Which of these method is a better approach:  An incremental load from the DB or importing data via Kafka into the DWH? What are advantages and disadvantages?   As i don't have any technical basics about Kafka i couldn't understand the question and respond it.  What do you understand from this question and how do you approach it?  Thanks you.",self.dataengineering
189,pnldw2,NullNoneNull,https://i.redd.it/x3yr1gwldbn71.png,Roast My Resume,,i.redd.it
190,pnixoi,CreativeAd6756,https://www.reddit.com/r/dataengineering/comments/pnixoi/soda_sql_vs_dbt_tests/,Soda SQL vs DBT tests,"Hi everyone,   Just wanted to know your experiences/thoughts around these above data testing tools.   Which one is better? We are already using dbt in our project so should we go with DBT tests?  Or Soda sql have more advantages over DBT tests?   Could you please help me to understand and choose one over another?",self.dataengineering
191,pni2ak,rmoff,/r/apachekafka/comments/pnhpra/kafka_summit_is_this_week/,🎉 Kafka Summit is this week 🎉,,self.apachekafka
192,pnhqti,Competitive-Cut-8051,https://www.reddit.com/r/dataengineering/comments/pnhqti/my_company_hired_an_old_man_as_data_engineer/,My company hired an old man as data engineer!,"I am fairly new in workspace (1.5 years) as data engineer after graduation. Work for a small company (60+) employees. My company hired a very old man (~50) years old as data engineer. He never worked on cloud, had little programming expertise. And was hired because he was doing online courses.  Anyway, i am responsible for training him about the work I do but It is getting really frustrating for me. I taught him things 3 or 4 times and he still doesn’t get it. While I go through stuff I literally have to tell him every single details such as click here click there (every time). I expect that after going through stuff once he would try to explore by himself. But every time I sit to help him all my time and energy is exhausted.   My questions is — is it normal? Am I taking it the wrong way? Would it be okay if I talk to my boss about this?",self.dataengineering
193,pne26t,cgdownbeat,https://www.reddit.com/r/dataengineering/comments/pne26t/are_microsoft_certifications_worth_it/,Are Microsoft Certifications Worth It?,"I keep seeing posts on LinkedIn saying that people have passed their certifications for Data Engineer associate/expert levels. But when I look to do it myself, it costs just over £100 to take one course/exam. Quite frankly that seems like a big sum of money to me for something that will probably only ever last about 3 years. Simply due to the fast development of the field we are in. But in that same 3 years working as a Data Engineer I feel like a certication would be redundant anyway.  So why would people do it?",self.dataengineering
194,pnd1wk,killer_unkill,https://www.reddit.com/r/dataengineering/comments/pnd1wk/de_interview_in_amazon_europe/,DE interview in Amazon Europe,"Hello Folks,   I have a scheduled interview for Data Engineering role in Amazon. What kind of questions I can expect in Phone screening ?    I have a more than a decade of experience working in traditional DWH/ETL (Informatica, Ab Initio, Teradara ) and currently working with Airflow and Snowflake.    Do I need to grind leet code ?",self.dataengineering
195,pncn0n,Vorskl,https://www.reddit.com/r/dataengineering/comments/pncn0n/productiongrade_sql_code/,Production-grade SQL code,"Hi all,  I am looking for a book/blog/youtube etc on what makes the production-grade SQL code? I.e. how amazons-alike deploy it in terabyte-scale applications.",self.dataengineering
196,pnc09t,meta-pirate,https://www.reddit.com/r/dataengineering/comments/pnc09t/data_platform_data_infrastructure_and_data_stack/,"Data platform, data infrastructure, and data stack. Same concept but different perceptions?","Yesterday I was discussing with a couple of friends (a data scientist and a data analyst) about terminology to better communicate with stakeholders, and although we all agreed that Data platform, Data Infrastructure, and Data Stack are often used interchangeably, these terms seem to lead to different perceptions:  * To the data scientist: data stack and data infra are pretty much the same, while data platform seems more for ingestion and move data around (i.e. tools like Segment and Snowplow) * To the data analyst: data stack feels the most complete and updated term, data platform seems specific to tools and sounds old, data infra seems the most low-end and most specific to DevOps/DataOps. * To me (PM): data infra seems the most complete, data stack seems the more high-level, mostly tool specific, and data platform seems the most business-friendly term with the focus on tools.  What are your thoughts?",self.dataengineering
197,pn87z0,therealiamontheinet,https://www.reddit.com/r/dataengineering/comments/pn87z0/new_streamsets_community_platform/,New StreamSets Community Platform,"We're excited to announce that we have launched our new StreamSets Community platform. We hope you will also join us on the new platform that allows you to easily engage with your peers, search for previous answers (unlike on Slack), share best practices, browse our Knowledge Base, and also earn badges! Please register at [https://community.streamsets.com/](https://community.streamsets.com/).  We hope to see you there!   Cheers,   Dash",self.dataengineering
198,pn5ipq,vananth22,https://www.dataengineeringweekly.com/p/data-engineering-weekly-55,"AWS PartiQL deep dive, Paige Berry's share your data insights to engage your colleagues, Pinterest analytics as a platform on Druid, Confluent's protecting data integrity in the confluent cloud, Databricks implementing more effective FAIR scientific data management with lakehouse, StarTree story of",,dataengineeringweekly.com
199,pn52p1,doversoledeli,https://www.reddit.com/r/dataengineering/comments/pn52p1/help_choosing_the_most_usable_tools_to_learn_data/,Help choosing the most usable tools to learn data engineering/analytics to get an entry level job and then transition into programming,[removed],self.dataengineering
200,pn4c08,Playba1133,https://www.reddit.com/r/dataengineering/comments/pn4c08/currently_an_analyst_looking_to_pivot_to_an/,"Currently, an Analyst Looking to Pivot to an Engineer Position","Hi All,   I'm sure this question has been asked in some form or another but want to understand what makes the most sense for my situation.   Background:  Graduated 5yrs ago with a Bachelors in Economics  Have 5yrs of work experience 4 as a Business Analyst and 1 as a Data Analyst. The four previous years were spent in the finance industry but for the past year, I've worked at a start-up where I've had the opportunity to work on more data-related things.  Skills: Able to use SQL, Python, and R fairly well and have recently started a project at work that leverages Airflow. I am also fairly comfortable working within the AWS ecosystem.   Certification: AWS Solution Architect  Next Steps: Would you recommend any of the following:  1. I really put my head down at work and grind in hopes of attaining an internal promotion.  2. Keep working but start trying to complete a master's online somewhere part-time.  3. Look to maybe gain some additional certifications (I was thinking the AWS Data Analytics one but open to suggestions)  Appreciate your time and let me know what you think.",self.dataengineering
201,pn347o,Diawhaties,https://www.reddit.com/r/dataengineering/comments/pn347o/is_data_engineering_recession_proof/,Is Data Engineering Recession Proof?,"Are data engineers relatively safe during economic recessions, or are they one of the first positions to be put on the chopping block? Did you guys notice a lot of layoffs/furloughs among data engineers?  I’m quite young and have not yet experienced a real economic recession. I’ve heard horror stories from older mentors who have been around much longer.",self.dataengineering
202,pn2tc5,MaxGanzII,https://www.reddit.com/r/dataengineering/comments/pn2tc5/amazon_redshift_research_project_minimum_and/,"Amazon Redshift Research Project : Minimum and Maximum Values by Data Type (white paper, PDF)","https://www.amazonredshiftresearchproject.org  https://www.amazonredshiftresearchproject.org/white_papers/downloads/minimum_and_maximum_values_by_data_type.pdf  The official Redshift documentation indicates minimum and maximum values for data types. These values contain factual errors, typographic errors, and errors of omission (some data types have no values given). The actual minimum and maximum values are presented. During investigation, it also became clear that the mechanisms used to connect to Redshift - `psycopg2`, `psql`, `ODBC`, etc - all seems to be performing significant data processing, and at times are getting it wrong, leading to behaviour such as the silent modification of inserted values and incorrect values being presented from `SELECT`. Finally, there appears to be a bug in Redshift’s handling of large `float8` values.",self.dataengineering
203,pn1hnx,sadcsgradthrowaway,https://www.reddit.com/r/dataengineering/comments/pn1hnx/how_to_begin_working_towards_working_as_a_data/,How to begin working towards working as a data engineer,"I've decided to spend the next several months just gaining any/all desirable skills to getting a job in software development. I know getting a high-level understanding of data engineering could take a year+, so I just want to get started on learning some relevant technologies that could lead to an entry level data analytics / backend dev position, where the experience - I believe - could lead to being a data engineer down the line.  The reason I 'think' data engineering is what I want to do is because I really enjoyed working with SQL databases and am currently taking a course with Hadoop which is pretty fun so far; I also like working with ML tools. Simply put, I just like working with and analyzing data. If my understanding of this career is all wrong, someone correct me and point me in the right direction please.  TLDR:  What courses/certifications/tools should a new grad learn to work as a data engineer (eventually)",self.dataengineering
204,pmy9v4,artmutation,https://github.com/denissa4/vlookup-custom-formula-for-web-API-JSON,"I’ve found a reallyuseful excel formula, which can vlookup API from excel. As simple as using formula =vlookupweb(link-to-api, field-to-query, time-out-in-seconds-between-calls, token-if-needed, body-for-post-or-empty-for-get-request)",,github.com
205,pmtenl,Delicious_Attempt_99,https://www.reddit.com/r/dataengineering/comments/pmtenl/data_warehouse_interview_question/,Data warehouse interview question,"Hi All,  In one of my recent interviews, I got this question - How do you build the data warehouse from scratch?  My question is -  What would be the sequence while answering this question?  Thanks in advance",self.dataengineering
206,pmsaxs,krishkarma,https://www.reddit.com/r/dataengineering/comments/pmsaxs/want_to_switch_to_data_engineering_domain_after_4/,Want to switch to Data engineering domain after 4 years of workex in SAP.,[removed],self.dataengineering
207,pmr1vh,NeverNotNowPlease,https://www.reddit.com/r/dataengineering/comments/pmr1vh/the_rube_goldberg_machine_of_etl/,The Rube Goldberg Machine of ETL,"Hi,  I have recently joined a project that does ETL. More precisely, they retrieve data from one system, cleanse it and transform the data model before finally loading it into the data warehouse.   My own experience with ETL is limited, which makes it difficult to assess the quality of their design. But I have a gut feeling that it may not follow best practice.  The design is like this:  1. Data is extracted from a source into tables on a schema     1. Contraints (same as source)    2. Keys (whatever combination of columns that works) 2. Data is cleaned and then moved to new tables on a new schema. 3. Data is transformed and moved to new tables on a third schema    1. transformation includes some changes to data values, concatenation of columns etc. 4. Lastly, data is moved to the DW. This is done with queries consisting of:    1. joins across all schemas    2. logic statement like case when and where clauses.    3. CAST functions to change datatypes.    4. renaming of columns  To me, this seems like an unnecessary complicated engine, which is very difficult to comprehend. What are your thoughts?",self.dataengineering
208,pmpnil,martypitt,https://www.reddit.com/r/dataengineering/comments/pmpnil/api_startup_founder_looking_to_understand_data/,API startup founder looking to understand data engineering problems,"Hi  I'm the founder of a startup called Vyne ([https://vyne.co](https://vyne.co)).  We build a intelligent data layer than can connect, integrate and transform data automatically.  We started out trying to solve API integration, (Vyne gives an automatic federated API gateway) but have had interest in people using Vyne in data pipelines, to handle the transformation automatically.  The data pipeline space (and data engineering in general) isn't a space I know well - I'm an API guy.  So I was hoping to connect with some data engineers to understand the problem space better, and how Vyne might play in this space.  This isn't a sales pitch - I'm trying to learn about how engineers solve the transformation space at the moment, and what the pain points are.  Anyone here open to spending 30 minutes with me on a Google Meet?",self.dataengineering
209,pmoukw,Born-Comment3359,https://www.reddit.com/r/dataengineering/comments/pmoukw/what_are_the_best_resourcestutorials_blogs_books/,"What are the best resources(tutorials, blogs, books) to prepare for a Data Engineer interview?",Any suggestions from people who have successfully passed DE interview would be appreciated.  Thans in advance.,self.dataengineering
210,pmleuu,Charithe,https://link.medium.com/Jp5yx2iUtjb,Metadata-driven data ingestion from Salesforce to Data Lake using Azure Synapse Pipelines,,link.medium.com
211,pml0zr,feirnt,https://www.reddit.com/r/dataengineering/comments/pml0zr/noaa_data/,NOAA Data,"Little rant: Am I the only one who's tried to navigate the incomprehensible miasma that is NOAA's sites/FTP repositories?   Exploring weather data is just a hobby for me. After years of piddling around, I finally found a clue to what I was looking for on some rando data science prof's site, and found a trail to something useful. NOAA's site tools were utterly useless.   Is there such a thing as an index that describes what data they have and where it is?",self.dataengineering
212,pmkh7u,Wonderful_Resource_1,https://www.reddit.com/r/dataengineering/comments/pmkh7u/what_are_the_most_important_concepts_anyone/,What are the most important concepts anyone should know about Data Warehousing and BigQuery?,"Hi,  I switched career from software engineering and quite new in this space. I know what database is, but I am still learning about data warehousing.  What are the top concepts (please give me 1-2 topics to study) I should know about data warehousing?  The company is also plan to use BigQuery since we are on GCP. Is there anything special about BigQuery that I should know about? My understanding is it's a serverless data warehousing service that we need to pay for storage and per query run.  Thank you so much everyone for your answer.",self.dataengineering
213,pmj0p7,pbj800100,https://www.reddit.com/r/dataengineering/comments/pmj0p7/anyone_working_two_jobs_at_the_same_time/,Anyone working two jobs at the same time?,My work is fully remote and my company is extremely flexible and we do mostly R&amp;D without real deadlines. I'd love to pick up a second part time job (to have two engineering roles) but I wonder if it's really feasible. I feel like I could easily add another 20 hours a week... I code things for fun in my spare time anyway so if the job was right (I love my current job) then I imagine it could work for me. I might be very much underestimating how hard this is though.,self.dataengineering
214,pmgbeb,mohityadavx,https://www.reddit.com/r/dataengineering/comments/pmgbeb/rotational_proxies_for_selenium_scraping/,Rotational Proxies for Selenium - Scraping Cloudflare Protected Website,"I am scraping a website that permits three successful requests and after that blocks that IP for 24 hours. Selenium is able to handle Cloudflare so far but I need a recommendation for a rotational proxy service that can provide me with a large number of reliable proxies.   A single page is about 1,500 KB and I intend to scrape about 10,000 of them in one day so need something economical and that does not charge on basis of data consumption.",self.dataengineering
215,pmg3la,nonkeymn,https://www.youtube.com/watch?v=SpaFPPByOhM,"Data Engineering Road Map - How To Learn Data Engineering ""Quickly""",,youtube.com
216,pmewpj,Va_Linor,https://www.reddit.com/r/dataengineering/comments/pmewpj/knn_k_nearest_neighbors_a_super_simple_algorithm/,kNN = k Nearest Neighbors: A super simple algorithm in Machine Learning,&amp;#x200B;  *Processing video 55yqc98tpxm71...*,self.dataengineering
217,pmeio7,O_its_that_guy_again,https://www.reddit.com/r/dataengineering/comments/pmeio7/im_looking_for_advice_on_a_beginner_data/,I'm looking for advice on a beginner data engineering project.,"Hi, I'm currently in a data science bootcamp and am trying to think through a few ideas for a capstone project. One is that it is more infrastructure focused than data science focused. For reference, I spent about 4 years in economic market research working with internal and external data via python and web scraping. I'm newer to SQL and the data engineering field in general, and hope to continue building and heading that direction upon graduation.  &amp;#x200B;  What are some suggested industry standard benchmarks or tools for engineering project given a 6-week project timeframe? In my research, I'm thinking a good project flow would include a scrape/ingestion of multiple text data sources (twitter) to a data storage, with deployment to a AWS server, and a sentiment analysis. I'm wondering if there is anything else I should consider.  &amp;#x200B;  We are going through SQL and Spark over the course, so I'll be using those to the extent that I can, along with python. But I'm curious as to what else I should consider in terms of benchmarks or tools?",self.dataengineering
218,pmeg4k,O_its_that_guy_again,https://www.reddit.com/r/dataengineering/comments/pmeg4k/advice/,Advice,"Hi, I'm currently in a data science bootcamp and am trying to think through a few ideas for a capstone project. One is that it is more infrastructure focused than data science focused. For reference, I spent about 4 years in economic market research working with internal and external data via python and web scraping. I'm newer to SQL and the data engineering field in general, and hope to continue building and heading that direction upon graduation.   &amp;#x200B;  What are some suggested industry standard benchmarks or tools for engineering project given a 6-week project timeframe? In my research, I'm thinking a good project flow would include a scrape/ingestion of multiple text data sources (twitter) to a data storage, with deployment to a AWS server, and a sentiment analysis. I'm wondering if there is anything else I should consider.  &amp;#x200B;  We are going through SQL and Spark over the course, so I'll be using those to the extent that I can, along with python. But I'm curious as to what else I should consider in terms of benchmarks or tools?",self.dataengineering
219,pmecrp,omkarkonnur,https://www.reddit.com/r/dataengineering/comments/pmecrp/sql_parsing_and_data_lineage/,SQL Parsing and Data Lineage,"Have been looking for a way to map data lineage across the project for all our ETL flows.  Although, I have been fairly successful with ETL tools, I have not yet been able to find a way to extract data lineage from SQL queries (which are usually written at the source side of the ETL process).   Any ideas on how to approach this? I tried using SQL parsers &amp; didn't really get a good one yet. Any pointers appreciated",self.dataengineering
220,pme7ne,forward-bat-8326,https://www.reddit.com/r/dataengineering/comments/pme7ne/types_of_de_role/,Types of DE Role,[removed],self.dataengineering
221,pmd4j0,Faintly_glowing_fish,https://www.reddit.com/r/dataengineering/comments/pmd4j0/what_is_the_best_channel_to_find_experienced_de/,What is the best channel to find experienced DE,"Hi folks, so I am a DE at a small startup but we are growing fast, so we need more DEs.  As a matter of fact, we have a goal of tripling the team by then end of the year.  Problem is, we cannot even get any right people to respond after trying various channels.  We tried LinkedIn, Angelist, and a lot of direct reach outs.  After directly reaching out to nearly a thousand people, from me, from our recruiters, Head of TA, even our CTO, just 10% of people have even bothered to respond and the handful of people that are willing to talk didn't want to actually switch jobs even before salary conversations, or were just not very qualified. On the other hand, we are getting people that are programmers but have no experience in data pipelines. But we need people to hit the ground running.  What are you guys experiences, and did anyone manage to find any channel to get hold on experienced DE?  Maybe we are reaching out to the wrong group of people?  Or should I just give up and get people with no experience and train them instead?",self.dataengineering
222,pmaywe,Boruroko,https://www.reddit.com/r/dataengineering/comments/pmaywe/is_aws_edu_good_to_get_started_as_an_aspiring_de/,Is AWS Edu good to get started as an aspiring DE?,"My first post here on this sub.  I´m still at college, and get more and more the idea that AWS is super important for DE, and in general in my future career path. Unfortunately, I´m blocked right now from creating a proper AWS account (no credit card).  I signed up for AWS Educate instead. There are different free courses there. Are they good to reach a sufficient level of AWS knowledge for a junior DE position?  Also, there are DS/ML course there. I see no DE course mentioned explicitly. However, there is this course 'Data Integration Specialist'. From the 'trailer', it kind of sounds like a DE direction.  What resources can you recommend me from there to acquire AWS DE skills? Thanks for your insights!",self.dataengineering
223,pmarue,Material_Cheetah934,https://www.reddit.com/r/dataengineering/comments/pmarue/is_this_the_most_outlandish_thing_youve_heard_in/,Is this the most outlandish thing you've heard in your DE career?,"## Background  Was recently hired in July to work on helping move data processing in a health care org's billing team to Azure. Hiring manager couldn't really comment on their existing infrastructure at the time citing *trade secrets*. Was assigned a TPM who the hiring manager said can also double as a dev if I really need it. TPM is all on olap cubes being the engineering marvel of the world, and Informatica being a better SSIS and as good as it gets. Whatever, just won't use him to help me code, who cares. After first month's worth of meetings, turns out this is a **SAS SHOP**, mainframe and EG 8.2.  ## Getting to the Outlandish thing Was pointed to something ""small"" that I can implement in Azure as a POC for viability + costs. After learning to read shitty SAS code, or interleaved SAS + SQL, I created a diagram(using Graphviz) for a complex pipeline. Of course nothing is documented except for in the code(no comments in code). About 80 different sets of transformations on 40+ tables resulting in a large data file of 400 cols x 300 mil rows for 1 month of data(its for regulatory reasons). Needless to say there is a dependency in there which resides in another group. The data they produce in SAS is used within this ""small"" pipeline. My manager says, well lets focus on that, it should be easier, since it is a subset only.  # THE outlandish thing Talking with the folks in the team where the dependency originates, it seems that they get data from some of the same sources as my prior assignment. Not bad, it should be somewhat recognizable. And they quality test the data before publishing, not bad at all, I can at least guarantee I am getting *quality* data. Now time to figure out how they quality test the data, because well they are using SAS mainframe. I figure they must ftp everything out and then use some kind of modern suite of tools to test.  **WRONG**, they use an excel spreadsheet in SAS mainframe. I ask, so its a manual data test? *No, its automated, it saves us alot of time* I asked for more details on how it all works together. Well, here it goes, the publish data into datasets(SAS format), then they have an intermediate process fire off in SAS to read from the excel file the ""rules"" which are encoded in the columns under specific tabs for specific data sets. The rules then get *applied* to their datasets and a report gets published, again in Excel, when a dataset doesn't meet acceptance criteria.  My follow up question, how do you make changes, and do you have any documentation on where to make changes when something upstream changes? *Now that you mention it, it is a pain to work with changes*.  **No shit sherlock**  I feel like I am being told to bite off more than I can chew...I cannot imagine even trying to read the SAS mainframe code that interleaves itself w/ the Excel sheet. WTF is wrong with people? I am waiting for Ashton Kutcher to jump out of somewhere and tell me that I've been Punk'd.",self.dataengineering
224,pmalqb,Boar2000,https://www.reddit.com/r/dataengineering/comments/pmalqb/work_are_letting_me_choose_what_skills_to_learn/,"Work are letting me choose what skills to learn, what should I do?","I was lucky enough to land myself a grad job at a medium sized data consulting company here in the UK.   While the company have certain tools and languages they want me to learn and use, they have also given me the opportunity to choose the skills and technolgies that I want to learn also.  Do you guys have any recommendations on what I should research/look to learn going forward?",self.dataengineering
225,pmafnw,ezio20,https://www.reddit.com/r/dataengineering/comments/pmafnw/building_data_pipelines_using_docker_and_skaffold/,Building data pipelines using Docker and Skaffold,"Hi Guys, could you please suggest any resource / blog / Youtube video/ book that can give a simple tutorial in building data pipelines using Docker ans Skaffold?",self.dataengineering
226,pm70ur,AbdullahMohammadKhan,https://www.reddit.com/r/dataengineering/comments/pm70ur/how_is_data_engineering_different_from_big_data/,How is data engineering different from big data ?,"So, I am a newbie in both of the fields. I was having a feeling that they both are same in terms of the tools that are used . Can anyone explain what are the differences in terms of the tools and  technologies that are used in works ?",self.dataengineering
227,pm62rs,Aaron-SWE,https://www.reddit.com/r/dataengineering/comments/pm62rs/is_jupyter_notebook_used_by_data_engineers/,Is Jupyter Notebook used by Data Engineers?,"Self-studying for a role in DE, and curious if Jupyter Notebooks are used.  I love using it when I'm working with Pandas so I can quickly and easily see what effect I'm having on the data frame.  But I'm guessing they wouldn't be much use in a production environment?  Do you use Notebooks, and then convert them to a .py script once complete? Do you avoid them altogether?  Just curious how this stuff actually works in a real work environment.",self.dataengineering
228,pm4rw9,MassiveSubstance9617,https://bestqualityandaffordableproducts.blogspot.com/2021/09/ibm-online-professional-certificate-in.html,IBM Online Professional Certificate in Data Science,,bestqualityandaffordableproducts.blogspot.com
229,pm1wll,vaitesh,https://www.reddit.com/r/dataengineering/comments/pm1wll/unable_to_connect_to_sql_server/,Unable to connect to SQL server,"Hey Fellas,  I have been trying to connect the adventures works DW to the Microsoft SSMS. but I am unable to connect it and it is throwing an error network related or instance specific error. More details in the image. I have selected the mixed mode authentication while installing the SSMS. Thanks in advance",self.dataengineering
230,pm16vw,hibluemonday,https://www.reddit.com/r/dataengineering/comments/pm16vw/career_advice_leave_full_time_analyst_position/,Career Advice: Leave full time analyst position for Big Tech DE contract position?,"Bit of background, I'm a data analyst in the East Coast with 2 YOE. I'm also pursing my Master's in Computer Science part-time. My initial plan was to quit my job a year from now to study full time and also search for DE or SWE internships.  I was recently contacted by a recruiter and have been interviewing for a 6 month DE contract position at a BigTech firm (rhymes with goober). I have good experience with the tech stack at said company and frankly think that it's a great fit. It's a growing team and contract extension/conversion is very much in play - obviously taking this with a big fat grain of salt.  I currently make $65k in a MCOL area and the DE position would equate to $110k if I'm there a year. No discussion of benefits yet (which will be through recruiting company) but I'm not expecting it to be too stellar given it's a contract.  I'm curious if anyone had thoughts on my particular situation and in contract DE positions in general? Even if this is short term, is it worth the jump for my career or should I stay put and pursue internships with my student status?  Aside from the lesser stability compared to full-time and the further discussion of benefits, I'm pretty awed at the opportunity but want to be levelheaded with this.",self.dataengineering
231,plv523,Tropical_Wasp,https://www.reddit.com/r/dataengineering/comments/plv523/is_my_job_scope_too_broad_or_is_this_normal/,Is my job scope too broad? Or is this normal.,"Hi All,  I started a software development role 18 months ago but the role itself has mutated into more of a data engineering/analyst role. I didn’t mind this transition, in fact I really like my current role.  Unfortunately, when I started the team had no data infrastructure to speak of.  My only issue is I don’t know how to ‘classify’ what I do in a singular job title and I’d like to either progress or get a salary increase in the near future.  Over the last 18 months I have done the following:    Created a Python tool that auto uploads files from network drives and legacy on premise systems to AWS S3.   Setup Snowflake ingestion steps (Stages, pipes, streams.)   Developed tasks that auto move data from staging tables (streams) to dimension tables.   Create views/facts - e.g. Joined different data sources to form larger enriched data sets to feed BI tools.  Undertaken story capture with users and delivered projects end to end (extraction, upload, transformation, modelling of data and dashboard design/implementation)   Create custom data sets in SQL for use in BI tools. For example - A user this week wanted to see how sales orders were slipping. So I had to compare historical records. (Also setup the historical tracking for sale orders too, because our crm wasn’t tracking certain key field) then took that data and developed a dashboard within tableau. I work with the user to verify their data along the way.  I really enjoy the last few stages of this. Building models in sql, enriching data bringing different sources together and presenting it in tableau. What would this job role be called? Is that more of an analyst role?  Any advice on role definition, work scope will be very much appreciated.",self.dataengineering
232,plulrs,IamWarmduscher,https://www.reddit.com/r/dataengineering/comments/plulrs/how_do_i_launch_the_airflow_ui_with_airflow_on_an/,How do I launch the Airflow UI with Airflow on an EC2?,"I have Airflow on an EC2 instance and I launched the webserver and scheduler. How do I open the UI for starting the dags? For example, on my local machine, I would type `localhost:8080` in my browser. What do I type in my browswer to get the UI for the EC2?",self.dataengineering
233,plslu0,QuirkySpiceBush,https://www.reddit.com/r/dataengineering/comments/plslu0/best_way_to_get_basic_cloud_experience/,Best way to get basic cloud experience?,"I've been a data engineer for the past decade, at a small state agency that has basically all on-premise servers. So my skills with traditional relational databases &amp; data warehouses are pretty decent, but I have basically zero experience with cloud technologies.  I've decided to look for a new job, and it's evident I need to acquire *some* kind of cloud experience that's relevant to data engineering, but the choices are bewildering. AWS, Azure, Snowflake - what is a platform or technology that will allow me to get my feet wet? Is there a certification I should pursue? Thanks!",self.dataengineering
234,pls679,Accomplished-Can-912,https://www.reddit.com/r/dataengineering/comments/pls679/resume_review_cant_get_any_call_back/,Resume review - can't get any call back,Can you guys help me with a resume review or show me how you guys have done yours . I noticed that I don't get any call back after sharing my resume.   Thanks,self.dataengineering
235,plp7bj,Repeat-or,https://gretel.ai/events/build-a-synthetic-data-pipeline-using-gretel-and-apache-airflow,Build a synthetic data pipeline using Gretel and Airflow,,gretel.ai
236,plnou5,SadelaPapita,https://www.reddit.com/r/dataengineering/comments/plnou5/salary_renegotiation/,Salary re-negotiation,"Hi,  I am DE with 2 YOE. I recently got offer from a Data Prep SaaS Company and I accepted it. I got another offer from Amazon with 20% more pay than the previous one.    I want to pursue the first offer, but compensation there are offering is less than what I can get. Should I re-negotiate with them ? If yes, please advise how to got about it.    In case they ask for my offer letter from Amazon, should I share it?",self.dataengineering
237,plmukq,Imaginary-Ad2828,https://i.redd.it/jx0bfb06yom71.jpg,Some of these job postings out there are absolutely hilarious. WTF is this jumbled mess of shit?,,i.redd.it
238,plmjrm,aCoolGuy12,https://www.reddit.com/r/dataengineering/comments/plmjrm/what_tools_can_replace_talend/,What tools can replace Talend?,"Hi, I have been assigned the work to migrate some jobs written in Talend, which copy data between Salesforce, Zuora, and a MSSQL database.  Is there any tool that comes to mind which can replace this setup?  I thought of Airflow+python scripts, but because there are not enough software engineers in the company, I am also required to think of an UI-based / more-friendly alternative to just python scripts...  Any ideas? Thank you",self.dataengineering
239,plmi06,neuralscattered,https://www.reddit.com/r/dataengineering/comments/plmi06/i_think_my_eyeballs_have_rolled_back_as_far_as/,I think my eyeballs have rolled back as far as they are physically able.,"Client: ""Here is a file with the changes you need to make. Please make them.""  Me: ""Ok, done.""  Client: ""OMG EVERYTHING IN OUR PRODUCTION IS FAILING. Here is a new file with the changes you need to make. Please make them. I will escalate if I need to.""  Me: ""Ok, done.""  Client: ""OMG IT DIDN'T GET FIXED. I'll get back to you about this when I get back from vacation.""  \&gt;:(",self.dataengineering
240,plko0g,drollerfoot7,https://www.reddit.com/r/dataengineering/comments/plko0g/how_to_automate_data_cleaning_with_data_bricks/,How to automate data cleaning with Data Bricks,"Hi,  &amp;#x200B;  I just started my job and I got put onto a project to create a framework. My task is to automatically clean the data in the bronze folder in the datalake and copy it to silver.   I honestly have no idea how to automate this. I asked my senior colleague and he said the following:  &amp;#x200B;  https://preview.redd.it/0anbcspkcom71.jpg?width=1435&amp;format=pjpg&amp;auto=webp&amp;s=60cbbc6e873980daea5fe1a4e4f2363fd5308281  The reason I'm asking you guys is because this time of year is very busy for him and I already interupted him so much in contrary to my other colleagues who started around the same time and are already pretty skilled.  &amp;#x200B;  If anyone can clear things up or maybe even give a better solution all advice is appreciated!",self.dataengineering
241,pljfsl,ckdatanerd,https://www.reddit.com/r/dataengineering/comments/pljfsl/best_ways_to_learn_advanced_databricks_pyspark/,Best Ways To Learn Advanced Databricks (Pyspark),"I am a new Data Engineer (3 months), and so far I am in love with my job. I have a lot of opportunity to work on what I want to, and some of the cooler projects I have gotten involved with use Databricks (pyspark — ETL pipelines, optimizing ML notebooks for production, optimizing cluster usage). I have found various sources online on my own (reviewed documentation, read the book Learning Spark 2.0), but I have been having trouble finding a book that covers more advanced pyspark utilization and dives into how to optimize different parameters. I also am interested in resources pertaining to Databricks cluster optimization. The book Learning Spark 2.0 has been immensely helpful for me so far, and I am in hope of finding an “advanced” version of it or other useful resources. Thanks!",self.dataengineering
242,pli6w5,Material-Bridge,https://www.reddit.com/r/dataengineering/comments/pli6w5/creating_an_end_to_end_object_detection_pipeline/,Creating an end to end Object detection Pipeline,I want to build a AI system that ingests X Ray images and classifies diseases in real time. It will eventually generate and provide a report about the severity of the disease to the user. Can you give me suggestions as to how I should approach this?   I was thinking about using a NOSQL db (Cassandra) to store the images that are gathered from an API. Then creating an automated pipeline which trains an Object detection algorithm which will classify the images. Would tools like MLflow be useful? Could really use some help over here.,self.dataengineering
243,pli3o3,peace_keeper977,https://www.reddit.com/r/dataengineering/comments/pli3o3/how_to_become_a_data_engineer/,How to become a data engineer ?,so what would be the best way to start off this route ? What specific skills are needed that make it different from data science/analysis ?,self.dataengineering
244,pli0og,MediumZealousideal29,https://www.reddit.com/r/dataengineering/comments/pli0og/skills_required/,Skills required,"I'm working as a Data engineer having 2 years experience. I am very good at spark. I know SQL, hdfs, hive, control M..I also learned airflow, sqoop and basics of spark streaming, Kafka and AWS. I want to apply for data engineer positions in Europe. If you're from Europe, please suggest me what are the areas the interview is more inclined towards in your country? It will be very helpful for me.",self.dataengineering
245,plhsuw,Marksfik,https://flink.apache.org/2021/09/07/connector-table-sql-api-part1.html,Apache Flink: Implementing a Custom Source Connector for Table API and SQL,,flink.apache.org
246,plhkf2,Snoo-89741,https://databand.ai/blog/how-to-track-metadata-with-airflow-cluster-policies-task-callbacks/,How to Track Metadata with Airflow Cluster Policies and Task Callbacks,,databand.ai
247,plgyrf,dadadawe,https://www.reddit.com/r/dataengineering/comments/plgyrf/test_cases_for_data_warehouse/,Test cases for data warehouse,"Hello,  I'm looking for some generic test cases for the dimensional part of my data warehouse (the star schema). Specifically test cases that can run automatically after each update to the model.  **Some cases I already know to implement:**  * Are all FK\* in Facts present in a Dimension? * Do all the FK in the Facts for a unique line? * Are any FK in the Facts null or missing? * Is each line in a Dimensions unique by its PK? * Transitivity: if I sum the same amount in two different facts over the same keys, do I get the same amount? * Timeliness: is the max repoting date in each table the expected date? * If I perform a specific operation (sum, count, ...), do I get the expected result, where expected result is computed from the source data (ex: count of unique customers, sum all of revenue, ...)   &amp;#x200B;  Any and all input very welcome!  &amp;#x200B;  \* By FK in Fact I of course mean each individual ID that links to a dimension",self.dataengineering
248,plg9j3,RstarPhoneix,https://www.reddit.com/r/dataengineering/comments/plg9j3/how_to_master_apache_airflow/,How to master apache airflow?,I am interested in learning apache airflow. I don't know from where to start learning. Can anyone recommend me any complete  tutorial series/YouTube  etc related to apache airflow? Any help is highly appreciated.,self.dataengineering
249,plg0kt,JB__Quix,https://www.reddit.com/r/dataengineering/comments/plg0kt/realtime_streaming_data_gemo_gamedemo/,Real-time streaming data GEMO (game+demo)!,"At Quix we're very proud of our last demo. Please go and check it! It **takes 60 seconds**, is actually pretty **fun** and a good showcase of how to stream data with our platform!    [https://quix.ai/data-stream-processing-example/](https://quix.ai/data-stream-processing-example/)    For more info on what's going on behind the scenes check [this blog post](https://quix.ai/data-stream-processing-example-explained/). Any feedback is really appreciated :)",self.dataengineering
250,plczpk,distributeddataframe,https://www.reddit.com/r/dataengineering/comments/plczpk/best_conferences_and_events_for_data_engineering/,Best conferences and events for Data Engineering?,"Hi, first time posting here. I was wondering which conferences and events do people generally to attend to find out about the latest and best data-engineering tools and libraries?",self.dataengineering
251,plbjqi,mars_urge,https://www.reddit.com/r/dataengineering/comments/plbjqi/recommendations_for_elasticsearch_alternatives/,Recommendations for ElasticSearch Alternatives,"I am currently using ElasticSearch for quick aggregations based off of boolean logic searches. The main problem I'm facing is refreshing the main index (1.4TB and growing rapidly). My approach is to reindex the full set of data every 24 hours (at night) so that I can include the newest data, remove some selected old data and perform the reindex so that the freshest set of data is available by morning.  I'd like to move towards some alternative tool that lets me query the data the same way ES does getting sub second response to queries with basic boolean logic. (example: document contains X AND (Y OR Z) but not A) . Ideally I would be able to insert and delete records to make the data refreshes near real time as opposed to my current approach.  Anybody have any suggestions?",self.dataengineering
252,plaxd7,77_65_61_73_65_6c,https://www.reddit.com/r/dataengineering/comments/plaxd7/what_it_skills_do_i_need_to_know_if_i_want_to_be/,What IT skills do I need to know if I want to be a data engineer?,"If none, what would be helpful? I have some experience with the basic services of AWS (EC2, S3, Batch, DynamoDB, etc.). I also program pretty well (mainly Python and a bit of JS at my job)",self.dataengineering
253,pl9n2x,DatKalvin,https://www.reddit.com/r/dataengineering/comments/pl9n2x/is_this_architecture_cost_effective_performant/,Is this Architecture cost effective &amp; performant? What's your suggestion.,"Hi everyone, I work as a DE for a startup in the fintech/crypto currency space. We're currently designing our data pipeline architecture to replicate data from our RDS(MySQL) production database to Snowflake. We want a solution that is cost-effective  and will give us near real-time data delivery. Some micro-batching is fine. Our data size is not huge, about 400GB.  We're currently on AWS.  Our planned architecture is: RDS(MySQL) stream to S3 in parquet format using AWS Database Migration Service as new records becomes available. Then Snowpipe picks up data from S3 based on event notification from S3, put in a queue and load to Snowflake data warehouse in a stream format. We are also thinking of Kinesis Firehose replacing DMS but haven't figured out the possibility of this. Do you see any problem with this design with respect to performance &amp; cost?   What would you advice?  Any thoughts/suggestions will be much appreciated.",self.dataengineering
254,pl7srk,johne898,https://www.reddit.com/r/dataengineering/comments/pl7srk/hudi_vs_delta_lake/,Hudi vs Delta Lake,What are people picking these days?,self.dataengineering
255,pl773l,Puzzleheaded_Rub2568,https://www.reddit.com/r/dataengineering/comments/pl773l/data_entry/,Data Entry,What options are there for creating forms for teams to enter data? I’m sure most people run into the issue where business users have Excel files that serve as make shift data entry applications. Where they use a data extract on a worksheet to serve as drop down values in some fields and have made up values in other fields. What’s the best way to help transition that data entry to an enterprise solution. Auto-fill off of database fields is often a request to have in these form selections.,self.dataengineering
256,pl6o15,priyasweety1,https://www.reddit.com/r/dataengineering/comments/pl6o15/sample_glue_etl_projects/,Sample Glue ETL projects,Does anyone have any step by step ETL projects of glue right from raw to consumption layer  Any links or references,self.dataengineering
257,pl5blv,putinwhat,https://www.reddit.com/r/dataengineering/comments/pl5blv/ingesting_data_into_delta_lake/,Ingesting data into Delta Lake,"After reading through a lot of the documentation I'm still pretty confused as to how Delta Lake ingests data. I think the terminology of ""Delta Lake is an open source storage later that sits on top of your data lake"" is what's mixing me up.   If I want to use Delta Lake in Azure, would I first write raw data into an Azure data lake, and then ingest it into Delta? Or should I ingest into Delta straight from the source? The data is a mix of batch and streaming at the moment coming from Kafka.",self.dataengineering
258,pl3pos,TheLoveBoat,https://www.reddit.com/r/dataengineering/comments/pl3pos/im_a_data_engineer_how_do_i_become_a_better/,I'm a Data Engineer. How do I become a better Software Engineer?,"I became a DE after working in analytics and gravitating towards building infrastructure. I work with the modern data stack, so I'm comfortable building data pipelines using tools like Airflow and dbt. I've also worked on CI/CD, data quality, and observability within data.  However, I still have a large skill/knowledge gap when it comes to software engineering fundamentals.      I can write DAGs and data models, but when it comes to spinning up new services, provisioning infrastructure, or debugging complex software issues, I'm much less comfortable.  I recognize that since I can code and know my way around the stack, I'm starting from a good baseline. For someone with me background, what is the best way to learn software engineering?",self.dataengineering
259,pl2qxc,py_vel26,https://www.reddit.com/r/dataengineering/comments/pl2qxc/business_analyst_to_data_engineer/,Business Analyst to Data Engineer,"Hello,  My goal is to one-day be a data engineer; however, I don't have any practical I. T. experience.  I have completed some data analyst projects as well as a data engineer project.  Today I got a good job lead for Business Analyst and the pay is amazing.  I was told the Business Analyst uses SQL but I'm sure I wouldn't use it much.  I researched and found out the company does have Data Analyst and Data Engineer roles.  I guess I'm wondering If I'm wasting my time with this role, even if the pay is good.  It's not that technical.   Any thoughts?",self.dataengineering
260,pl1coz,alfie1906,https://www.reddit.com/r/dataengineering/comments/pl1coz/recommendations_for_free_online_mysql_hosting/,Recommendations for free online MySQL hosting?,"I'm trying to find a provider that I can use to host a MySQL database online. I'm working on a local Python app that I'd like to be able to interact with the DB, but lots of the free hosting plans I find don't allow for remote access, which is what I need.  The project is for a client that plans on using the app locally on different machines, otherwise, I would just set up a local MySQL server that he could use. I'd really like it if all instances of the app could share the same DB as it would just make life much easier! I also don't know much about cloud/web development, so I'm trying to avoid hosting the app online or one of the more complicated cloud solutions for a DB.  So my question is does anyone know of a free MySql hosting service that allows remote access? I've already had to pitch my client the idea of using a remote DB to reduce the workload that the app is doing locally, so I'd rather not have to now try and persuade him to pay for a service or it may put him off the idea completely",self.dataengineering
261,pl0j5w,Godmons,https://www.reddit.com/r/dataengineering/comments/pl0j5w/changing_datawarehouse_model/,Changing Datawarehouse Model,"Hi everyone,     I recently joined a company as a Data Engineer (Analytic oriented), and at my suprise, they don't have much logic behind their current Datawarehouse model.  What I mean is that they do not have a clear schema or way to modelize their data (either with Kimball-like approach or anything) , which leads to a lot of different kind of datasets (more or less aggregated / raw) loaded into the Datawarehouse.  I feel like there is a huge need of structuring &amp; refactoring the data model, BUT,  THE PROBLEM HERE : Lot of their current dashboards are built on top of this old datawarehouse model. Lot of work would be needed to change pipelines &amp; dashboards.  I feel like our C-levels (CEO, COO) are waiting a lot of productivity from our Analytic team, and we will need to discuss with them in order to make room (especially time) to do things right.   Do you guys have any tips or experience to share about those kind of situations ? I would be glad to hear them,     Thanks in advance for you incoming inputs and have a good day :)",self.dataengineering
262,pkxfd4,secodaHQ,https://www.reddit.com/r/dataengineering/comments/pkxfd4/rethinking_the_data_catalog/,Rethinking the data catalog,"Sturgeon's law states that 95% of everything is crap.   Nowhere is that more true than the data world. Most dashboards are wrong, most tables are trash, most ETL is broken, and most metrics are incorrect.   For some reason, most metadata management tools choose to solve this problem by trying to catalog this encyclopedia; they're building a library. But in reality, most of the books are just bound-up junk mail and shouldn't be cataloged in the first place. To solve this problem, opinionated data discovery powered by search is the key.  Over the last six months, our team at Secoda has been making data accessible to people using the modern data stack. We believe that every company will need a data discovery tool in the future. Existing tools have been able to deliver value to data teams, but have not cracked the elusive data discovery problem because of their focus on technical cataloging, instead of cleaning the overwhelming mess and fragmentation in our current data stack. After speaking with tons of data teams, a few things became clear to us:   1. Existing tools are not comprehensive in the data knowledge that they capture. They traditionally focus on only one area of data knowledge, which is technical metadata.  2. Data discovery tools should reduce the number of questions data teams get from people trying to use company data 3. Existing data discovery tools are built for technical users, but aren’t intuitive for non-technical users. 4. Existing tools combine curation and consumption into one view.  We found that teams are looking for a tool that captures all data knowledge in one place and reduces the number of questions data teams get from non-technical teams. The tool should focus on the business users because they are the source of many discovery questions and have few options to find the answer for themselves.   Since coming to this understanding, we’ve made some significant changes to our product. The first significant change is that we don't think of Secoda as a data catalog. Instead, we think of Secoda as a knowledge management tool that helps data teams share metadata, charts, queries, and documentation with any employee. Data teams are creating more data at an earlier stage, and are looking for a way to organize this knowledge and share it with employees. If you want to read about how we think about the ideal data discovery tool and how Secoda will continue to try and address the insights we’ve gathered from data teams, here's the full length article outlining how we think the data discovery layer should fit into your data stack: [https://www.secoda.co/blog/rethinking-the-data-catalog](https://www.secoda.co/blog/rethinking-the-data-catalog)",self.dataengineering
263,pkwx94,Vabaluba,https://www.reddit.com/r/dataengineering/comments/pkwx94/is_this_group_moderated_actively_can_it_be/,Is this group moderated actively? Can it be improved?,"As the title says. I love this sub, but not sure if it's just me, but everyday there are new posts created with people asking same questions (I.e. how to get in, get a job, doing x,y,z what are my chances) which creates a lot of unnecessary spam.  Are mods looking into organising the sub and moderating it to avoid this? For example, create weekly/daily thread of 1. Help on Tasks; 2. Help on entering field, etc.   I am happy to help the mods (or join them) if help is wanted/needed. Or am I missing the point somewhere and my expectations are not relevant here?",self.dataengineering
264,pkwd1a,MiyagiBi,https://i.redd.it/wq2zmnizvgm71.jpg,Data engineering on Azure,,i.redd.it
265,pkw1yb,short_n_ugly,https://www.reddit.com/r/dataengineering/comments/pkw1yb/please_recommend_good_text_based_learning/,"Please recommend good text based learning resource to learn distributed systems , Hadoop and Apache spark using Scala ?","I prefer to learn via reading, so looking forward to text based learning resources.  Something like mozilla guide for web developer. Please help .",self.dataengineering
266,pktsrr,Daneil-kishan,https://www.reddit.com/r/dataengineering/comments/pktsrr/can_firebolt_replace_snowflake/,Can firebolt replace snowflake?,"I know it is too early to judge firebolt. But we are planning to go for cloud DW and snowflake was the firth thing popped up, but then firebolt suggested and mainly because of the pricing - snowflake cost way higher than firebolt but I understand that it has lot offer than firebolt.  But I just wanted to know is there any possibility that firebolt would be good or better to stick with snowflake, and I cannot find any good document from firebolt side to compare these two products. So I can present something to the team.  (We use tableau as our analytical platform and our database is SQL server so any suggestions would be very welcome) Thank you 🙂",self.dataengineering
267,pktcp3,noNSFWcontent,https://www.reddit.com/r/dataengineering/comments/pktcp3/can_someone_help_me_understand_the_difference/,Can someone help me understand the difference between the the docker-compose files?,"So I was doing a few follow along tutorial for Apache Airflow with Docker.   I was stuck on my second tutorial.   The main difference I faced between the first and the second tutorial is that the first docker compose file just fired up instantly and Airflow was up and running.   The second docker compose file was stuck installing tons and tons of dependencies which never seemed to end.    **The first docker compose file is  -**           version: '3.9'               #asking for a postgres database     services:       postgres:         image: postgres:9.6         environment:           - POSTGRES_USER=airflow           - POSTGRES_PASSWORD=airflow           - POSTGRES_DB=airflow         logging:           options:             max-size: 10m             max-file: ""3""               #asking for a webserver       webserver:         build: ./dockerfiles         restart: always         depends_on:           - postgres         environment:           - LOAD_EX=n           - EXECUTOR=Local         logging:           options:             max-size: 10m             max-file: ""3""         volumes:           - ./dags:/usr/local/airflow/dags           # - ./plugins:/usr/local/airflow/plugins         ports:           - ""8080:8080""         command: webserver         healthcheck:           test: [""CMD-SHELL"", ""[ -f /usr/local/airflow/airflow-webserver.pid ]""]           interval: 30s           timeout: 30s           retries: 3   For the above docker compose file, the dockerfiles folder had Dockerfile with the following text -       FROM puckel/docker-airflow          RUN pip install requests     RUN pip install pandas  **The second docker compose file is -**         version: '3'     services:       postgres:         image: postgres:9.6         environment:           - POSTGRES_USER=airflow           - POSTGRES_PASSWORD=airflow           - POSTGRES_DB=airflow         ports:           - ""5432:5432""            webserver:         image: puckel/docker-airflow:1.10.1         build:           context: https://github.com/puckel/docker-airflow.git#1.10.1           dockerfile: Dockerfile           args:             AIRFLOW_DEPS: gcp_api,s3             PYTHON_DEPS: sqlalchemy==1.2.0         restart: always         depends_on:           - postgres         environment:           - LOAD_EX=n           - EXECUTOR=Local           - FERNET_KEY=jsDPRErfv8Z_eVTnGfF8ywd19j4pyqE3NpdUBA_oRTo=         volumes:           - ./examples/intro-example/dags:/usr/local/airflow/dags           # Uncomment to include custom plugins           # - ./plugins:/usr/local/airflow/plugins         ports:           - ""8080:8080""         command: webserver         healthcheck:           test: [""CMD-SHELL"", ""[ -f /usr/local/airflow/airflow-webserver.pid ]""]           interval: 30s           timeout: 30s           retries: 3       When the second docker compose was stuck in a dependency hell, I noticed both were pulling the same Airflow puckel image.   So, I tweaked the second docker compose to looked like the first one and it first right up. Any ideas as to why this happened? Did certain things not get installed ?",self.dataengineering
268,pkt2us,vy52,https://www.vcloudata.in/2021/09/big-data-analytics.html,Big Data Analytics,,vcloudata.in
269,pksnth,Engineering-Design,https://www.reddit.com/r/dataengineering/comments/pksnth/gcp_education_google_vs_coursera/,GCP education: Google vs Coursera ?,"For a data engineer who is experienced with coding and SQL and Azure, though unfamiliar with GCP, which training paths would you recommend ? Time invested more important than the cost.  1. Official Google training [https://cloud.google.com/training/data-ml#data-engineer-learning-path](https://cloud.google.com/training/data-ml#data-engineer-learning-path) 2. Coursera training (""Google Cloud Big Data and Machine Learning"") [https://www.coursera.org/learn/gcp-big-data-ml-fundamentals/home/welcome](https://www.coursera.org/learn/gcp-big-data-ml-fundamentals/home/welcome)",self.dataengineering
270,pkqvvu,Remarkable-Use6337,https://www.reddit.com/r/dataengineering/comments/pkqvvu/looking_for_some_advice_from_data_engineers/,Looking for some advice from data engineers,"I have been practicing my SQL and Python skills online using Stratascatch, Leetcode and Codesignal. They state that they are from actual real world interview questions, but how good are they?  I was able to code almost all of their easy, medium level questions, and for the difficult level, i needed help from youtube but was able to understand the logic and such.. and still practicing everyday.  I have a bachelor of math degree and had been working as a staff accountant. I do not have any experience related to data engineering, but actively looking for an opportunity, but it is so hard. What i am seeing is that most data engineer positions require experienced employee somewhere like 3+ years, and it is hard to find entry levels.  Do i actually have a chance in landing a career with my current skills.. Looking for honest opinions and advices. I just do know where i am at, and where i should go you know what i mean",self.dataengineering
271,pkqv9x,aadityarock2000,https://www.reddit.com/r/dataengineering/comments/pkqv9x/roadmap_for_an_engineer_to_data_engineering_jobs/,Roadmap for an Engineer to data engineering jobs,"I am in my final year of electronics engineering and I wanted to shift to data science/ data engineering roles. I am working on research projects in Deep learning and realised that I also am interested in data engineering type roles too.   As a non Computer science Engineer, what would be the path (subjects to read) to get into a good data engineering role? I know SQL knowledge is primary, but what are the others? what is the **order** in which one should read them?",self.dataengineering
272,pkqv0t,Remarkable-Use6337,https://www.reddit.com/r/dataengineering/comments/pkqv0t/online_coding_practices_like_stratascatch/,"Online coding practices like Stratascatch, Leetcode, and Codesignal, how good are they?","I have been practicing my SQL and Python skills online using Stratascatch, Leetcode and Codesignal. They state that they are from actual real world interview questions, but how good are they?  I was able to code almost all of their easy, medium level questions, and for the difficult level, i needed help from youtube but was able to understand the logic and such.. and still practicing everyday.  I have a bachelor of math degree and had been working as a staff accountant. I do not have any experience related to data engineering, but actively looking for an opportunity, but it is so hard. What i am seeing is that most data engineer positions require experienced employee somewhere like 3+ years, and it is hard to find entry levels.  Do i actually have a chance in landing a career with my current skills.. Looking for honest opinions and advices. I just do know where i am at, and where i should go you know what i mean",self.dataengineering
273,pko8di,Aspiring_DE,https://www.reddit.com/r/dataengineering/comments/pko8di/azure_sql_vs_snowflake_database/,Azure SQL vs Snowflake Database,"Hello everyone, I'm trying to get your view on Azure SQL as well as Snowflake. I see snowflake a lot on job postings these days which makes me think it's more popular. In contrast I've never heard of Azure SQL. At my company, we're in the early stages of a project to consolidate parts of multiple SQL databases into a cloud Database. Our current database is in MS SQL and our company infrastructure is all Microsoft. I've not used snowflake or Azure SQL. But I'm wondering if there is a reason for snowflake's popularity. How easy/hard is it to maintain a cloud database? How do these two stack up against each other? Any insight is appreciated.",self.dataengineering
274,pknfs8,fatredbeaver,https://www.reddit.com/r/dataengineering/comments/pknfs8/advice_on_reflect_changes_to_contents_of_database/,Advice on reflect changes to contents of database based on refreshed scraped data,"Hi all! I have a particular use case for a web scraper I'm building which I can't quite figure out the best practice for.   Currently, I am scraping a website that updates its data on a semi-regular basis, say, every 5-15 minutes. As such, I've scheduled a scrapy spider to crawl the site every 2 minutes or so. The amount of data scraped is not too huge, so that isn't an issue. The data scraped is cleaned and inserted into a Postgres database using SQLAlchemy as a basic interface (nothing too complicated, similar to what you can find in tutorials).  However, I have a particular use case where I'd like to **highlight changes** (to end-consumers of the data scraped and stored in the database). The reason is because the data I am scraping is supposed to have the same values over time (think: something like the start time of an Olympics event). Hence, any changes should be rather ""alarming"" and alerted to end-users. Meaning, suppose the following is the initial state of the data  data_name | value | website_update_time        A               1             09:00GMT        B               0             09:00GMT  And on the next scrape the value of A changes to  data_name | value | update_time        A               2         09:02GMT        A               1         09:00GMT        B               0         09:02GMT        B               0         09:00GMT  How should I best reflect this change of A? Is this something that is best handled on the SQLAlchemy/Scrapy end? Or perhaps on the database end? e.g. adding a new column, say, `current_value` and `previous_value`.  I am not sure if I am overthinking this. If it is not clear at the moment, please feel free to clarify and I will try my best to provide as much information as possible. Any advice would be greatly appreciated!",self.dataengineering
275,pkmu6e,chillatillum,https://www.reddit.com/r/dataengineering/comments/pkmu6e/etl_using_aws_lake_formation_question/,ETL using AWS Lake Formation question.,"Hi,     I have a database in RDS/ Aurora that would serve as some of my fact data. This data is a comparatively small part of the whole database and I'd typically get that via an SQL query with a bunch of joins and where clauses. I'm looking into whether I should build ETL myself (as is our current default) or whether now is the time to use some of the technology that's out there instead. My goal is to end up with files that represent the data that I would otherwise get from my SQL with some light row processing...     Say I want to use AWS Lake Formation... it looks like BluePrints would be convenient to use, and I could create a \`database snapshot\` for the initial load and then \`incremental database\` for new records/ changes. Both should include the tables that I would otherwise include in my SQL query and then I assume I'll have to do processing (using e.g. AWS EMR or DataBricks) on these output files to get to the data I'm really interested in.      Does that sound about right/ the way people do things? I assume because I want to track changes on the database directly I'm pretty stuck with tailing entire tables and do the joining later? Or would there be a way (using Lake Formation or Glue) to do this without having to track a bunch of data I'm not really interested in?",self.dataengineering
276,pkmggc,datanoob2021,https://www.reddit.com/r/dataengineering/comments/pkmggc/streaming_pipeline_question/,Streaming Pipeline Question,"I have built batched pipelines in the past and the invoking of the process was usually triggered by AirFlow or ECS.   If I am building out a streaming pipeline using Firehose on AWS, I am for some reason having trouble visualizing how the process gets kicked off. Do I write my Python script to continually run/listen for new data? Do I just have Airflow constantly run it over and over?  I know AWS has tools like step functions but I was hoping to use some free tools like GreatExpectations and other libraries to check data before sending it over to Firehose/S3/Redshift.",self.dataengineering
277,pkmbdl,doversoledeli,https://www.reddit.com/r/dataengineering/comments/pkmbdl/need_assistance_with_finding_the_right_program_to/,Need assistance with finding the right program to help transition into data engineering/programming/analytics in Ontario,[removed],self.dataengineering
278,pkkrc7,Buremba,https://metriql.com/blog/2021/09/07/olap-in-modern-data-stack/,OLAP in modern data stack: metrics layer,,metriql.com
279,pkjdeb,rishis18,https://i.redd.it/v8d45wabl4m71.png,Data Engineering Roadmap,,i.redd.it
280,pkidc9,IntellijentAspect,https://www.reddit.com/r/dataengineering/comments/pkidc9/separating_production_data_from_analytical_data/,Separating production data from analytical data?,[removed],self.dataengineering
281,pkgol5,themeansquare,https://www.reddit.com/r/dataengineering/comments/pkgol5/a_new_sub_for_ml_engineering/,A new sub for ml engineering," Good day everyone and we hope you're all doing ok.  we felt a vacuum for a sub dedicated for ML engineering on Reddit. ML engineering as in ""application of ML' in real world. We are sure that a lot of people here do ML engineering as a job and they will be interested in having a place to share articles, ask questions, and in general, have a chill time with their passion.  /r/ML_Eng focuses on the following:   \- Application of Machine learning   \- Implementing papers   \- SMACK Stack and similar data pipeline tools   \- Databases   \- Model deployment   \- DevOps related to ML   \- Creating frontends for your model   &amp;#x200B;   /r/ML_End is a place where intermediate to advanced programmers who aren't PhDs in ML can feel welcome. Anyting regarding application of ML is welcome. So join us there and we hope it all will be for the better!",self.dataengineering
282,pkdprn,Thriven,https://i.redd.it/szcfz1x81bm71.jpg,2nd time this week and it's Wednesday,,i.redd.it
283,pkde18,deveid,https://www.reddit.com/r/dataengineering/comments/pkde18/refresh_hive_tables/,Refresh Hive tables,How can I refresh hive tables using python,self.dataengineering
284,pkd8uk,deveid,https://www.reddit.com/r/dataengineering/comments/pkd8uk/how_to_validate_a_table_refresh_in_hive/,How to validate a table refresh in hive,"I have tables in a hive table. I would like write test cases to validate if the tables refreshed. Using pyspark, but not sure what cases I can check for. How can I go about this using python?",self.dataengineering
285,pkcb4i,yorzz,https://www.reddit.com/r/dataengineering/comments/pkcb4i/new_incoming_data_engineer_how_to_prep_myself/,New incoming data engineer. How to prep myself before going into the new job?,"Hello there! I have accepted an offer recently for a data engineer position at a tech consulting company. My experiences are limited with about 10 months as a backend software engineer using mostly python.  I will have about a month of downtime when I resign from my current position and start the new job. I would like to get some ideas on what to ‘self-study’ during that timeframe to prepare myself going in.  My sql is pretty basic, so I was thinking brushing up on it, but I have tried datacamp in the past and had not much of a success. What would be a good resource to brush up on sql for beginners? What else should I be focusing on? Database/ data modeling maybe? Any recommendations would be very much appreciated!",self.dataengineering
286,pkar0h,Ok-Sentence-8542,https://www.reddit.com/r/dataengineering/comments/pkar0h/is_terradata_any_good_compared_to_other/,Is terradata any good compared to other competitors like snowflake or databricks?,"Hey guys  I was wondering is [terradata](https://www.teradata.com/) any good compared to other vendors like [snowflake](https://www.snowflake.com) or [databricks](https://www.databricks.com)? Does anyone have experience with it? In this [gartner](https://www.teradata.com/Resources/Analyst-Reports/2020-Gartner-Magic-Quadrant-for-Cloud-Database-Management-Systems) report it was named a ""leader"" but what actual capabilities do they have for data analytics ml and so on? And what's their architecture?  &amp;#x200B;  Cheers",self.dataengineering
287,pk8jgp,twopairisgood,https://lakefs.io/thoughts-on-the-future-of-the-databricks-ecosystem/,Thoughts on the Future of the Databricks Ecosystem,,lakefs.io
288,pk78fp,ClumsyRooster,https://www.reddit.com/r/dataengineering/comments/pk78fp/airflow_spark_other_tool/,"Airflow, Spark, other tool ?","Hello!  I'm a software engineer transitioning to DE in my company. I read lots of resources about what I need to know to make this transition. And, wow! There are lots of new stuff to learn, that's awesome :)  I work for a small company and we don't have terabytes of data (our DB is less than 50Gb). We have lots of projects in mind with it: from sales analytics to machine learning.  But, I'm a lone data guy (even then, I don't consider myself like this for now :) ) without much experience in this field.  &amp;#x200B;  So here's the thing: I didn't want to jump directly into AWS/Azure/GCP for now, because I don't know their tools very well.   What I did first, is simply use Apache Airflow v2 because it looked like I could orchestrate some scripts (I mean DAGs, but they looks like scripts to me, I'm probably doing something off!). After more research, it looks like using only Airflow for ETL is wrong and I shouldn't do that (why though ?). So I took the time to use Airflow + Spark. But to me, it only feels like I'm just adding more complexity. As I said, we don't have lots of data (for now at least), if I handle something like 5Gb per ETL, that's the maximum.  So, I just need advice. Is it wrong to only use Airflow (2.1.3) for ETL ? Should I build a more complex infrastructure because I don't know how much data I will have to process tomorrow ?  What's the advantage of using Spark instead of Airflow for small processing ? Is there some simple architecture to begin with without being blocked ?  Thanks a lot for reading, I tried to find answers in this subreddit, I found some very interesting stuff but not all the answers I need.",self.dataengineering
289,pk60gp,TheInsaneApp,https://i.redd.it/v8d45wabl4m71.png,Data Engineering Roadmap,,i.redd.it
290,pk49eh,HovercraftGold980,https://www.reddit.com/r/dataengineering/comments/pk49eh/anyone_here_a_de_at_accenture_and_care_to_share/,Anyone here a DE at Accenture and care to share their experience?,Ie what you do /how you like it / hours / what it means to be a DE in consulting ie so you have to make ppts and client face a lot or what exactly do you do that may differ from industry ?,self.dataengineering
291,pk3lvp,Crafty-Prior-9785,https://www.reddit.com/r/dataengineering/comments/pk3lvp/how_much_to_trust_glassdoor_reviews/,How much to trust Glassdoor reviews?,"I just got an offer from a series b startup that is a 30 percent raise from my current position. However, the Glassdoor reviews are horrendous.",self.dataengineering
292,pk2xv3,AmDprimer,https://www.reddit.com/r/dataengineering/comments/pk2xv3/just_some_motivation/,Just some motivation,I have got a job in data engineering field recently and I’m really worried about my performance there. Any tips for newly starting data engineers and most importantly I have been into software development in Python so will i like it?,self.dataengineering
293,pjubnx,data-steve,https://www.reddit.com/r/dataengineering/comments/pjubnx/snowflake_pipeline_to_public_facing_embedded/,Snowflake Pipeline to Public Facing Embedded dashboards,"Hello Fellow Data Engineers,  I work for a SAAS company that provides software services related to payment transactions to our clients. We want to embed some analytics / reporting dashboards into our ADMIN portals for paying customers to see their own ordering trends.  We are currently using SnowFlake as our DW. Does anyone have recommendations or considerations of how this might be implemented in a cost effective, high uptime, and easy implementation?  I want it to be a seamless experience of data exploration (self service slicing and dicing of data)  so there is no ""waiting experience"", efficient use of resources where it doesn't hit our snowflake clusters with every user parameter change, and of course predictable / economical cost (we don't have unlimited budgets).  I am open to batch and near real time designs.  I am considering a few solutions via tools/ capabilities like Snowflake Materialized Views, Apache Kylin, Snowflakes Multi-cluster Dynamic Warehousing feature, and time-series databases. The part I struggle with most is, how do we take this data from snowflake and visualize it in a portal with cached liked results. What visualization tool options do I have available? How do can I cache the results in a way its performant?  &amp;#x200B;  Best,  Data Steve",self.dataengineering
294,pjtz2s,stackedhats,https://www.reddit.com/r/dataengineering/comments/pjtz2s/will_i_regret_it_if_i_start_using_informatica/,Will I regret it if I start using Informatica?,"So my firm already has an informatica license and has some workflows set up in a different department.  As the sole dev in the firm, who was brought on in March, I have the ability to decide my own stack, and don't NEED to use informatica.  I'm currently working on a Python wrapper to handle a simple pipeline's construction (wrapping Pyodbc and using subprocess bcp), which should work fine for the time being.  I would have to maintain Airflow/Luigi by myself and have zero experience with them so I don't really want to go that route right now.  I'm worried that using informatica to orchestrate will bite me in the ass later with technical debt and wasted time wrestling with it down the road, though I'm not sure how well I'll be able to get by without any orchestration tool.  What are your thoughts?",self.dataengineering
295,pjtwyf,Pixie_Gump,https://www.reddit.com/r/dataengineering/comments/pjtwyf/can_someone_please_guide_me_so_that_i_can_land_a/,Can someone please guide me so that I can land a decent job in DE,Can someone please post a step by step guide so exactly what all I have to learn so that I can crack a decent interview in DE. I am really confused with the plethora of resources available online and if someone could sequentially tell me what all to prepare it would be of great help. Thank you!,self.dataengineering
296,pjt56n,gamecockguy2003,https://www.reddit.com/r/dataengineering/comments/pjt56n/best_way_to_get_snapshotfull_dataset_from_changes/,Best way to get snapshot/full dataset from changes,"Maybe a newbie question, but looking for advice. I'm working on taking domain data from a web service and making it available in a data lake. The web service provides a feed of changes (inserts, deletes, and lots of updates) that are already being consumed as files into the lake. My goal is to create a representation of the full set of that domain data based on those changes, with the output being something like a parquet file (though this is flexible).  The challenge I'm running into is that these represent ""random"" changes throughout the dataset. The full set is roughly a billion or so records, and the changes are maybe 5-10 million a day. We'd like to apply the changes to update the full dataset every 4 hours.  I've been trying to do this with Spark but am struggling with doing this performantly given the changes are spread throughout the dataset. Is there a different way I should be approaching this? Open to any thoughts.",self.dataengineering
297,pjsuu8,Irmodude2003,https://www.reddit.com/r/dataengineering/comments/pjsuu8/best_way_to_get_snapshotfull_dataset_from_changes/,Best way to get snapshot/full dataset from changes,"Maybe a newbie question, but looking for advice. I'm working on taking domain data from a web service and making it available in a data lake. The web service provides a feed of changes (inserts, deletes, and lots of updates) that are already being consumed as files into the lake. My goal is to create a representation of the full set of that domain data based on those changes, with the output being something like a parquet file (though this is flexible).  The challenge I'm running into is that these represent ""random"" changes throughout the dataset. The full set is roughly a billion or so records, and the changes are maybe 5-10 million a day. We'd like to apply the changes to update the full dataset every 4 hours.  I've been trying to do this with Spark but am struggling with doing this performantly given the changes are spread throughout the dataset. Is there a different way I should be approaching this? Open to any thoughts.",self.dataengineering
298,pjpolp,Jeeya_Singh,https://www.reddit.com/r/dataengineering/comments/pjpolp/sap_sd_training_certification/,Sap sd training certification,[removed],self.dataengineering
299,pjoho6,Cloudy_Waves,https://youtu.be/XZGhIYwbaDo,Azure Data Factory Tutorial,,youtu.be
300,pjn7mi,dataengineerdude,https://www.confessionsofadataguy.com/dask-vs-pyspark-performance-and-other-thoughts/,Dask vs PySpark – Performance and Other Thoughts.,,confessionsofadataguy.com
301,pjidry,VictorAVB,https://www.reddit.com/r/dataengineering/comments/pjidry/step_by_step_guide_on_how_to_scrape_thousands_of/,Step by Step guide on how to scrape thousands of Hotel data from booking.com (no code),"Hi Guys, thought I would share this guide   Let me know if you found it useful, see here",self.dataengineering
302,pjh4ya,pbj800100,https://www.reddit.com/r/dataengineering/comments/pjh4ya/how_often_do_you_think_about_security_when/,How often do you think about security when writing code?,"I've recently been thinking about code vulnerabilities and I realised that (perhaps due to not coming from a CS background) I have never really learned anything about writing secure code. I don't even really know much about what makes code vulnerable, how to assess third party software for security, etc. I wanted to know if this is something that's largely overlooked in the data engineering community? I can imagine if I was working as a general software engineer developing apps with larger potential consequences, I'd see this come up more, but I'm curious if people ever think about security when it comes to building data pipelines? Of course a lot of us are dealing with sensitive data and there's always privacy concerns, so I'd think this is important for us too!",self.dataengineering
303,pjgybd,nfrankel,https://hazelcast.com/blog/hazelcast-anti-patterns/,Hazelcast Anti-Patterns,,hazelcast.com
304,pjf1ui,Gawgba,https://www.reddit.com/r/dataengineering/comments/pjf1ui/taming_big_data_with_apache_spark_and_python/,Taming Big Data with Apache Spark and Python - Hands On! question,"Very highly rated course but wondering given the rate of speed things move in the data engineering realm (and the fact that this course was developed I think like 5+ years ago) whether anyone who took the course can comment on how timely and relevant the information is today, i.e. the course uses Spark 2 when the most recent version is 3, and seems to have a very long section on RDDs but I thought I'd heard that RDDs were no longer in favor and dataframes are where it's at.  Basically trying to use my learning time as efficiently as possible.",self.dataengineering
305,pjeobz,Krypton_Rimsdim,https://i.redd.it/um6c2b3gzzl71.jpg,Entry Level DE Intern position Resume Review: Is anyone willing to critique a Resume? (I am an ETL Dev (2 YoE) currently doing my master's in Business analytics applying for DE summer internships.),,i.redd.it
306,pjel3o,Ok-Message1053,https://www.reddit.com/r/dataengineering/comments/pjel3o/saving_email_attachments_in_cloud_storage_email/,Saving email attachments in Cloud Storage - email question,"So basically I have a client who would like to send csv files via email. I want to automate a process to receive these emails and drop them into GCS. I'm considering using Mailgun to receive the emails and trigger a Cloud Function using a webhook. This might be a silly question (and tbh probably not really a data engineering question) but I'm wondering how I can get a new email address for my client to email the data to (as in, I don't want them to have to email a specific existing user at our company). I'm confused about whether this is something that Mailgun can handle or if I just need to get my company to set up a new email (I'm hoping there is a way around that because setting up an email NOT for a specific user is difficult for whatever reason). When I send emails from Mailgun, they are sent FROM 'mailgun@newdomain.com' so it'd be great if I could also send emails TO something like 'mailgun@newdomain.com'. If I'm understanding it correctly, it sounds like you just configure your existing domain though so there's not a generic Mailgun email that you can send to? Is there any other method (totally separate from Mailgun) that I can use to do this?",self.dataengineering
307,pje6cu,Ok-Message1053,https://www.reddit.com/r/dataengineering/comments/pje6cu/is_anyone_familiar_with_mailgun/,Is anyone familiar with mailgun?,"I have a client who wants to send some data via email (as a csv attachment) so I'm thinking of using mailgun to receive the emails/trigger a webhook event. I understand the concept of routes and webhooks with mailgun, but I'm very confused about the whole domain set up part. I just want my client to be able to send to a brand new generic email and I don't really care about the domain... doesn't matter if it's associated to my company or not (could be something like 'data@xyz.mailgun.com'). Can anyone help clarify how to do this? If I get our IT team to set up the MX records, I don't know how I can get a generic email like 'data@...' I don't want to use an existing user's email address. Sorry I'm not at all knowledgable with this email stuff so hoping someone can ELI5 because even googling what an MX record is is confusing to me. Thank you!!",self.dataengineering
308,pjc2ka,Reddit_Account_C-137,https://www.reddit.com/r/dataengineering/comments/pjc2ka/how_to_correctly_crawl_lots_of_pages_without/,How to correctly crawl lots of pages without getting a bot checking screen from cloudflare?,So i'm using a websites API to gather data but I want some additional data from each individual item in the API list which requires me to scrape around 1000 pages.  Once I get to around item 113 I get the cloudflare anti-bot screen which stops my code dead in its tracks. I'd like this scrape to occur every two hours so the code can take longer if needed. Is adding a time delay within the loop a good solution or is there a better way? If a delay is a good solution what is the minimum time I can use to avoid this issue? 50ms? 1s? 10s? The robots.txt file for the website does not provide any info on this topic.,self.dataengineering
309,pj99xo,noNSFWcontent,https://www.reddit.com/r/dataengineering/comments/pj99xo/please_help_me_understand_how_to_go_about_working/,Please help me understand how to go about working on a personal project.,"So I'm trying to build a project where whenever a book's isbn is scanned, the frontend of the project would return the second hand prices of the book from different websites.   The data flow looks somewhat like -        Scan isbn          if isbn already in database              return prices         else             scrape the corresponding website for the prices (maybe add them to the database if doesn't take too much time)              then display the prices on the front end    I am also in the process of looking for data science jobs so want to use tools like docker and a noSQL database like Mongodb to add to my resume.   I need help figuring out the architecture for the project.   I know there are different backend server containers like FastAPI available in docker as well as noSQL databases like Mongodb.   What I need help figuring out is how to use them all together?    Apologies if the question is a bit vague.",self.dataengineering
310,pj6hoa,Away-Suggestion-5845,https://www.reddit.com/r/dataengineering/comments/pj6hoa/a_case_study_example_for_an_interview/,A Case Study Example for an Interview,"Hi guys, I want to share with you a case study which i have to answer. According to you what would be an appropriate approach for this case study?   **Case Study:**  The event management company provides the platform for their customers to publish their events and manages all transactional processes. The ‘shop’ platform stores all orders(included historical datas) of a customer in a relational database and the DWH imports all those orders daily into their own PostgreSQL DB. So,the DWH is always in sync with all changes and all new orders.To reduce the amount of data which has to be stored in the ‘shop’ DB and to improve the platform performance, the company wants to remove all historical order data from the ‘shop’and only keep a few days until the event has happened and no cancellation is possible anymore.   **Task 1:** Think about consequences for the (daily) data import of the Data Team and what other aspects might be influenced by such a change. What issues has the DWH to solve before such a change in the shop can be realized?   **Task 2:** Besides storing all data in a relational database, the event platform already uses Kafka to communicate updates between its different subsystems, opening up a completely new way of getting order infos for the DWH. Would you rather prefer an incremental load from the DB (less changes on DWH side) or would you prefer importing order data via Kafka into the DWH? What are advantages and disadvantages?  Any help would be much appreciated.  Thanks.",self.dataengineering
311,pj1kvs,umarcja,https://www.reddit.com/r/dataengineering/comments/pj1kvs/how_fast_and_scalable_is_timescaledb_compare_to_a/,How fast and scalable is TimescaleDB compare to a NoSQL Database?,"My goal is to store raw and parse logs.  I've read a lot about TimescaleDB recently and it seems that it's one of the most powerful Time-Series Database, so it makes sense for storing logs has they are time-based.  My question is do anybody here has encounter problems with speed at a certain scale?  I've seen a lot of benchmark results on timescale on the web but they all come from [timescale.com](https://timescale.com) so I just want to ask if those are accurate.  I know that is another question in my post but how better is it to store time-series data into a time-series database instead of a NoSQL database?",self.dataengineering
312,pj0y4g,Fnmokh,https://www.reddit.com/r/dataengineering/comments/pj0y4g/how_to_cumpute_a_financial_assets_risk_metric/,How to cumpute a financial asset's risk metric ?,"Hello everyone,  I have been looking for a variant/base formula for the BTC Risk metric displayed in the videos of Bnejamin Cowen [(Youtube video here)](https://www.youtube.com/watch?v=6CiFVI24CXM) in order to replicate it, or adapt it to my views of the crypto market.   Any suugestions are welcome, i have been looking for this for quite a while now, and the metric is very insightful i think.  Thanks guys",self.dataengineering
313,piuy93,masek94,https://www.reddit.com/r/dataengineering/comments/piuy93/how_to_synchronize_data_from_staging_database_to/,How to synchronize data from Staging database to Production database.,"Hello guys,   &amp;#x200B;  I am working on data pipeline that basically queries data from MongoDB, transform it, map it to Relational DataBase model and load it to the staging table. And then from staging table, I am loading data into dimension tables, fact tables,s and some bridge/linking tables (decomposition tables).  Everything is based on SQL merge. So If a given record exists (base on somewhere conditions) I am only updating it, in another case, I am inserting it into the tables.   &amp;#x200B;  What I need to achieve is that if all steps in the staging environment will pass without failure I need to copy data into the production tables or recreate the whole pipeline in the production database. And after that to clear whole staging db tables (it's a question of the retention policy, but for now let's assume that I will clean them)  Do you know what are best practices there are and how I can achieve them?   I am using python as my main tool. And I totally don't know how to do that. I think that just copying data from staging into production tables would be the easiest one. Do you have any hints on how to approach my problem ?",self.dataengineering
314,piu290,raghukveer,https://www.reddit.com/r/dataengineering/comments/piu290/is_it_mandatory_to_work_as_a_dbadata_analyst_to/,Is it mandatory to work as a DBA/Data Analyst to become a DE?,"Hi, I am following Wiki, learned Python, SQL foundations, and practicing exercises now, will move to data warehousing next.   But, most of the job-related posts here talk about transitioning from other roles such as DA/DS/ and some from DBA.  In past, I worked as an IT consultant for 2 small companies with Docker, Kubernetes, Linux, but have no experience with data-related works.   Is there such a limited scope to start as DE? can we get an entry-level position purely based on personal projects? or should I become a DA and learn on the job for a year or two?",self.dataengineering
315,piu0zk,HansSlinger,https://www.reddit.com/r/dataengineering/comments/piu0zk/does_your_organisation_have_explicit_dataops_roles/,Does your organisation have explicit DataOps roles?,"I'm curious to know what proportion of organisations have DataOps Engineer roles, or if they told these functions into existing Data Engineers.  I suspect the answer is ""once you get to a Data Engineering team of size X, you need DataOps engineers"". Also keen to know, in your experience, what number is X?",self.dataengineering
316,pisfxe,kaartman1,https://www.reddit.com/r/dataengineering/comments/pisfxe/can_i_get_some_advice/,Can I get some advice?,"Hi all, I am wondering if I can get some career advice. Sorry if this is a wrong sub. I have been coding for a while in Oracle PL/SQL and my career has been stagnant. I want to pivoting and get into data engineering. I am learning Python. Looks like there are many technology stacks, wondering what I should learn to get a break.",self.dataengineering
317,piryhi,1100010,https://www.reddit.com/r/dataengineering/comments/piryhi/data_engineering_project_tutorial/,Data engineering project tutorial,Is there any videos that go through the process of creating a data engineering project from start to finish?,self.dataengineering
318,pipz87,1100010,https://www.reddit.com/r/dataengineering/comments/pipz87/data_engineering_project_tutorial/,Data engineering project tutorial,Is there any videos that go through the process of creating a data engineering project from start to finish?,self.dataengineering
319,piop8y,powok,https://www.reddit.com/r/dataengineering/comments/piop8y/where_do_jobs_gets_scheduled_in_dev_or_prod/,Where do jobs gets scheduled in dev or prod ?,"Hi Guys ,  A noob question here , when we have a CICD pipeline in DE and if I make chages in dev and it gets deployed to prod. 1. How do you deploy it , is it automatically or is it a daily deployment done by a senior DE. 2. Where does the job gets scheduled is it in dev or uat or prod or all places and do all the environment read the same data source ?  Thanks for your information.  Regards",self.dataengineering
320,pio9i3,TheCauthon,https://www.reddit.com/r/dataengineering/comments/pio9i3/what_are_some_of_the_best_data_engineering_jokes/,What are some of the best data engineering jokes you have seen?,Just looking for some of the best data engineering jokes you’ve come across.,self.dataengineering
321,pingxm,MaxGanzII,https://www.reddit.com/r/dataengineering/comments/pingxm/amazon_redshift_research_project_materialized/,"Amazon Redshift Research Project : Materialized Views (white paper, PDF)","https://www.amazonredshiftresearchproject.org  https://www.amazonredshiftresearchproject.org/white_papers/downloads/materialized_views.pdf  Materialized views are implemented as a normal table, a normal view and a procedure, all created by the `CREATE MATERIALIZED VIEW` command, where the procedure is called by the `REFRESH MATERIALIZED VIEW` command and performs refresh. The table is created by `CREATE TABLE AS`, which is why column encodings cannot be specified. The encoding choices made by Redshift are extremely poor. A full refresh makes a new table, populates it, and uses table rename to replace the existing table. An incremental refresh uses an `insert` followed by a `delete`, using the system columns `deletexid` and `insertxid` to keep track of which rows have changed, and as such runs a full refresh when any of the tables used by the materialized view have either manually or automatically been vacuumed, as vacuum resets in a table the values in the `deletexid` and `insertxid` columns, with additional columns being present in the materialized view, one plus one for every table used in the materialized view SQL, to track rows. The table underlying the material view is never vacuumed, except by auto-vacuum, which I suspect running so infrequently as to be inconsequential. Auto-refresh is an undocumented black box, likely subject to ongoing unannounced change, and its behaviour is unpredictable. On a small single column table on an idle cluster refresh occurred normally after about 55 seconds; with one row inserted per second, refresh occurred after between 54 to 1295 seconds (twenty-one minutes).",self.dataengineering
322,pin0kq,priyasweety1,https://www.reddit.com/r/dataengineering/comments/pin0kq/different_approaches_to_do_incremental_delta_load/,Different approaches to do incremental delta load in S3,I have historical data as well as incremental data in s3 what’s the best efficient way to load and transform only incremental and load incremental data in Postgres or Redshift. Is it good to add a runtime column to filter only latest updated records for huge volume of data. Approx nearly 30T.   Optional Can someone define a sample data flow from landing to consumption layer.,self.dataengineering
323,pii29n,dolphinday2,https://www.reddit.com/r/dataengineering/comments/pii29n/programming_etl_vs_sql_etl/,Programming ETL vs SQL ETL,"I have seen a lot of people mention 2 different types of ETL- ETL using a programming language (Scala for example) and ETL using SQL. My confusion however is that from my experience, ETL in a programming language, say Scala, is still really just ETL using SQL. Because for Scala for example, you would use Spark SQL, which is really just SQL at the end of the day.   So I guess in my mind, I don't see any distinction between programming ETL and SQL ETL, since programming ETL is still ETL using SQL. But people seem to reference it as if there is this great big distinction, which I just don't really see to be honest. Am I crazy or is there some big difference between the two that I am in fact missing?",self.dataengineering
324,pig33m,djl0077,https://www.reddit.com/r/dataengineering/comments/pig33m/dimensional_data_model_design_for_salesforce/,Dimensional Data Model Design for Salesforce,"I am working with Salesforce data for the first time. I need to design a dimensional model that will serve as a strong foundation for analytics as my company will be looking to apply it to the data of several dozen subsidiary companies (each with their own salesforce instances - yes...I know...).  Would anyone be willing to share some designs they've created in the past that I can use as a starting point and/or share some best practices to hopefully save me some pain?  Looking at this simplified ERD created by FiveTran (see link below). Seems the analytics my company is after are heavily focused in the ""Sales"" grouping, but would love to hear about anything anyone out there has modeled in the past.  ERD link:  [https://docs.google.com/presentation/d/1fB6aCiX\_C1lieJf55TbS2v1yv9sp-AHNNAh2x7jnJ48/edit#slide=id.g3cb9b617d1\_0\_237](https://docs.google.com/presentation/d/1fB6aCiX_C1lieJf55TbS2v1yv9sp-AHNNAh2x7jnJ48/edit#slide=id.g3cb9b617d1_0_237)  Thanks in advance!",self.dataengineering
325,pifayk,Katkool,https://www.reddit.com/r/dataengineering/comments/pifayk/how_to_write_readable_and_organized_pipelines/,How to write readable and organized pipelines?,"I am embarking on my first major data engineering project, which will involve ELT batch processing of API data (JSON) and uploading it to AWS's RDS platform. While I have my own reasons for wanting to build this pipeline, I would also like for it to serve as a good portfolio project with proper documentation and readability for others, both technical and non-technical. How do I organize my code and write proper documentation?  As a beginner, some good practices I've heard of are:  ** Coding **  * Write most of the code inside functions, then call these functions sequentially to illustrate the overall data pipeline * Use descriptive names for functions and variables, also write good comments describing the code * Don't write the entire pipeline in one file, separate major processes into different files  ** Documentation ** * Document database structure using ERDs, the process workflow using block diagrams (or whatever else, just something that illustrates it), and describe table columns.  * Describe the project files structure and show where each file in the project is and what it does  I realize this is a lot so maybe to keep it simple, can you suggest a good video/article to read?",self.dataengineering
326,pie34a,umarcja,https://www.reddit.com/r/dataengineering/comments/pie34a/storing_json_logs_in_aws_for_querying/,Storing JSON logs in AWS for querying,"I need to store the raw JSON logs created by my software and allow users to see those logs and filter them.  I thought of using the Parquet format as it's small and powerful, and this format will allow me to filter base on one column easily, but the problem is that I need to provide the raw file in the end.  I'm using an S3 bucket to store the raw logs files at the moment and trying to figure out what's the best way to store those logs. Is it completely wrong to use Parquet and have a column that stores raw JSON logs?   In the end, the goal is to let users browse and filter their own logs by HTTP status.  I thought of Athena to query Parquet files or dump raw logs in Elasticsearch. Still, Elastic might be overkill because I will purge data every 48 hours (let's say that the average number of logs will be around 2 million).",self.dataengineering
327,pidua2,joseph_machado,https://www.reddit.com/r/dataengineering/comments/pidua2/process_to_understand_deliver_on_your_data/,Process to understand &amp; deliver on your data engineering task,"Hello Redditors,  Starting as a data engineer can be overwhelming. I have seen data engineers feeling overwhelmed by the perceived complexity of their assigned tasks. I, myself, was also often lost when I started as a data engineer. But with experience, I now know (or at least most of the time) how to approach a task. With this in mind, I wrote an article that outlines one approach you can follow, to understand and deliver on your data engineering tasks.  [https://www.startdataengineering.com/post/how-to-deliver-on-your-de-tasks/](https://www.startdataengineering.com/post/how-to-deliver-on-your-de-tasks/)  I'd be curious to hear about any other steps/processes everyone uses. Appreciate any feedback. I hope this helps someone. :)",self.dataengineering
328,pic7z2,soundbarrier_io,https://www.reddit.com/r/dataengineering/comments/pic7z2/how_do_the_pandemic_data_pipelines_work/,"How do the ""pandemic"" data pipelines work?","I was wondering if somebody has information about how the data pipelines work that are used around the world to produce the beautiful coronavirus maps and plots and numbers and trends with have become very familiar with in the last 1.5+ years. Same with information about vaccination.  * How does data get reported, i.e. what tools are used? * What does quality assurance look like on this data? Any details about SLAs?",self.dataengineering
329,pi8zx1,AndroidePsicokiller,/r/webscraping/comments/phusc1/scaling_the_web_scraping_operation/,Scaling web scraping operation,,self.webscraping
330,pi70aq,h2otoo_frog,https://www.reddit.com/r/dataengineering/comments/pi70aq/is_boomi_as_bad_as_it_seems/,Is Boomi as bad as it seems?,"I’m working on a team that uses Dell Boomi as the main process to bring things from some source to target databases. I’m not sure how much of my problem is based on how the existing team set up infrastructure and how much is because it’s really unintuitive.  It just took me a week to write something that would make a basic API call, loop through the pages, save to AWS S3, then send a command to the database for loading.  I could have done the same thing in Python in a day and I’m not even an expert-level Python developer.  Once you hit the “test” button, it seems your only option is to watch each of the icons light up green until it finishes, or one icon turns red. At this point, you click each gumdrop button (they call them “shapes”) to get a fortune cookie of information. When a variable (dynamic process property) is assigned, it doesn’t actually tell you which one is assigned; you have to exit the test mode to determine based on ordinal position of all variables in the shape.  I just spent an entire day trying to figure out why a stored procedure call was failing.  I have conflicting feelings over my sense of accomplishment to do a day’s worth of work in a week.",self.dataengineering
331,pi2lxk,Ok-Message1053,https://www.reddit.com/r/dataengineering/comments/pi2lxk/replicating_data_from_azure_sql_to_bigquery/,Replicating data from Azure SQL to Bigquery,"I have access to an Azure SQL database and I need to keep an exact replica of a couple of those tables in Bigquery. I only have read access to the Azure db and the clients are difficult to work with so I can't really ask them to set up anything for me on their end. They update these tables daily and have provided me a stored procedure that tracks the changes (updates, deletes and inserts). I'm currently using Python to run the stored procedure, saving the results in a BQ table, and then running a BQ merge statement. I'm having a bit of trouble with this (it seems like sometimes rows aren't deleting, but it's inconsistent and random so I have yet to identify the cause of this), but I'm worried that as the Azure DB grows, it's going to become expensive anyway.  &amp;#x200B;  Any ideas of a better way to do this instead of a merge statement?",self.dataengineering
332,pi0l0t,babblingfish,https://www.babbling.fish/vertica-survival-guide/,Getting Started with Vertica: An introduction for people that already know SQL,,babbling.fish
333,phuwcs,Born-Comment3359,https://www.reddit.com/r/dataengineering/comments/phuwcs/what_are_the_most_importantmusthave_tech_skills/,What are the most important/must-have tech skills someone should know (or learn) to be hired as a data engineer?,By tech skills i mean both programming languages/frameworks and knowledge of design/architechture/concepts.,self.dataengineering
334,pht1qq,goeb04,https://www.reddit.com/r/dataengineering/comments/pht1qq/should_i_go_after_masters_or_certificate/,Should I go after Masters or Certificate?,"I came from a useless Psychology degree, and to make a long story short, I have managed to land in a Data Developer role that I have serve din for the last 4 years now (I have been in data related roles for the last 6-7 years). This role is pretty niche though, I don't get to use SQL or really use big data tools or such. No metrics or anything.  I develop/maintain product data for complex products that walks users down a decision tree in a 2D/3D modeling environment that is drag and drop. The data gets developed in our vendors' data tool (format can either be in db3 or XML) and publish the data to their server so our Software Developers can work on it downstream.   I took the next step in the role by using python (learned on my own after hours) to scale the data development by using lxml (god bless xpath!) and pandas to wrangle data into the XML files needed. I do get unstructured raw data to work with to develop product data and I probably have written close to 100 modules at this point. One module basically is a central repository for hundreds of functions and thousands of lines of code/comments. It is working surprisingly well and I feel like I automated maybe 30-40% of my work at this point. I even use Python to run validation tests on my data and it is phenomenal at catching errors. My ticket count for errors/bugs have drastically reduced. Now I am tinkering with scraping data (Selenium) from some of our JS based repository sites that I typically had to pull basic csv exports from (awful site that will never get updated but need raw data from there at times).  Our department is a bit behind the times in my opinion and I need to flourish as I don't feel challenge don some days.  I feel like I mastered this role and I think I should make the step up to a data engineering type of role. My issue right now is, I am seen more as a data wrangler who can use maybe python and advanced Excel to get the job done but I feel like I will get pigeonholed and I want to have more leverage in my desultory career. The skills I learned in this job are fairly niche and I haven't used a SQL statement in years and I only touch the Oracle ERP to do some pricing models.  What is your recommendation on what course/education stream I should take to jump into a data engineering role. I miss using SQL and I do want to learn more about what ETL tools are out there and the best practices to utilize them. My worry is about not so much wasting money, but more so time, on something that won't allow me to transcend in my career.  If you managed to read this prolonged post, I appreciate it. I trust all of your opinions more than any other I can get. I don't want to make this a novel so feel free to follow up with any questions.  Thanks!  tldr; Current 'Data developer' with a few years of python scripting, basic SQL (and MS Access), advanced Excel formulas looking to figure out what education path I need to become Data Engineer and look legit to recruiters",self.dataengineering
335,phsl1v,jamiej723,https://www.reddit.com/r/dataengineering/comments/phsl1v/should_hive_llap_be_used_for_both_read_and_write/,Should Hive LLAP be used for both read and write operations?,"Long time lurker in need of help to settle a debate!   So in our organisation we've started to adopt hive llap to improve the speed of ad-hoc SQL queries of our data analysts.  We've seen some major speed improvements so far and are keen to roll this out to many more (100s) of users.  The one snag is that llap has been configured only for read operations (Selects) and the responsible team are convinced it shouldn't be used at all for write operations (create, insert, drop) which many of our analysts use in their querying workflows e.g. for beaking up complex queries by writing intermediate tables then bringing these together to complete their analysis   There doesn't seem to be any resources online that addresses this specific question, so would be greatful for any experiences from others:  Should LLAP be used for both reads and writes? (we would like to avoid switching between solutions for reading and writing)  Should we consider working with analysts to reduce the amount of write operations they perform?  Alternatives we should consider?   Thanks friends!",self.dataengineering
336,phsexf,FleyFawkes,https://www.reddit.com/r/dataengineering/comments/phsexf/how_to_open_a_data_with_tbs_of_data/,How to open a data with TB's of data,"I'm working with python, and predominantly pandas.  Let's say i have db/csv or other format with dozen/hundreds of TB's of data.  Let's say pandas cannot handle it, How to open it?",self.dataengineering
337,phoowr,cyrilou242,https://medium.com/@cdecatheu/data-quality-timeseries-anomaly-detection-at-scale-with-thirdeye-468f771154e6,Data Quality: Timeseries Anomaly Detection at Scale with Thirdeye,,medium.com
338,phnp95,CookieKlutzy4407,https://mechomotive.com/react-js-react-free-courses-javascript/,Why React JS is more popular than other JavaScript Frameworks?,,mechomotive.com
339,phkcm4,pbj800100,https://www.reddit.com/r/dataengineering/comments/phkcm4/interesting_papersdata_related_topics/,Interesting papers/data related topics,"Has anyone come across any interesting articles or can recommend any interesting topics related to anything in the engineering/data science/developer space? I'm looking for something fun, interesting or just random to present at my job. We've covered things like bias, developer productivity, computer hardware, etc. Just looking for ideas  :)",self.dataengineering
340,phi80k,TurbulentSquirrel577,https://www.reddit.com/r/dataengineering/comments/phi80k/choose_a_data_warehouse/,Choose a Data warehouse,[removed],self.dataengineering
341,pha14p,Ser_Black,https://www.reddit.com/r/dataengineering/comments/pha14p/gcp_data_engineer_certification/,GCP data engineer certification,"Hello,  I'm trying to prepare to the data engineering Certification for GCP. I already have an account in a cloud guru and I think automatically I do have one for Linux academy, I found lots of courses in there but which is time efficient and cover the whole spectrum of the certification.  Last year, I passed the cloud engineer one relying on a course in linux academy, I didn't pass the first time because the course was outdated. That's why to not repeat the mistakes of last time. Which course in these two sites do you recommend I should do to pass the certification.  My sincerest thanks",self.dataengineering
342,ph8vqo,Perturbed_Parakeet,https://www.reddit.com/r/dataengineering/comments/ph8vqo/feature_store_for_image_features/,Feature store for image features,"Hi all, I'm looking for some feedback regarding some design choices I made for an ML application (in Python) of mine.  &amp;#x200B;  The primary goal of my application is to compare features of **large** geospatial images (GeoTIFFs). The features are numpy arrays computed by running a pretrained model (like VGG or Resnet) over the image. Naturally feature computation is slow since I have to tile the images and run the model over each of the tiles. This is fine but to avoid doing this multiple times I have structured the application in the following way:  &amp;#x200B;  1. Given two input images, compute image hash of each and check if they're in feature store (Redis currently) 2. If no, tile the image(s)  and extract features. The features will be a numpy array of shape (T, H, W) where T is number of tiles, H is height of tile and W is width of tile. This is then stored in Redis where key is the image hash and value is the numpy array. If the image hash is in Redis, I retrieve the features from Redis. 3. Do comparison.  &amp;#x200B;  This is working fine for me so far but I would like to know if Redis is the optimal choice for querying features in this application. I ask because I anticipate a lot of images and I'm not sure if an in-memory key-value is ideal for this. Any feedback on this point or the application in general would be highly appreciated.",self.dataengineering
343,ph8h1d,inhiphuong,https://www.reddit.com/r/dataengineering/comments/ph8h1d/advice_on_onsite_interview_with_facebook_for_data/,Advice on Onsite Interview with Facebook for Data Enginner role? What should I expect from the case studies?,"Hi everyone, I will be having an onsite interview with Facebook for their DE position (new grad) soon and was wondering if anyone has recently done the interview could share what I could expect from their case studies of product challenges? I know they will be testing on product sense, data modeling, SQL, and Python. This is my first time interviewing for DE so I really don't know what to expect.   Thanks a ton!",self.dataengineering
344,ph4vvy,vy52,https://www.vcloudata.in/2021/08/move-data-from-mysql-server-database-to.html,Move data from a MySQL Server database to Azure SQL Database with Azure Data Factory,,vcloudata.in
345,ph4m1r,geop_kc,https://www.reddit.com/r/dataengineering/comments/ph4m1r/help_with_technical_interview/,Help with technical interview,"Hello everyone.I am currently at the stage of the technical interview with Accenture in a few days for the role of Associate Data Engineering.Its somewhat a junior level job,but the expect more from a junior data engineer compared to a junior developer for some reason,at least that’s what the HR told me in the interview we’ve had.My problem is,I recently graduated from a physics department,and I mostly know SQL and some basic things from Power BI and Matlab.There’s no experience in my resume and nothing fancy in terms of knowledge,so I’m not sure what they expect.Any recommendations or ideas about what would they want,and also how could I be ready for the interview,at least somewhat ready.A have a few days ahead so I could learn some basic staff and some theory.Thanks in advance.",self.dataengineering
346,ph4fvj,WalrusWhich202,https://www.reddit.com/r/dataengineering/comments/ph4fvj/how_to_querying_data_faster/,How to Querying data faster,"I work as a data scientist and currently building data architecture with a size of 500k rows in rdb.   I already extract the data and currently it took around 12 hr to extract 500k rows data into useful data. The problem is it took that much of time to translate the data based on sql query and some small calculation(average for example)  I've looked into spark, hadoop, cassandra, hdfs, and others. It says that those framework is usually used for processing big data and can query data fast.  So i was wondering are those the solution to my problem?",self.dataengineering
347,ph4edv,hatchikyu,/r/sre/comments/ph4e4a/alternatives_to_certification_for_professional/,Alternatives to certification for professional development,,self.sre
348,ph4bxd,flat_rain,https://www.reddit.com/r/dataengineering/comments/ph4bxd/i_created_rcsv/,I created r/csv, I create a new subreddit for discussing csv files. [Link](https://www.reddit.com/r/csv) .  It needs additional moderator ASAP.,self.dataengineering
349,ph4b06,annmjac,https://www.reddit.com/r/dataengineering/comments/ph4b06/data_lake_vs_nosql_db/,Data lake vs NoSQL db,"Hello, new to data engineering. What would save in your unstructured DB if you already have a data lake. I'm just trying to find the difference. Thanks!",self.dataengineering
350,ph3gdk,SnooBeans3890,https://www.reddit.com/r/dataengineering/comments/ph3gdk/cant_find_a_data_engineering_animation/,Can't find a data engineering animation,"A couple of months back, there was an animation done with sources on the left and visualization tools on the right with a mini hammer going from left to right and moving data across. Unfortunately, I cannot find it anymore. Does anyone have it?",self.dataengineering
351,ph2ylb,Andrey_Khakhariev,https://towardsdatascience.com/overview-of-ui-tools-for-monitoring-and-management-of-apache-kafka-clusters-8c383f897e80,Overview of UI Tools for Monitoring and Management of Apache Kafka Clusters,,towardsdatascience.com
352,ph2um4,satyronicon,https://www.reddit.com/r/dataengineering/comments/ph2um4/what_is_the_one_de_tooltechnology_that_got_you/,What is the one DE tool/technology that got you the job and fill up 80% of your daily work? Perm or Freelancer,"I know there are many tools we are using, but there is always that one or two technologies that earns us most of our reputation, gets us the job, we use most in our daily work.  Let us know your   ** job title,   ** job type (i.e. perm., freelancing) and   ** what the one DE tool/technology that got you the job and fill up 80% of your daily work?   *_you can copy paste the template too if you want_*  - Job Title:  - Job Type:  - Tool or (up to three) Tools I use most:",self.dataengineering
353,ph17wi,Efficient_Camel6175,https://www.reddit.com/r/dataengineering/comments/ph17wi/data_engineering_project/,Data Engineering Project,Can someone provide Simple Data Engineering Project using Hadoop and Spark for reference?,self.dataengineering
354,ph15au,SukalpVashishtha,https://www.reddit.com/r/dataengineering/comments/ph15au/microservice_shared_database_antipattern_question/,Microservice shared database antipattern question,"Hi Data Engineering experts,  I have a general design question regarding streaming data processing and microservice architecture.   Is it a fair design pattern that a stream processing framework (spark/kafka streams etc) populates a database and the data is made available to query via a microservice. It kind of violates the loose coupling principle of microservice architecture in a way.  Else, we can let the inserts operations to be also done by the same microservice  so that database is completely encapsulated by it.  Which option is better? Other suggestions are also welcome.  Thanks in advance :)",self.dataengineering
355,ph0uwb,chungmaster,https://www.reddit.com/r/dataengineering/comments/ph0uwb/keeping_track_of_a_large_number_of_kakfa_topics/,Keeping track of a large number of kakfa topics and how they are connected,Hey guys! Totally new to this sphere but I just joined a pretty large org that has a pretty complicated web of services connected via a lot of kafka topics.  I imagine this is not the only place to have these so how do you guys keep track of where the data is flowing from?   &amp;#x200B;  We do have some documentation but as new services get added or old ones deprecated the flow changes pretty fast and everything becomes out of data again. Do you guys have any good techniques or tools for tracking the data flow?,self.dataengineering
356,ph0sct,targetXING,https://www.reddit.com/r/dataengineering/comments/ph0sct/when_is_the_spark_python_dataframe_api_not_enough/,When is the Spark Python DataFrame API not enough?,"I've found at least one use case where I want to compute the cosine similarity between embeddings in two tables. But I could also just use an inline scala expression to do this.   Another example would be classes, I suppose.",self.dataengineering
357,ph0ngk,RassmusRassmusen,https://www.reddit.com/r/dataengineering/comments/ph0ngk/are_you_put_off_the_job_by_the_job_spec_and_the/,Are you put off the job by the job spec and the technical test,Does the interview process put you off applying for that role.  [View Poll](https://www.reddit.com/poll/ph0ngk),self.dataengineering
358,pgz0h2,GreekYogurtt,https://www.reddit.com/r/dataengineering/comments/pgz0h2/any_interest_in_a_compiled_advanced_sql_content/,Any interest in a (compiled) Advanced SQL content for interviews ?,I'm planning to write a blog sharing the content that helped me clear 5+ Jr. Data Engineer Rounds.      Please let me know if there is interest for the same.     Also comment !remindme 2 days to get notified of the blog in 2days.,self.dataengineering
359,pgyx6s,thinkingtitan,https://www.reddit.com/r/dataengineering/comments/pgyx6s/cloud_solutionsbigquery_snowflake_as_data/,"Cloud Solutions(BigQuery, Snowflake) as Data Warehouse Solutions?","Hi Folks,  I am currently managing an on prem data warehouse built with SSIS and hosted on SQL Server. For a while now, I have been trying to understand the buzz around cloud solutions for data warehouse. In our data warehouse, the biggest challenge is the presence of multiple data sources leading to complex data integration scenarios. As we integrate more and more sources, the data model(IBM's IIW based on Anchors) goes a level up with complexity. It has become harder to onboard new members of the team, given how much context they need to be educated with before making changes.  I understand, cloud lowers cost of compute and storage. Also, data processing can be tweaked on many levels in the cloud. But without proper data integration, how is Snowflake/BigQuery being touted as **Data Warehouse** Solutions? How is that any different from **Data Lake**, for example?  I am keen to hear about your experience(s).",self.dataengineering
360,pgw3w9,fhoffa,https://www.linkedin.com/posts/patrickcuba_datavault-activity-6839311161381539840-wIfB,Say NO to Refactoring Data Models: Data Vault has the answer,,linkedin.com
361,pgv4vw,el_jeep0,https://www.reddit.com/r/dataengineering/comments/pgv4vw/considering_taking_analytics_engineering_role_for/,"Considering taking Analytics Engineering role for a while, can I go back to Data Engineering after, will it be harder?","I learn best on the job, analytics engineering appeals to me because it's (internal) customer-facing, it will deepen my SQL/DBT skills and allow me to maybe pick up some looker. Long term I am trying to land a job at GCP/AWS some other big data SAS because I like helping people and varied work.  I worry though that is might be viewed as less technical/that the comp will be less than if I stay in DE. Would love to hear y'all's thoughts even if it's just how delusional I am about what working at a big SAS would be like.  Thanks in advance!",self.dataengineering
362,pgtfxn,Vorskl,https://www.reddit.com/r/dataengineering/comments/pgtfxn/any_deds_udemy_courses_worth_stocking/,Any DE/DS Udemy courses worth stocking?,"Hi,   The labor day is coming and Udemy will most likely run its $9.99 sale (again). Are there good courses worth stockpiling?  I'm eyeing Airflow,  BigQuery DE  and Lazy Programmer DS courses.",self.dataengineering
363,pgt5qx,paradox10196,https://www.reddit.com/r/dataengineering/comments/pgt5qx/have_to_travel/,Have to Travel ?,Maybe I’m not understanding a lot of the job requirements but why are there data engineering jobs with a requirement to travel 10-25% of the time? I’m under the assumption that a lot of jobs can be WFH but just curious. Is it more of teaching/educating other offices?,self.dataengineering
364,pgsv68,nonkeymn,https://www.reddit.com/r/dataengineering/comments/pgsv68/we_dont_need_data_engineers_data_scientist_need/,"We Don't Need Data Engineers, Data Scientist Need Better Tools - A Data Engineers Response To A Data Scientist","A few weeks ago I came across an article titled ""[We Don't Need Data Engineers, Data Scientists Need Better Tools](https://towardsdatascience.com/we-dont-need-data-engineers-we-need-better-tools-for-data-scientists-84a06e6f3f7f)"".  Now, reading the article, this title was somewhat clickbait. The author was focusing more on the end portion the data workflow. Where data scientists sometimes need data engineers or ML engineers to implement their code.  I have many times ""productionized"" data scientist's logic.  So I get what they were going for. However, I also feel like they drastically minimized the role of a data engineer to being a support role of a data scientist vs. a key component for most businesses.  Regardless if they have a data science team or not.  I decided to respond to this article with my own content titled.   [We Don't Need Data Engineers, Just Better Tools - A Data Engineer Responds To A Data Scientist](https://www.youtube.com/watch?v=z_sR2CVG8kk)  But I would like to know your thoughts? Do you agree with the authors opinion or do you see data engineers having a much larger role.",self.dataengineering
365,pgs1z3,Kassme,https://www.reddit.com/r/dataengineering/comments/pgs1z3/spark_dfs_vs_pandas_dfs/,Spark DF’s vs Pandas DF’s,"Hello fellow DE’s, I’m curious for those of you that use notebooks to interface with spark (I’m working with PySpark notebooks in Azure Synapse) - do you think it’s worth learning how to use native spark DF’s? Currently I have been told to just use pandas because it’s what I’m familiar with but I wonder if it’s worth the effort to learn to use the DF methods that are native to spark.  Thanks in advance!   PS pls refrain from recommending an option C in this case, I don’t get to choose the stack I have to work with in this project, it’s either pandas or PySpark df’s. Thx",self.dataengineering
366,pgrgwn,shibu_76,https://www.reddit.com/r/dataengineering/comments/pgrgwn/what_is_the_semantic_data_model_what_is_the/,What is the Semantic Data model? What is the difference between Logical Data Model &amp; Semantic Data model?,I have heard the term 'Semantic Data Model' many times. But I was never been able to understand it and could not get a simple layman answer to it. Can somebody share some thoughts around it or share some material on it?,self.dataengineering
367,pgq50c,king_booker,https://www.reddit.com/r/dataengineering/comments/pgq50c/processing_different_file_format_data/,processing different file format data,"Currently in my organization, we process plain CSV's that we get from another software.  It places the files in an hdfs location, which kafka picks up and does some processing over it and pushes it into hdfs in an RC file format. Which is later converted into parquet tables in hive.   My question is, if I had say JSON/XML instead of CSV,  how should I process this? Is it better to read the json into a kafka topic, push it to hdfs as a json, create a hive external table with json serde and then convert it to parquet in other staging tables?  Or should I push the data in some other file format in hdfs?",self.dataengineering
368,pgpqzs,nullQueries,https://www.reddit.com/r/dataengineering/comments/pgpqzs/working_on_a_youtube_channel_for_data_ideas/,Working on a Youtube channel for Data. Ideas?,"Recently I've been working on a Youtube channel where I go over some topics related to data engineering, architecture, and BI.  I've done some animated explainer type ones, podcast style discussions, and recently tried the normal youtube talking head videos.  I'd love to do some more hands-on projects once I get better (and faster) at editing.  My goal is to convey complex data topics in digestible chunks with a modern level of audio/video quality.  What are some topics you would find helpful?  What style of video do you prefer?  Suggestions and ideas are welcome!  [https://www.youtube.com/c/nullQueries/videos](https://www.youtube.com/c/nullQueries/videos)",self.dataengineering
369,pgpfgh,GRHervas,https://www.reddit.com/r/dataengineering/comments/pgpfgh/is_there_any_way_to_improve_this_relational/,Is there any way to improve this relational schema for a database of gym routines?,"As part of the development of a small personal project, I would like to store all the information related to my gym routines, firstly to have some traceability and order (so far info stored in different excel files); and, secondly, for a possible further analysis.  The minimum requirements is that my planning is based on *Programs* (in the order of months), each of which is composed of different *Blocks* (in the order of weeks), which in turn contain different *Workouts* (corresponding to 1-day sessions). I would like to record my performance against planned (repetitions, weight, RPE...), load volume per muscle group, as well as historical 1RMs. This translates into the proposed relational schema shown in the linked ERD.  [ERD for gym routine database](https://preview.redd.it/dy6acqyoa5l71.jpg?width=1359&amp;format=pjpg&amp;auto=webp&amp;s=7ea2ca4387b07228f47099cde44b335075f070be)  Some doubts arise:  1. Could the relationship marked in red between *LogWorkouts* and *Workouts* be problematic? 2. Is the *Logs* part of the schema optimizable?  Any other improvement proposal or or problem that I have overlooked will be welcome :)",self.dataengineering
370,pgm25e,ApocalypseAce,https://www.reddit.com/r/dataengineering/comments/pgm25e/how_to_deal_with_db_performance/,How to deal with DB performance?,"I'm using Postgresql as a datawarehouse for a small business, with relatively low data volumes.  The data then gets read into a BI tool.  I know it's not an OLAP db, but I've been convinced that it works for most small to medium warehousing needs.  There's a db with many different schemas to store different department data. To prepare the data for BI, I write many ""VIEWS"" in a dedicated schema to serve to the BI tool.  However, I have also written some Monster SQL™ with several subqueries, with functions, lateral and left joins, ALL in ONE. And these, as one might expect, don't perform well when queried in the BI tool. And the one I'm writing now just faced a 60s timeout while it's performing its calculation.  I cannot decide yet if we need to go cloud for this data volume. Hardware is decent, running at least 4-6 cores at minimum at any one time, with up to 8GB+ of RAM available on demand for Postgres.  What can I do to improve this performance? Am I simply using the wrong tool i.e. Postgres won't cut it?",self.dataengineering
371,pglodj,espartago,https://www.reddit.com/r/dataengineering/comments/pglodj/deleting_only_azure_synapse_workspace_from_a/,Deleting only Azure Synapse Workspace from a Resource Group,"Hi!   Well, as the tittle indicateds, does anyone know if it's possible to delete or unlink only synapse workspace from a resource group?   I was doing some research but I am not clear if there is a way to not delete the rest of the resources created in that specific Resource Group.   I would appreciate any help, suggestion, articles or resources that you think might give me some clarity on the best action plan.  Thanks in advance for your time!  Pdst: I am relatively new to the Azure ecosystem so please keep that in mind, but will accept any ideas regardless of difficulty",self.dataengineering
372,pgh979,AMGraduate564,https://www.reddit.com/r/dataengineering/comments/pgh979/data_technology_fight_data_fabric_vs_data_mesh/,Data Technology fight: Data Fabric vs Data Mesh?,"What are these (definitions, examples, etc), and which one applies to which cases?",self.dataengineering
373,pggz3t,pknpkn21,https://www.reddit.com/r/dataengineering/comments/pggz3t/cicd_for_de_pipeline/,CI/CD for DE Pipeline,"Hi,  We are currently in the process of setting up a data processing pipeline for few of our data sources and the processing logic is designed in Pyspark with Airflow for orchestration in AWS environment.  I am currently looking for some references on how to setup the CI/CD process for it. Are there any articles, books or videos which cover the CI/CD and Dataops in general and the best practices for it ?  Specifically interested in knowing how to package applications with the necessary configurations/parameters and testing artifacts, continuous deployment process, infrastructure provisioning for transient clusters, ongoing monitoring and maintenance.    Please share your thoughts. Thanks in advance.",self.dataengineering
374,pggll0,DataScienceDigest,https://dataphoenix.info/data-phoenix-digest-02-09-2021/,Data Phoenix Digest - 02.09.2021,,dataphoenix.info
375,pggbqv,hatchikyu,https://www.reddit.com/r/dataengineering/comments/pggbqv/would_a_data_engineer_role_map_be_useful/,Would a data engineer role map be useful?,"A while back, I saw a data engineer ask the DevOps roadmap guy to make a roadmap for DE.   So I decided to start on a role map that might also help.   Here's a very early version:  [It's early days and I'm learning more about DE by the day.](https://preview.redd.it/vnu7tffe32l71.png?width=2191&amp;format=png&amp;auto=webp&amp;s=f062b3ce0cbaa15164fdba1231fed9e4b4840400)  Already made one for SREs (below)  [SREs seem to be finding this role map useful](https://preview.redd.it/zu0g8c1mz1l71.png?width=2511&amp;format=png&amp;auto=webp&amp;s=5a5a968da717fa6b595340738a2790c5e32581b0)  What do you think? Should I continue with the DE role map?   Also, I will need help with this, as DE is a new space for me. So any contributors will be welcomed with open arms. I'm not going to set up a Git repo yet. Going to keep it no-code (Trello) for now.",self.dataengineering
376,pgfklv,Aaron-SWE,https://www.reddit.com/r/dataengineering/comments/pgfklv/junior_data_engineering_interview_questions/,Junior Data Engineering Interview Questions?,"So I'm not applying to jobs now, but figured I'd start preparing for stuff like this anyway as my interview skills are horrible.  I'm just wondering if anyone has a list of common questions that might get asked, including both DE specific and more general interview questions?  Apologies if this has already been asked.",self.dataengineering
377,pge9a6,Complex-Stress373,https://www.reddit.com/r/dataengineering/comments/pge9a6/how_many_microservices_or_applications_are_you/,How many microservices (or applications) are you maintaining as DE?,"The title says everything. Are you on charge of 2, 4, 6, 10, 20 microservices?.   Is there any recommended amount?",self.dataengineering
378,pgddwn,the_travelo_,https://www.reddit.com/r/dataengineering/comments/pgddwn/whats_the_best_approach_to_schema_discovery/,What's the best approach to Schema discovery?,"On my landing zone, I get CSV files.  My data lake currently has landing -&gt; raw -&gt; curated  Can I just use glue to let it autodiscover the schema and use that to have a contract with data consumers on curated?  Or should I manually check the schema and compare to what's expected in raw?  What's the best approach here?  Seems like a dumb process that I have to manually tell the schema to the ETL pipeline",self.dataengineering
379,pgcq36,vinsanity1603,https://www.reddit.com/r/dataengineering/comments/pgcq36/aws_setup_recommendation_help/,AWS Setup Recommendation HELP,"Hi, I have an application that will dump json data to an object storage (probably S3). There will be five types of json, but each one will have the same schema. Now, I was thinking what AWS service to use for ETL and the OLAP/dsahboarding part.  I was thinking AWS Glue since I can take advantage of the crawlers. EMR can be an option but the maintenance and configuration overhead cost might be too much. But my question is, can AWS Glue handle incremental load to a database like PostgreSQL or Redshift, as well as data modelling?  For the OLAP System, I know it's not best practice, but I was thinking of just using an AWS Aurora  for it. Probably build a star schema on Aurora and load data from AWS Glue to Aurora. Redshift can be an option but the cost might be too much, and might be an overkill.  End goal is a simple BI Dashboard. And some python modelling. I'm looking for the best, simplest, most straightforward approach possible.  Wanna hear some thoughts and recommendations DE fam!     https://preview.redd.it/9uf599vug1l71.png?width=962&amp;format=png&amp;auto=webp&amp;s=1859c9bc7b85386266c0e7393941329c62fe2f50",self.dataengineering
380,pgc2h6,VamsiPenmetsa77,https://www.linkedin.com/feed/update/urn:li:activity:6839064801965162496,COUPONS for top Data Engineering courses on UDEMY,,linkedin.com
381,pg8guw,vinsanity1603,https://www.reddit.com/r/dataengineering/comments/pg8guw/aws_setup_recommendation/,AWS Setup Recommendation,"Hi, I have an application that will dump json data to an object storage (probably S3). There will be five types of json, but each one will have the same schema. Now, I was thinking what AWS service to use for ETL and the OLAP part.     I was thinking AWS Glue since I can take advantage of the crawlers. EMR can be an option but the maintenance and configuration overhead cost might be too much. But my question is, can AWS Glue handle incremental load to a database like PostgreSQL or Redshift, as well as data modelling?     For the OLAP System, I was thinking of just using an AWS RDS PostgreSQL for it. Like load data from AWS Glue to RDS. Redshift can be an option but the cost might be too much.      End goal is a BI Dashboard. I'm looking for the best, simplest, most straightforward approach possible.   Wanna hear some thoughts and recommendations DE fam!  https://preview.redd.it/79k0q0xyzzk71.png?width=962&amp;format=png&amp;auto=webp&amp;s=02d666f9a05f7d1d1fa39a754d270f1d39d44e00",self.dataengineering
382,pg7p0g,Prestigious_Flow_465,https://www.reddit.com/r/dataengineering/comments/pg7p0g/what_is_the_real_use_of_ms_ssis/,What is the real use of MS SSIS?,I did a basic course on SQL Server Integration Services (SSIS) and used it to upload data from Excels to SQL Server on a scheduled time. I am now wondering where is the real power of this tool?  Why is this tool so powerful and where is most used? In which scenario or need is the best option?    What was your experience?  Is it worth learning more deeply or Python/Pentaho are better to go with? Maybe it's outdated in current Big Data era?,self.dataengineering
383,pg3d2h,MOONSfan,https://www.reddit.com/r/dataengineering/comments/pg3d2h/need_help_with_group_by_cube_function_in_redshift/,Need help with Group by Cube function in Redshift.,"Hi all, I need help in doing postgres equivalent of group by cube in Redshift. I can do this in postgres but the data size is too huge. Also the number of columns I need to do group by are 5-6. In Union all my only approach? Please suggest any alternate solutions as well.   Kindly let me know if this isn’t the right subreddit to ask.",self.dataengineering
384,pg366j,jana_50n,https://www.reddit.com/r/dataengineering/comments/pg366j/what_are_the_key_differences_between_dataetl/,What are the key differences between Data/ETL Engineerand Analytics Engineer?,"The job requirements seem similar but I’m still not clear on the differences.   Also, are there differences in experience qualifications for these positions?",self.dataengineering
385,pg2pu3,stackedhats,https://www.reddit.com/r/dataengineering/comments/pg2pu3/whats_the_fastest_way_to_move_data_from_python/,What's the fastest way to move data from Python into SQL Server?,"So, I'm able to retrieve all the data from the source database, do the processing, filtering and some optimizations converting everything into a dictionary of dictionaries (to represent a portfolio and it's individual holdings).  This takes \~ 5 seconds (I used string agg to compress all the holdings of each portfolio into a pipe separated string so pulling 200K+ records is super fast) , and the goal is to then insert the data into a staging table for a data warehouse (basically the source format is awful and we're doing ETL to fix that).  I've written a wrapper around PYODBC, so it's pretty trivial to connect to the database I want to insert and execute a sql statement for insertion, but I'm concerned that the performance will suck if I'm doing hundreds of thousands of individual insertions.  Because of quirks in the data, it's much easier to deal with some of the processing in Python than trying to chain a bunch of CTE's together in SQL.  So, assuming that it costs almost no time to write this out to a csv or convert it to a dataframe or what-have-you, would there be any major performance benefits to doing that to utilize some other method of insertion?  For the business case I don't really care about a 10 or 20% performance increase (though that would still be nice if I can do so while following KISS), but I'm not familiar enough with the way these things are optimized to say what the best option is.  Ideally, I'd like to be able to reuse the solution for a much bigger table I'll be writing to the database later.  Thanks!",self.dataengineering
386,pg2eyh,Acceptable_Cap4799,https://www.reddit.com/r/dataengineering/comments/pg2eyh/data_expert_perspective_is_needed/,Data expert perspective is needed,"   Hello,  I'm working on dataset management and labeling software market research. It would take approximately 5 minutes to take the survey.  [https://forms.gle/CT9ZwEKng8VYEMpLA](https://forms.gle/CT9ZwEKng8VYEMpLA)  Thanks!",self.dataengineering
387,pg1r40,Low-Fisherman-9468,https://www.reddit.com/r/dataengineering/comments/pg1r40/future_prospects_of_the_data_engineering_industry/,Future prospects of the Data Engineering industry?,Basically what do you think of the demand for data engineers will be  in the next 10 years? Do you think the industry will flatten out or increase over time?,self.dataengineering
388,pg19p5,callmerudolph,https://www.reddit.com/r/dataengineering/comments/pg19p5/want_to_leave_data_engineering_what_are_my_options/,"Want to leave data engineering, what are my options?","Hey all, like the title says I want to leave it all behind. I'm burnt out and genuinely do not enjoy the coding, troubleshooting, pipeline building, etc. What are some realistic options for me to transition into that wont be such a great pay cut and out of the field. I only have two years of experience working as a data engineer. I'm more interested in strategy and architecture design but no idea what job to look for. any help appreciated, thanks.",self.dataengineering
389,pg19m4,YellowPride95,https://www.reddit.com/r/dataengineering/comments/pg19m4/have_been_promoted_to_a_position_at_my_work_where/,"Have been promoted to a position at my work where I will be establishing the first official data warehouse for the company (&lt;50 People). No prior experience, but my boss and management trust me I can do it. Still in undergrad part time for mathematics, want to be quant in future. How do I do it?","Currently, the only thing that is needing to be regularly updated is a API Call via Powershell to pull JSON, and then running queries via a .bat file in SQL Server to sync the new Data up into the table. This is automated through task scheduler on Windows Server 2018. The data then reflects on PowerBI.   The SQL Server &amp; this ""pipeline"" was established before I came on with someone that didn't have much IT Background, and it works well (for now). The business is Insurance, so the current ""pipeline"" is sufficient for now.   But my boss (who recently joined and is part time) shared some additional plans to add other things to our eventual Data Warehouse (e.g CRM, paycom, etc) in Q1/Q2 next year.  His background is in data, and he is aware I will mostly need to learn/teach myself. But he's shared he has the faith and the patience that I can pull it off, hence why he entrusted it to me (which is awesome!!).   I came from a Help Desk Position, and I definitely prefer this type of work over helpdesk. I've been doing this type of work for 3 months.    I know how to code (I wrote an application that helped parse out specific from existing JSON into our DB) but I definitely would like to gain as much experience as possible but deliver a great product. What do you recommend I start looking at?   My current plans by November are:   1) Establish ETL Pipeline for CRM Data   2) Utilize Apache Airflow  3) Figure out how to build a test environment",self.dataengineering
390,pg10zd,Vorskl,https://www.reddit.com/r/dataengineering/comments/pg10zd/anyone_here_is_doing_databricks_academy_sql_or/,anyone here is doing Databricks Academy SQL or DataEngineering paths?,"hi,  pls comment if you're doing either SQL or DE learning path at Databricks Academy.  I'd like to discuss some questions / issues.  thanks",self.dataengineering
391,pfxupg,Initial_Squirrel2693,https://www.reddit.com/r/dataengineering/comments/pfxupg/what_does_aws_coding_round_consist_usually/,What does AWS coding round consist usually?,"Hi,  I have a coding round at a company for a DE role. The recruiter said it will be coding in python and AWS. I know for python its usually DS and Algorithms. But what to expect in AWS coding round? I have given interviews at several companies for DE role, but this is my first time coming across such a round, Questions consisting of AWS in coding part.  The services mentioned in my Resume are S3, Redshift, Lambda, ElasticSearch, EC2, etc.   Thanks in advance :)",self.dataengineering
392,pfxa7r,raghukveer,https://www.reddit.com/r/dataengineering/comments/pfxa7r/has_anyone_read_the_database_system_concepts_book/,"Has anyone read the Database System Concepts book, how good it is?","CMU Databases course([https://15445.courses.cs.cmu.edu/fall2020/](https://15445.courses.cs.cmu.edu/fall2020/)) suggests Database System Concepts (7th Edition) by Avi Silberschatz, Henry F. Korth, and S. Sudarshan, has anyone read it? I am thinking of buying it, is it useful for DE?",self.dataengineering
393,pfx3aw,urs123,https://databricks.com/p/thank-you/ebook-migration-guide-hadoop-to-databricks-164255,Databricks book: migrate Hadoop to Databricks,,databricks.com
394,pfwuyg,AutoModerator,https://www.reddit.com/r/dataengineering/comments/pfwuyg/quarterly_salary_discussion/,Quarterly Salary Discussion,"This is a recurring thread that happens quarterly and was created to help increase transparency around salary and compensation for Data Engineering. Please comment below and include the following:  1. Current title  2. Years of experience (YOE)  3. Location  4. Base salary &amp; currency (dollars, euro, pesos, etc.)  5. Bonuses/Equity (optional)  6. Industry (optional)",self.dataengineering
395,pfv18d,Godmons,https://www.reddit.com/r/dataengineering/comments/pfv18d/spark_jobs_should_we_define_schema_on_read/,Spark Jobs : Should we define schema on read ?,"Hi guys,     When importing a dataframe from parquet or other file format in spark, we can either infer or specify the schema (data types).  On my previous company, we were using spark dataframe &amp; INFERING before processing.On my current company, we are specifying the schema on each read. Is it a good practice ? Do we really benefit from it or is it just waste of time ?   Of course, in both case, the output is saved with data types specified or casted for Data Analytics purposes.  Can you guys share your thought and experience about that ? Thanks in advance ! :)",self.dataengineering
396,pftkri,Fatal_Conceit,https://www.reddit.com/r/dataengineering/comments/pftkri/as_a_new_de_1yoe_how_should_i_approach_the_job/,"As a new DE &lt; 1YOE, how should I approach the job market to get good training?","I’ve got 6 months under my new title, but it’s not working out like I’d hoped. We don’t have access to tools like airflow and the basic tech stacks needed for the role. There aren’t any other DEs on the team to learn from and I’m scared I’ll pop out of this position with bad habits. What places are best for learning the role? Startups?",self.dataengineering
397,pfs3yw,marshr9523,https://www.reddit.com/r/dataengineering/comments/pfs3yw/indian_des_how_does_the_job_market_and_salary/,Indian DE's: How does the job market and salary look like at the moment in India?,"I saw a recent post on DE salaries in Toronto, and the comments section had a lot of info for US, Canada, Europe, but really less for India. Hence thought of making this post.  I recently read an article as well from Analytics Vidhya, saying that DE salaries were ranging around 7-20 lakhs (that's ~10k - 27k $), for a fresher to 7+ YE, respectively. Is this really the scenario? Looked pretty low to me. What are your thoughts?",self.dataengineering
398,pfqecp,Bianca_di_Angelo,https://i.redd.it/mygj8vjv2vk71.png,The Requirements bother me,,i.redd.it
399,pfpe2o,Shekhar_Dudi,https://www.reddit.com/r/dataengineering/comments/pfpe2o/how_to_upload_on_prem_data_to_cloud_via_etl/,How to upload on prem data to cloud via etl?,[removed],self.dataengineering
400,pfopmb,Marksfik,https://www.ververica.com/blog/the-impact-of-disks-on-rocksdb-state-backend-in-flink-a-case-study,The Impact of Disks on RocksDB State Backend in Flink: A Case Study,,ververica.com
401,pfoo5c,NoblemanOnAMission,https://www.reddit.com/r/dataengineering/comments/pfoo5c/firebase_to_oracle_sql/,Firebase to Oracle SQL,Usually I take whole JSON dump from Firebase Realtime Database and then use it in oracle...I want to find a way to take data from Firebase  based on the values of one column instead of the whole data using Oracle SQL developer,self.dataengineering
402,pfmclj,axtharvxa,https://youtu.be/GHW_s6tDSZg,What is data science and what is it's future,,youtu.be
403,pfkb4c,Maleficent_Ratio_785,https://www.reddit.com/r/dataengineering/comments/pfkb4c/how_much_the_tier_a_companies_like_faang_pays_for/,How much the Tier A companies like FAANG pays for data engineers in Toronto/Canada ?,"Here is the Glassdoor data which I don't think is correct  Amazon 110-140k CAD fb, Google 100k avg - I don't believe this data",self.dataengineering
404,pfk9fz,el_jeep0,https://www.reddit.com/r/dataengineering/comments/pfk9fz/taking_advantage_of_the_hot_de_job_market_to/,Taking advantage of the hot DE job market to learn a bunch of different stacks by spending &lt;1.5yrs/job.. ethical?,"Title says it all, there's a lot of different ways to do things, right now companies REALLY want data engineers with experience (because hiring for de is hard). Will switching around backfire ya think?",self.dataengineering
405,pfigsg,QuaternionHam,https://www.reddit.com/r/dataengineering/comments/pfigsg/freelance_de/,Freelance DE,"Hi! I have the opportunity to make some freelance DE jobs (dbt and cloud technologies mostly) but i have no idea on how to calculate my fee (it's per job , upfront). They'll sent me the requirements and i'll send them the fee basically  Any guidelines?",self.dataengineering
406,pfic3r,LinaVak,https://www.reddit.com/r/dataengineering/comments/pfic3r/newbie_question_on_s3/,Newbie question on s3,Hello! I have never used s3 before. My colleagues are storing some data and just told me I can access it! They gave me the user access credentials and secret access. They told me the url is company name-project name- name.  How do I access this data? I made an AWS account but don’t know how to get to the bucket? What’s the https url?? I have low programming skills..,self.dataengineering
407,pfi3dj,octacon100,https://www.reddit.com/r/dataengineering/comments/pfi3dj/is_azure_a_viable_career_path_for_de/,Is azure a viable career path for DE?,"I’m in a mostly MS shop and they mostly use c# and f# to move data around. Having seen data seem to move away from companies like oracle and ms to open source over the years, I’m interested in moving over to python and tech like snowflake and databricks, but I’m thinking it might be tough to push the tech team that way. Is it worth trying to give things like azure data factory a go or would it be better to push for the more popular tools?",self.dataengineering
408,pfhxyi,IllustriousBalance50,https://www.reddit.com/r/dataengineering/comments/pfhxyi/airflow_and_macbook_m1/,Airflow and Macbook M1,I just started an internship so I'm super green with all this technology. I'm trying to learn airflow but I'm having a lot of problems and getting build errors  with the tutorials I'm following. I'm thinking some of my problems are coming from my new Macbook (first time Mac user long time Linux) with the M1 chip. I'm using a Rosetta emulator however so theoretically all should be good?  Any advice (general or otherwise) or links to readings or tutorials would be awesome. Thank you!,self.dataengineering
409,pffh3i,needmorecharact,https://www.oreilly.com/library/view/data-pipelines-pocket/9781492087823/,"For those that have read O’Reilly's Data Pipelines Pocket Reference book, would you recommend it?",,oreilly.com
410,pfemin,Zorkol02,https://www.reddit.com/r/dataengineering/comments/pfemin/cloud_certification_help/,Cloud Certification Help,"Hey everyone!  I’ve been reading about gcp/aws/azure data engineer certifications, but I don’t know where to start. First thing is that I currently live in Spain, but will leave in a couple of months to Florida so I’m not sure if the cloud platform brand varies. Second is that I use MacOs, so I’m not sure if learning Azure will fit me, even though I started reading the Azure cloud fundamentals documents.  Can anyone give me tips? Or some type of path I should follow? I know Coursera has good stuff, but I dont know the order in which I should do it or if I should take a look at datacamp data engineer python. Also I’ve never worked with cloud platforms, but I get the idea of what it does.   I dont have work experience yet, only personal proyects. I know python, sql, bash, pyspark and airflow, but not an expert yet just in case.  Thanks a lot for your help!",self.dataengineering
411,pfbuap,Maleficent_Ratio_785,https://i.redd.it/zun2sq91mqk71.jpg,How much are data engineers getting paid in Toronto? Are these numbers from Glassdoor accurate?,,i.redd.it
412,pfbshs,PositiveVibesYourWay,https://www.reddit.com/r/dataengineering/comments/pfbshs/whats_the_biggest_data_brick_youve_taken/,What's the biggest data brick you've taken?,Mine was huge this morning. Absolute brick,self.dataengineering
413,pf86jp,OkieDaddy,https://www.reddit.com/r/dataengineering/comments/pf86jp/need_help_with_redshift/,Need help with Redshift pivoting/unpivoting/functions,"Redshift question here:     So the system I'm working in has roughly the following schema. The pivot happens in the source system, so I unfortunately don't have control over how that happens at the moment. What I am left with is a schema similar to the following.         create table vehicle_questionnaire_pivoted (id int, is_runnable int, leaks_oil int, color int, vehicle_type int, manufacture_date date);          insert into vehicle_questionnaire_pivoted values (1,1,3,4,27,'1/1/2011'), (2,3,1,4,43,'1/1/2001');          create table codes_table(id int, text_value varchar);          insert into codes_table values (1,'sometimes'),(3,'never'),(4,'blue'),(27,'van'),(43,'car');  Now, my table realistically contains between 150 and 200 different answer codes from a questionnaire,  all pivoted out to columns, and I need to look up the corresponding text values from the codes\_table. In Postgres I can do it, albeit slowly, with a function that passes in the code value, and returns the code text. used such as     SELECT fn\_get\_value(is\_runnable) as is\_runnable,   fn\_get\_value(leaks\_oil) as leaks\_oil,   from vehicle\_questionnaire\_pivoted   However, said function is not performant, and not even available in Redshift. Aside from taking pivoted data, unpivoting it back to rows, joining up, then repivoting(all of which sucks, again because redshift doesn't have a pivot/unpivot function)...or joining to the table multiple times(once per column), is there a way to do this within Redshift SQL? I know I could dump it out to a dataframe and do it all there, and I may well end up doing that, but I'd prefer to keep it confined to the db if possible, if there is a simple solution I'm just missing.",self.dataengineering
414,pf6vf3,BigMightyTroll,https://www.reddit.com/r/dataengineering/comments/pf6vf3/snowflake_vs_databbricks_lakehouse_or_both/,Snowflake vs DatabBricks lakehouse or both together," Hi! I would like to ask for your opinion on the best approach to combine data lake with the data warehouse to serve both Business Intelligence and Advanced Analytics needs. We are building a data platform on AWS. We have the luxury to start from scratch. We have a demand to integrate a lot of business systems together, so the Business Intelligence part will be big. And we would need proper management, governance and lineage there. At the same time, there are massive IoT data volumes that would need a data lake and demand for advanced analytics, machine learning, etc.  The data warehouse guys want to use Snowflake for strong data warehouse and Business Intelligence. Data scientists want to use Delta lake and Databricks for the strong support of advanced analytics and better lake technology.  Both Snowflake and Databricks have options to provide the whole range and trying hard to build these capabilities in future releases. However, I feel like Snowflake is suboptimal for lake and data science, and Datbricks are suboptimal for BI and Data warehousing.  Should we try to choose one platform and compromise some part for the sake of simplicity or take the best from the two worlds and deal with the integration overhead?",self.dataengineering
415,pf6q2g,PencilBoy99,https://www.reddit.com/r/dataengineering/comments/pf6q2g/advances_in_modeling_representing_or_organizing/,"Advances in Modeling, Representing, or Organizing Data","Putting aside how data is physically organized and transformed, or where it's organized and transformed, are there any new advances (or just approaches you like) in organizing all this data that's been collected? So you have Star Schema like structures (maybe they're physical, maybe they're in memory, maybe they're just in some user's Power BI data set) - anything else new?",self.dataengineering
416,pf1od7,ps2931,https://www.reddit.com/r/dataengineering/comments/pf1od7/what_it_takes_to_be_a_deds_manager/,What it takes to be a DE/DS Manager?,Hi  I am an IT Manager. I don't manage any tech stuff. My role is purely resource management and project planning. I am interested in Data Sciences. Is their any specific skill set I need to learn to get a Data Science Manager role. What about online MBA. Is there any online mba specialisation available in DS/DE?,self.dataengineering
417,pf0933,GreekYogurtt,https://www.reddit.com/r/dataengineering/comments/pf0933/which_offer_to_choose_among_two_great_offers/,Which offer to choose among two great offers.,"I have two offers in india. Walmart labs vs level ai (US + new delhi based AI startup) recently got 13$ million series A funded.  Here's the pros and cons :   Level :  Location. Nice team. Only data engineer - opportunity and challenge.  Upside - the startup becomes huge.  Cons -   Startup fails(2 yrs) someday and I have to look for new job. No brand recognition. I need that cuz no top tier clg/job on resume. Will learn high level stuff but not deep one.  Walmart :  Great brand recognition in india, comparable to amazon - easier to move to big tech. I'm from a largely unknown college. Learn good engineering practices, from seniors and peers.  Cons : Location. Only walmart is the major tech company in Chennai. Need to stay here for 1.5yrs at least.  Don't know the teammates as the team is currently being built. Hiring manager seemed nice though.",self.dataengineering
418,peyiec,lurakona,https://www.reddit.com/r/dataengineering/comments/peyiec/thinking_about_pivoting_from_swe/,Thinking about Pivoting from SWE,"So for context, I am 23 year old and I graduated college last year March. I recently landed a role as Fullstack SWE. During my onboarding, I had to meet with the entire team and learn what everyone does. I spoke with a Data Engineer and this is what originally sparked my interest.  Anyways, my interest started to grow more and as the icing on the cake was me being moved to another team that stands in between the SWE team and the Data Scientists and Engineers. This team will allow me to learn more about the company's platform. I'll be doing a lot of debugging, but more importantly, it will give me the ability to pivot back into the SWE team or Data Engineering.  This basically means I'm in the perfect spot. I know the pay outlook is at some point going to get me to  100k+ which is good to me, as it will allow me to live comfortable. I'm currently at 75k. I'm also aware that the job outlook is actually growing for DEs. That being said, I've watched YouTube videos, but DE is not as popular as Data Scientist (which i do not want to become) and the notorious Software Engineering.   So that lands me here, I just want to know what the day-to-day REALLY looks like for a Data Engineer, is it super complex? Is it literally just living in SQL? Do I have to code super heavy like SWE? I know it varies from company to company, but my goal here is to get different/same povs and make a decision.",self.dataengineering
419,pewmbg,FickleLife,https://www.reddit.com/r/dataengineering/comments/pewmbg/azure_synapse_is_the_wrangling_data_flow_available/,Azure Synapse - is the Wrangling Data Flow available?,"I see articles about ""Wrangling Data Flow"" - effectively Power Query - in ADF but not in Synapse. I have had a look in Synapse and can't find it. Is it available yet?",self.dataengineering
420,per2m2,sudofk,/r/apachespark/comments/per0an/how_to_calculate_the_sum_of_the_results_returned/,How to calculate the sum of the results returned from the count function in a DataFrame using UDAF?,,self.apachespark
421,peq4be,m_usamahameed,https://www.reddit.com/r/dataengineering/comments/peq4be/kafka_stream/,KAFKA STREAM,"Anyone experience with KAFKA STREAM with Scala language? I'm a beginner in Kafka and currently, I'm taking help from Documentation but I completely lost (Due to heavy terms used in it.) 🥺  Can anyone share helping material or any hands-on experience tutorial or guide? 🤗  Thank you. 🤩",self.dataengineering
422,peptp0,Material_Cheetah934,https://www.reddit.com/r/dataengineering/comments/peptp0/how_do_you_serialize_and_save_transformations_in/,"How do you serialize and save ""transformations"" in your pipeline?","How do you guys store your transformations?  We have reports and regular file extracts that we generate for customers. For each report/file we have a CRM that we can tie back to the columns/tables/schema/sources that the data comes from. This helps us figure out right away if a request is duplicate hence, we can recycle existing logic instead of creating a new module. At least that is how it is supposed to go, one wrench that I cannot seem to figure out how to store is the transformations piece. Sometimes some of customers like data manipulated a specific way or some like data grouped and aggregated by a specific column. How do you capture that in a report/file specification that you can then use in a comparative fashion against other report/file?",self.dataengineering
423,pep8ew,prasanna_aatma,https://www.reddit.com/r/dataengineering/comments/pep8ew/data_bricks_academy_self_paced_learning_coupon/,Data bricks Academy self paced Learning Coupon code,"Hello fellow data people, try this code **DB\_EDU** ... working as on today \[31st August 2021: +5:30 GMT\]",self.dataengineering
424,peoguf,theporterhaus,https://www.reddit.com/r/dataengineering/comments/peoguf/data_engineers_that_transitioned_from_a_noncs/,"Data Engineers that transitioned from a non-cs role, what role did you transfer from?",  [View Poll](https://www.reddit.com/poll/peoguf),self.dataengineering
425,pentmy,metaplane_hq,https://www.metaplane.dev/blog/inside-data-with-ben-cohen-spoton,"Interview with DE leader at SpotOn (ex Cameo, Braintree, Cars.com): ""Focus on the most important company and team goals, and forget about imposter syndrome.""",,metaplane.dev
426,pemw62,WhoseCodeIsThis,https://www.reddit.com/r/dataengineering/comments/pemw62/alternative_tools_to_dbt_sql_and_python_for/,Alternative tools to DBT / SQL and Python for writing business logic? Trying to prevent creating a mountain of undocumented spaghetti,[removed],self.dataengineering
427,pel2v5,pendulumpendulum,https://www.reddit.com/r/dataengineering/comments/pel2v5/what_does_the_career_progression_look_like_for_a/,What does the career progression look like for a Data Engineer?,"This is my first job out of college (Data Engineer) and I was hoping I would do something more machine learning oriented or at the very least get to use some programming langauges (Python, Java, whatever) and do some coding. Instead all I use is Informatica and SQL to do ETL. What does the career progression for this role look like? Should I jump ship to something more SWE-oriented? What does the long-term compensation look like for a data engineer vs a more traditional software developer role?",self.dataengineering
428,pekrif,hairbear1234,https://www.reddit.com/r/dataengineering/comments/pekrif/de_contractor_at_faang_advice_for_quitting_taking/,"DE contractor at FAANG - Advice for quitting, taking some time off, and then jumping back in.","TLDR: Advice on quitting a job for 'personal reasons' aka burned out and wanna take a few months off to recoop as well as look for a FT job that aligns more with my values. Obviously don't want to tell my manager the real reason, so what is best etiquette? Also, does a few months gap of employment matter when applying for jobs?  Begin ranting --   I signed up for a 6 month DE contract job at a FAANG. I'm 4 months in and they want to extend my contract and have me interview for FT.   I have realized that Im not super excited for the work - SQL shop vs my last job of building cloud infra, airflow, python. I'm not a fan of the culture - I've had plenty of 'meetings' where my manager calls to vent about the other people on my team and how he's such a great manager. Also everyone is super stressed and stretched thin. WLB is pretty bad - pinged at all hours of the day and night including weekends. I'm on a 'high impact' team so I imagine it may not be as bad on other teams. They just need people to throw into the meat grinder, so I understand that contractors will get the worst of it which is why there is such high turnover.  Overall, when I think about it, sure, I could get $250TC and do it for a year as a fulltimer, and maybe it will be better since I'll have ownership or different team, but do I really want to do that? And after thinking about it, I really don't. My job before this paid half of what I'm making now, but I really liked the team and the work was actually more technically (programming) difficult and interesting.   The past year with covid definitely has highlighted that life's short and that at the end of the day, I could really care less about increasing conversion rate.  Now that I have the technical chops in the last few jobs as well as experiencing what it's like at a 'top' company, I feel like what I care about now is a good manager and team, better WLB than 60-70 hrs a week, and honestly, more programming in python. Also a company that doesn't drive people to be dicks. Preferably one that aligns with values (aka doesn't make the world a worse place?)  My plan is to give my two weeks notice later this week, and then take the next few months off to do the things I've been holding off on due to work the past few years. Then when I start looking again, I want to be more deliberate in the job search instead of blasting 100 job apps and choosing highest pay or best opportunity, now that I have experience. I have 3 years of DS/DE experience and a masters. 5 years total work exp.   We are fortunate to be in a high demand field with good pay and flexibility. It's allowed me to save up to take some time off as well as not have uber anxiety about having something lined up.   Does this seem reasonable? Am I being delusional or too optimistic about work?    I'll probably have a followup post once I have some time to reflect on what other things I want out of a job, and how to go about the job search more deliberately.   Thanks for all your input and advice. Much appreciated!",self.dataengineering
429,peieca,Ok-Sentence-8542,https://www.reddit.com/r/dataengineering/comments/peieca/how_do_you_guys_validate_your_data_how_does_your/,"How do you guys validate your data? How does your process look like and which tools are you using? Our main Stack: Databricks, Azure Datafactory, Data Lake","What kind of data validation tools are you guys using for validating your big data? We are using databricks and azure data lake and would like to test our parquet files against expectations such as:  &amp;#x200B;      Pipeline fails if:      - ""customerId null"" or ""customerId is not unique""     -  productPrice is smaller or equal to 0     - ...              Behavior:            --&gt; stop pipeline (don't write corrupt records)            --&gt; Create new jira issue of type bug               Pipeline alerts if:     - the number of rows changed more than 20% between yesterday and today     - ...              Behavior:         --&gt; Stop pipeline         --&gt; Send Email to Data Owner with Info              if Data Owner Accepts change as nominal:                 --&gt; finish pipeline       I've heard about [great expecations](https://greatexpectations.io/) and that [delta lake](https://delta.io/) wants to introduce expecations but to my knowledge its not yet released. So how could we model this process generally? We sadly use azure datafactory for orchestrating data pipelines. Could we use databricks + Great Expectations + some Error Handling Logic (Logic App, etc.) to model this workflow in azure datafactory?  &amp;#x200B;  Does someone have a reference architecture for this?  &amp;#x200B;  Cheers",self.dataengineering
430,pei4z6,insanetech_,https://www.lunaticai.com/2021/08/kafka-definitive-guide-2nd-edition-pdf.html,"Kafka: The Definitive Guide, 2nd Edition eBook",,lunaticai.com
431,pehg4l,imbrad91,https://www.reddit.com/r/dataengineering/comments/pehg4l/moving_to_data_engineering_utilizing_your/,Moving to data engineering - Utilizing your company's data infrastructure to learn DE within a corporate environment?,"Hi all,  I'm currently working at a project management office at an F100..and trying to pivot to a more data heavy role and after exploring the 'big 3' in data (analytics, science, and engineering); I believe DE is more of a match for me as I enjoy systems, backend logic, databases, etc. Eventually, I'd like to try building small pipelines where I can in my own role (even though the data we have is no where near 'big' data). But, im noticing in my company a lot of 'gatekeeping' when it comes to gaining access to certain tools. For example, we have implemented Azure last year, but all access to anything Azure related is blocked off by access which is linked to your job title and team.   Same gatekeeping goes for any data warehouses we have as well, and pretty much any SQL queries from what I heard have to be approved for quality before they can be used.   I understand why they do this, but how is one supposed to actually learn and pivot into a more data heavy role within a company when everything is just gatekept? Does any Data Engineers in here who learned while employed in a different role at a big corporation have any experience with practicing your learning &amp; development while utilizing your company's infrastructure?",self.dataengineering
432,peeqkc,Sumit0108,https://www.reddit.com/r/dataengineering/comments/peeqkc/can_we_run_machine_learning_code_in_standard/,Can we run machine learning code in standard Databricks cluster (even without explicitly using ML cluster)? Also what will be cost implications in case if I'd like to utilize ML cluster?,"Hi,  I'm exploring options for selecting Databricks cluster. Little confused as I can install ML libraries in Databricks. Looking out for trade-offs.",self.dataengineering
433,peek26,Illustrious_Role_304,https://www.reddit.com/r/dataengineering/comments/peek26/future_of_snowflake/,Future of snowflake,"In my org , we are moving from big data aws tech stack to snowflake based stack Just wanted to.know hows the future for snowflake  Will it fetch high paying jobs ? How many it will have charm",self.dataengineering
434,pebyoj,AmDprimer,https://www.reddit.com/r/dataengineering/comments/pebyoj/how_important_is_data_modelling_and_normalization/,How important is data modelling and normalization concepts in a Data Engineer job,[removed],self.dataengineering
435,peaup2,vananth22,https://www.dataengineeringweekly.com/p/data-engineering-weekly-54,"The 54th edition of data engineering weekly focuses on Uber's building scalable streaming pipeline, Confluent's How ksqlDB works, internal architecture, Microsoft's ML program management at scale, Great Expectations maximizing the productivity of the analytics teams, Shopify's five steps for",,dataengineeringweekly.com
436,peap7h,CoMatrix1234,https://www.reddit.com/r/dataengineering/comments/peap7h/a_self_governing_data_gateway/,A self governing data gateway,"In our experience we are spending a lot of time and money in data engineering and still falling short in  data governance. We are building a dataset api for accessing our data with automated governance. Automated governance is achieved by making decisions using knowledge graphs that represent sensitivity labeling, business context, business rules and business compliance. Knowledge graphs are built using business glossary (ontology, taxonomy, data definitions) and labeling the attributes on the dataset.   Do you guys see any issues with this approach?  Our ecosystem: AWS, Okta, S3, terraform, looking at Amundsen, Spark, cloud monitoring tools",self.dataengineering
437,pe9f42,TiDuNguyen,https://www.reddit.com/r/dataengineering/comments/pe9f42/purely_columnar_database_suggestion/,Purely columnar database suggestion,"Can someone suggest me a *purely* columnar database. My usecase requires it to  * optimize on read/add/remove columns * scale well with column number  * ease of use/maintenance, something as easy to read by columns as a `parquet` file but also support updating by columns.   I already have some options, but they are either too complicated (a lot of bloated features that I don't require) or not *purely* columnar (e.g.: Cassandra actually store data in partitions of rows and columns rather than actual columns).",self.dataengineering
438,pe3i1b,OkDetective1169,https://www.reddit.com/r/dataengineering/comments/pe3i1b/data_science_domain_job_title_and_description/,"""Data Science"" Domain: Job Title and Description confusion","Hey all, been lurking the past couple of weeks and I love the vibe here. I just had some questions that still seem unclear to me as I plan my future.  Just for some background: I just graduated from undergrad and ended up in a two year entry level training program in a data science organization at a really big, really old, healthcare company. It's been fun work this far, and I've been doing a lot of research about what I want to do in the future, because essentially I have to pick my path after the two years is up.  I really love the engineering aspect of helping the data scientists get what they need through pipelines using AWS/SQL/PySpark/Python and sometimes Tableau. I think I'm pretty decent at it so far from luckily from my experience in academia, although I know I still have a lot to learn and improve to be more efficient at scale.   I'm labelled an Analyst within my organization, do these skills translate to a data engineering role?  How does the title ""Data engineer"" differ across companies, and how do you prepare for this in future interviews? I want to learn discernment in this area so I don't take a role that isn't relevant to the skillset I'm building  For example, the people with the title ""Data engineer"" in my organization seem to just be focused on warehousing and management, and they don't seem to play much of a role at all in the products/models we make besides maintaining the cloud server. This is totally cool, but something I'm probably only partly interested in.  What is ETL? I feel I've researched this many times and get answers ranging from ""bitch work"" to ""building data pipelines"". If it's the latter, then why do people talk down on it?  I've already tried to Google some of these questions but the answers still seem unclear. I apologize in advance if any of this is repetitive, and I appreciate your time!",self.dataengineering
439,pe354d,syberman01,https://www.reddit.com/r/dataengineering/comments/pe354d/mainframe_offloading_is_db2_mirroring_a_good/,Mainframe offloading: Is DB2 mirroring a good tactic for making data available for modern apps?,"An application-modernization project is dependent on a db in DB2-mainframe.   Mainframe-guys are touchy feely and highly political - for whatever reason.  Since some 10+ apps depend on that same db, I think it is better to push for   ""mirror that db to a db2-linux-box""  Assumptions I'm making:   # Mirroring effort is one time political price, those 10+app/teams don't need to fight that political-price 10+ different times.  # Mirroring OUTSIDE mainframe avoids $MIPS cost of connecting to that mainframe.   And the MIPS-for-mirroring is shared cost of those 10-projects.  # Mirroring DB2-DB2 may not have additional licensing cost, or one time license cost.  # Once a mirrored DB2-linux is available, many things like kafka streaming, or other CDC tools can be applied without any mainframe-politics. All downstream apps will depend on that db2-linux, for further data-processing.    Is there any big holes in this line of thought?",self.dataengineering
440,pdywfg,Solid-Exchange-8447,https://www.reddit.com/r/dataengineering/comments/pdywfg/build_a_dwh_by_sql/,Build a DWH by SQL,"Hi all,   Taking the approach ""learning by doing"" because I find knowledge without project is useless for engineering. Of course that I'll be balancing between theory and practice with focus on the latter part. Therefore....  I'm looking for a resource/guided project that can guide me through (designing)/building Data Warehousing.   Any wise words are welcome for this approach!!!  Thanks heapssss &amp; peaceeeeee",self.dataengineering
441,pduu6y,Quan-Beo,https://www.reddit.com/r/dataengineering/comments/pduu6y/newbie/,Newbie,"Hello guys. I'm new to IT. After watching some introductions, I'm interested in Data Engineer. Can u guys give me a career path ( with resources if you can ), i have found it on google but it seems to be advanced. Tks guys.",self.dataengineering
442,pdt9z1,Right-Bathroom-5287,https://www.reddit.com/r/dataengineering/comments/pdt9z1/etl_guy_trying_to_be_data_engineer/,ETL guy trying to be Data Engineer,"Hello All, i am having 3.4 years of experience in ETL informatica and iam planning to upskill and change my company.  I got a week off from work as a break from burnout and planning to learn some data engineering tools or platforms, which can help me in getting better salary package during my on going job change..  what do you guys recommend and what's your opinions on this.",self.dataengineering
443,pdt2va,boggle_thy_mind,https://www.reddit.com/r/dataengineering/comments/pdt2va/ingesting_google_ads/,Ingesting Google Ads,"I'm looking into ways to ingest (EL) **Google Ads** data (for the first time, never done it before), so far I've looked into Fivetran, Stitch, Singer, Meltano and Airbyte - all of them indicate as having google-ads as a source. I've setup my development account and tried running the singer tap and got an error message saying that I need to be using the new API - 'New Developers must use the Google Ads API...'. It seems that if you had you developer account setup before, you can still use the old API and Singer tap (though the API will be deprecated in 2022), but new ones need to use the new API (and no Singer tap). Because Singer is having these issues I assume Meltano will as well.   **Question:** Anyone has any experience setting **Google Ads** recently with any of the providers mentioned above and did you encounter any issues? I'm somewhat hesitant as both Fivetran and Stitch have links to the Adwords API documentation, which might mean they are using the old API.",self.dataengineering
444,pdni20,sedition8,https://www.reddit.com/r/dataengineering/comments/pdni20/spark_design_patterns/,Spark Design Patterns,"Hey, I'm looking for some resources to prep for a system design interview. I know that the company I'm applying to uses spark for stream processing and asks questions based on their current infra.    I'm looking for resources that show how Spark would get integrated with an end to end system.     I understand that in said org's case:   1. Spark receives data from Kafka queues 2. Spark aggregates the data.     What I don't know is  1. How to store the data to make it easy for a web app to use.     Please send me any resources you have where this is explained in detail.",self.dataengineering
445,pdkps1,Material_Cheetah934,https://www.reddit.com/r/dataengineering/comments/pdkps1/how_do_you_guys_juggle_bureaucracy_and_deadlines/,How do you guys juggle bureaucracy and deadlines?,"Not sure if this post with resonate with start-up folks as I feel like they should generally have more open reins.  I work for a Fortune 100 company, we have to work with PHI. So with that said, everyone is scared shitless of doing anything that might risk exposure. Keep this in mind for the later portion of this post.  We have a general ""IT"" department that more or less acts like the ""guardian"" that business needs to go through so that they don't show up in the news. Without their sign-off, you can't do *anything* tech related. Needless to say, they move slow AF, but claim to be agile friendly(which seems to be on par for the course).  I was hired to move a SAS(yes...SAS) pipeline(their words, not mine) to Azure. At the same time another adjacent group is working on a similar thing with a different system. They are moving their on-prem managed informatica ETL into Azure because they cannot make any pipeline changes on time due to red tape overhead(their words). They invested in a POC environment, by that I mean they dropped 10 large to the ""IT"" guardian group to set up shop in the large enterprise Azure environment. You are not allowed to go around them, and 10 large is the lowest amount of money they take to play with your business group.  The architect and engineer the business group hired created a nice diagram that showed the flow of data at  a high level and then another set of diagrams that showed a detailed view. When it came time to start implementing, they couldn't set up anything, turns out they weren't allowed to change configuration of services without staff from the ""guardian"" IT department. They said they were told not to use app principals and instead store secure access keys for things like the blob service into key vault to access through scopes. Both of them said they've never seen or been in a company that is that anal about an isolated POC environment. Mind you there is **no** PHI floating anywhere. The two are using mock data to simulate data flow. After setting up a consultation with the ""guardian"" IT group, I found out that the POC is merely step 1. The run up to the production has constant architecture review board meetings, security assessment meetings, and then subsequent funding meetings(literally to meet about who is going to pay). Essentially your architecture goes through the review board and makes its way back to you with ""fix these"", akin to a PhD defense, but you are not allowed to fight back.  On top of that, certain services are not ""available"" or certified for use, such as DataBricks, because of ""security"" concerns. Last I checked DataBricks was HIPPA compliant given a proper setup.  Needless to say, I got torn a new one yesterday during my proposal meeting with the director/management. I kept mentioning that the adherence to a tight timeline is not advisable due to red-tape set up by the IT ""guardian"" department. My manager seemed to have my back as he kept re-iterating this review board process. The director was concerned about viability and whether or not a POC can be done on time. So he basically laid into me about the review process and why I am worried about it, why I can't just finish the POC on time. Mind you, my funding has not been approved to be disbursed...I've been literally working on my MSDN sub's $50 credit to make really really small POC pipelines with fractions of mock data. I really didn't have an answer and just froze. The manager stepped up for me, but he got shut down too. Essentially the director wants to hear something more discrete about factors that delay rollout other than red-tape or other experiences of the adjacent group with a similar use-case. He also asked if my manager has considered hiring more talent that can work with Azure. Kind of a low blow, but whatever.  I am starting to wonder, those of you that work in large corporations, you must have gone through something similar, right?   Personally, I feel like it is kind of weird to be so anal about an isolated POC environment that connects in no-way to actual business data. On top of that I think using app principals is helpful for isolating or controlling secure access to specific services. The crazy access policing reminds me of my high school computer lab days. Cherry on top is limited access to the full list of Azure services because ""security"" concerns that seem refutable.  I feel like to be successful, you gotta rise above that nonsense, just looking for pointers or other advice anyone can offer.  I am thinking of writing a formal report that goes through each of the known delay factors and type of mitigation strategy. But I feel like since I don't have an active POC going, the director probably won't care and might lay into me again. Just kind of odd to guarantee a timeline, when you *know* there are delay factors that you *will* have to traverse due to company structure.",self.dataengineering
446,pdka0i,HighlyIllogicall,https://www.reddit.com/r/dataengineering/comments/pdka0i/creating_analytical_layer_using_spark_on_top_of/,Creating analytical layer using Spark on top of Parquet files,"Hello,  I am trying to build an analytical layer using Spark I am still in the planning stage, the idea is we want to try new technologies within my team to see the potentials of such technology.  so, the source system is a SQL server system:   \- I am thinking of pulling all the data in a parquet format using Nifi    \- Using spark I am gonna create tables/views out of the parquet files  \- connect Power BI and other visualization tools to Spark views/tables  However my concern is:  \- How to update parquet files whenever there are changes in the source? is that possible?  \- will it be better to read directly from SQL tables and update the spark fact/dim tables? what made me consider parquet format is the fact that it is recommended by spark to be used.",self.dataengineering
447,pdhvp1,datameshlearning,https://datameshlearning.substack.com/p/favorites,List of the Best/Most Useful Data Mesh Content,,datameshlearning.substack.com
448,pdflom,Ajudada,https://www.reddit.com/r/dataengineering/comments/pdflom/how_to_change_career_from_it_operations_to/,How to change career from IT operations to development field? (Please read description),"I work for a pharmaceutical client in US.   While I was fresher and unaware of process to get tagged in a project, I realised after 4 months that I am not going to do any code development here.   I have now 2 years of experience in operations. I have debugging knowledge. I have good programming skills and basic AWS skills (S3, Athena, EMR, Glue).   Do I need to switch to development field by faking my 2 years of ops experience as Dev? Or Do I need to stick with my experience and find opportunities and showcase interviewer my knowledge in Dev?",self.dataengineering
449,pdfj10,Pitiful_Salt6964,https://www.reddit.com/r/dataengineering/comments/pdfj10/excel_powerquery_powerpivot_data_security/,Excel (powerquery/ powerpivot) Data Security,"What are the best practices to try and implement when trying to create some sort of infrastructure for Data Analysis? Currently just using Excel (powerquery/ powerpivot)  We have a 3rd party Network Admin so I didn't have someone at hand I could ask, I feel like he would be annoyed &amp; honestly he doesn't get paid to answer questions does he? I think his company invoices per job or probably something like that. I was thinking of requesting a new mapped network drive to assign for Data Analysis, would it be okay to have both exported data and the reports on the same drive but separate folders?  Can the folders security permissions be adjusted per user/group per group polices in AD?My networking knowledge isn't all that great so please correct me if i'm wrong.  If the user doesn’t have access to the data but does to the report, will the report work? Are there more settings I'd have to mess with within the report to get the data more secure? I see that there are privacy levels when a popup shows up with a checkbox to ignore policy levels.  Can someone please point me in the right direction as to what I need to learn?",self.dataengineering
450,pden85,babykinz777,https://www.reddit.com/r/dataengineering/comments/pden85/de_interview_process_for_quantumblack/,DE Interview Process for QuantumBlack,"Hi fellow DE's! I got an email to participate in QuantumBlack's coding assessment as part of the first step in their interview process for a junior data engineer position. Apart from the info they've sent and what's on glassdoor, how should I prepare? Would love any specifics like what questions they asked as well as the salary expectations from anyone who has gone through the process. Thank you!",self.dataengineering
451,pdejm8,Simonaque,https://www.reddit.com/r/dataengineering/comments/pdejm8/career_advice_unsure_if_i_should_pursue_a_masters/,Career Advice: Unsure if I should pursue a Master's of Computer Science since I have a non-technical Undergraduate Degree.,"Hi, I'm a Data Engineer in Calgary, Canada and I'm not sure what my next step should be. My situation is as follows:  - 3 months - present: Data Engineer for a Canadian Unicorn - 1 year: Data Analyst for a FinTech - 1 year: Jr. Business Analyst - Certificates: AWS CCP, AWS DAS, DB Spark Developer, MCAF, Gitlab Assoc. Currently studying for MCADEA. - Undergraduate degree: Political Science - University certificate in Data Programming(see Data Engineering) - Bootcamp Diploma in Data Science  I found a university that would accept me with my current education for a Master’s of Computer Science(remote learning) that I could complete in 16 months. I know Master's degrees are not that import in Tech, which is why I'm unsure if I should go through all that effort and cost. My goal would be to eventually move the United States and work for a company like Apple as a Data Engineer. My questions are as follows: - Could I get an HB1(or other work visa) for a Data Engineering position in the US without a technical degree, just based on my work experience and supplementary education? This is one of my justifications for considering the MsC. - How important is it to get the Master’s degree if I want to further my career as a Data Engineer regardless of location? Would I lack career progression without one or does work experience trump it? I’m worried that recruiters might filter applications based on education and exclude me. - Any other career advice/tips would be welcome",self.dataengineering
452,pdejcx,DataD23,https://www.reddit.com/r/dataengineering/comments/pdejcx/how_much_depth_should_one_have_in_ds_algorithms/,How much depth should one have in DS &amp; Algorithms for a DE position in a FAANG company?,Just as the title suggests I’d like to know (preferably from individuals who work at FAANG or who have worked at FAANG) the level of depth in DS &amp; Algorithms required to crack DE interviews at FAANG companies.   And How did you prepare for your interviews? (The DS &amp; Algorithms section),self.dataengineering
453,pddrm6,MaxGanzII,https://www.reddit.com/r/dataengineering/comments/pddrm6/amazon_redshift_research_project_effect_of_unused/,"Amazon Redshift Research Project : Effect of Unused Columns on Query Performance (white paper, PDF)","https://www.amazonredshiftresearchproject.org  https://www.amazonredshiftresearchproject.org/white_papers/downloads/effect_of_unused_columns_on_query_performance.pdf  Redshift is column-store and as such in principle queries are unaffected by the unused columns in the tables being queried. In practise, accessing any single column is unaffected by the number of columns, but, tentatively, it looks like accessing multiple columns shows that the more columns are present between a column and the final column in the table, the slower it is to access the column, and so the more columns are present in a table, the slower access becomes for all columns. The slowdown, even when there are 1600 columns, is very small for dc2.large and ra3.xlplus, but is much larger for ds2.xlarge. However, for normally sized tables, up to say 100 columns, even on ds2.xlarge the slowdown is very small.",self.dataengineering
454,pddhgi,MaxGanzII,https://www.reddit.com/r/dataengineering/comments/pddhgi/amazon_redshift_research_project_zone_map_sorting/,"Amazon Redshift Research Project : Zone Map Sorting By Data Type (white paper, PDF)","https://www.amazonredshiftresearchproject.org  https://www.amazonredshiftresearchproject.org/white_papers/downloads/zone_map_sorting_by_data_type.pdf  Redshift provides functionality to sort rows in a table according to the values in the table columns. Sorting is however not according to the entire values in the columns, but according only to an eight byte signed integer, known as the sorting value, which is derived from each value, where the method used to derive the sorting value varies by data type. This document describes in detail the methods used to derive sorting values. About half of the data types sort as would be intuitively expected, but half do not, and system designs which assume all data types to sort as intuitively expected are not functioning as their designers expect.",self.dataengineering
455,pdc5xv,samboylansajous,https://www.youtube.com/watch?v=QETFcaIyFeE,How to build a Flappy Bird A.I. in 10 minutes!!!,,youtube.com
456,pdanrj,Born-Comment3359,https://www.reddit.com/r/dataengineering/comments/pdanrj/are_andy_pavlos_lectures_good_to_prepare_for_a/,Are Andy Pavlo's lectures good to prepare for a FAANG data engineer interview?,"I couldn't find any good unified resources to prepare for a FAANG DE interview, and a lot of people adviced me Andy Pavlo's lectures on Youtube. So I am asking people who have already successfully passed a FAANG interview, would you advice me to take a look at those lectures?",self.dataengineering
457,pda1hh,Cloudy_Waves,https://youtu.be/Va7qiZacBu8,How to Become a Microsoft Certified Azure Data Engineer?,,youtu.be
458,pd7p1x,urs123,https://www.reddit.com/r/dataengineering/comments/pd7p1x/facebook_ai_research_now_open_source_reimagining/,"Facebook AI research, now open source: Reimagining database querying on unstructured data","[Facebook AI blog: Reimagining database querying on unstructured data](https://ai.facebook.com/blog/using-ai-for-database-queries-on-any-unstructured-data-set/)  I know this is more AI related than DE, but I feel that this would open a lot of possibilities in DE world.  To quote the blog post:  &gt;Why it matters:   &gt;   &gt;Neural may one day enable people to also query any data that is available online and is not stored in a traditional database. Such data can include text, images, and other modalities. Because so much of the world’s information exists outside of traditional databases, neural database technology could one day be used to access this data for specialized research, everyday tasks, and much more.",self.dataengineering
459,pd5lv0,randomdatascientist,/r/googlecloud/comments/pd5d9g/what_are_the_most_cost_effective_google_cloud/,What are the most cost effective Google Cloud products (or any ecosystem really) for running web scraping/http scripts and updating a database on a scheduled basis?,,self.googlecloud
460,pd5k9j,TheSqlAdmin,https://thedataguy.in/debezium-with-aws-msk-iam-authentication/,Debezium With AWS MSK IAM Authentication,,thedataguy.in
461,pctg6h,statistically_broke,https://www.reddit.com/r/dataengineering/comments/pctg6h/parquet_row_group_indexing/,"Parquet Row Group ""Indexing""","Posted this in another subreddit as well, but figured would cast my net wider since this is a bit niche:   I'm not what you would call *the best* or *most knowledgeable* on Parquet files, but I recently came across their file structure and was wondering..  Is it possible or even worth trying to checksum a row group?  Desired use case: filter out ""already processed"" row groups from a parquet file to limit the amount of data kept for downstream merging with existing tables.  For reference, using primarily Pyspark on Azure Databricks and processing from a Data Lake to a Delta Lake.  Ideal Process:   1. Grab IngestionParquetFile from Data Lake.   2. Scan and Checksum each of its Row Groups.   3. Filter to get Row Groups whose Checksum is not in a historical database.   4. Save Filtered ""new"" Row Groups as UpdatesParquetFile.   5. Save the list of Row Group Checksums in the UpdatesParquetFile to keep track of processing them to the database.   6. .... downstream processing of the much smaller UpdatesParquetFile.",self.dataengineering
462,pcrlzi,Professional_Crazy49,https://www.reddit.com/r/dataengineering/comments/pcrlzi/confused_about_de_vs_sde/,Confused about DE vs SDE,"**TLDR: What exactly is the difference b/w software engineers and data engineers? Also, please suggest some DE courses :)**  Background:   I currently work as a data scientist and I don't really enjoy the business aspect of data science. I don't like the aspect of suggesting recommendations to the business based on data without any guidance. I just don't get how someone is supposed to do that without any business experience. I also realized that **most** DS roles are just hyped up data analyst roles. At my job, I mostly work on PowerBI, python and SQL for data analysis (I definitely don't have the programming skills of a SDE).  The Confusion:  So one of my friends works as a software engineer 1 and he has worked with kafka, docker, aws, prometheus, spark. Now this is what confuses me - aren't these similar to the tools used by a DE? So what exactly does a software engineer do that is different from a data engineer? I tried researching  more on software engineering to understand the job scope but I got really confused. What I have understood so far is that there is a backend, frontend and full stack software engineer. Most software engineering positions require very solid knowledge of data structures &amp; algo and leetcode but there is nothing clearly mentioned on what tasks do they actually carry out?   Also, I was thinking of starting some DE course to see if I enjoy DE work or not. What courses would you recommend? I saw the dataquest DE course and lots of courses on Coursera and Udemy. Should I first start learning data structures and algo and work on leetcode as both DE and SDE need that or should I  first start a DE course to see if I actually like it.",self.dataengineering
463,pcoxd7,Air_Rogue,https://www.reddit.com/r/dataengineering/comments/pcoxd7/steps_to_become_a_data_engineer/,Steps to become a data engineer.,"Just as the title says, what are the steps you guys recommend I take to become a data engineer? I am a sophomore majoring in computer science with a minor in data science. I completed the Google Data Analytics Certification recently which reinforced my commitment to the data analytics field, however, I wish to become a data engineer instead of a data analyst. I am currently studying for the aws cloud practitioner exam and doing an introduction to data engineer course on LinkedIn Learn. What should my next steps be after the aws exam?",self.dataengineering
464,pcokrh,dathu9,https://www.reddit.com/r/dataengineering/comments/pcokrh/is_dbt_can_be_used_at_the_enterprise_level/,is dbt can be used at the enterprise level?,I just start evaluating the dbt (data build tool) product for our analytics team to perform analytical transformations with dbt. But I am seeing some limitations with my testing. Can some please comment is below items achievable with dbt?  1. Can dbt support multi db connections in one model query? (like DB Link) 2. Can dbt read one snowflake and save the results into postgre?  3. Can dbt read the data from s3 without EMR cluster (Spark cluster)? 4. Can dbt store the output to s3 or onedrive storage?,self.dataengineering
465,pcnuuk,dataengineerdude,https://www.reddit.com/r/dataengineering/comments/pcnuuk/how_often_to_run_vacuum_and_optimize_on/,How often to run VACUUM and OPTIMIZE on Databricks/Delta Lake?,"Building a new data warehouse in Databricks with Delta Lake. How often should you a person be running VACUUM and OPTIMIZE commands? I'm sure it depends on table size and updates, my case once daily, large tables, billions of records in some cases. Is once a week enough? Can't find much for recommendations on this anywhere.",self.dataengineering
466,pcn3lu,an_tonova,https://blog.discord.com/how-discord-stores-billions-of-messages-7fa6ec7ee4c7,Discord is switching to Cassandra. How and why,,blog.discord.com
467,pcmazc,t0il3ts0ap,https://www.reddit.com/r/dataengineering/comments/pcmazc/the_great_spaghetti_ie_view_chaining/,The great spaghetti .i.e.. view chaining,"To set some context, I have couple of tables in athena ( around 300 ) with total data spanning over 15 TB at max.   Recently bunch of analysts have joined my company and a great number of views have been created ( &gt; 170 ). Previewing some of the views scans 330 GB of data in Athena and expenses are through the roof.   On going through some of the views, I find queries where there is join of 25 views like a full on spaghetti.    Views have both pros and cons as they abstract the logic neatly but can be very expensive if ppl just join views with each other left and right without going deep down at all.      So, if any of you guys have been in such unfortunate circumstance and were able to solve it, I would really like the advice. Right now I think If I can get some tool which can show me graph of which view connects with which other view, maybe I can simplify it.",self.dataengineering
468,pceu26,skpfm_06,https://www.reddit.com/r/dataengineering/comments/pceu26/been_working_in_a_small_startup_for_almost_2/,"Been working in a small startup for almost 2 years doing pretty much everything on the engineering side, what should I focus on and pursue moving forward?","   TLDR: Pretty much handled Data Engineering, Backend Engineering, a bit of DevOps all by myself at a very tiny startup. Love designing large and complex systems that interact in all kinds of ways and developing the modules/components. What career path should I pursue to build systems with an AI core in it?  A little bit of background, I studied Electronics in my undergrad (graduated in 2017, have about 3 years of experience now) and discovered programming and my love/aptitude for it in my last semester. Since then, I have had this chip on my shoulder about not having come from a CS background academically, and whatever I have learned so far has been completely self-taught. I just googled things, read blogs, saw tutorials or tech talks on youtube. I am now confident in my ability to pick up a new library or face new problems and solve it eventually.  Nearly 2 years ago, I joined a tiny startup, and back then, I was hoping to become a Data Scientist and work with Machine/Deep Learning stuff. But I never got the opportunity at my job but was instead tasked with working on building a system around the trained models my colleagues would develop. But eventually, I have realized that the aspect of uncertainty when training a model and struggling to get good accuracy is not for a guy like me who sees things in absolutes. To that end, I have discovered that I actually am good at/ love designing systems and developing individual components. Here is a list of things that I do at my company (keep in mind that I pretty much did 90% of these all by myself),  1. Design the architecture of our product (has an AI core) or any service project (AI core again) I am on 2. Build flask apps that either do CRUD stuff or expose an ML model(s). I design the API specs and develop them. 3. Design and build data pipelines (both batch and streaming) (Kafka for streaming) 4. I single-handedly developed a module to spin up GPU servers on AWS which shut down after processing things. The design was mostly me under the guidance of my CEO. Sorta serverless approach. I am not familiar with the industry terms. 5. Design how things should be stored on S3. The prefix structure and all 6. Design the database schema and write flask apps with sqlalchemy ORM 7. DevOps stuff like starting servers and setting up the environment. Managing AMIs, Elastic IPs, Security Groups 8. Dockerise stuff when needed 9. I add logs and error report slack bot to everything I do.  My official title is Data Engineer but I think I am working on things that a backend dev and DevOps guy would work on as well. (Please let me know if I am donning a different role or I am wrong about the roles mentioned above)  Now, I absolutely love complex problems and designing and developing large systems that have a lot of services that interact in all kinds of ways. I'd like to think that I am good at it. Because I was doing most of these solo for so long, I haven't had the chance to delve deeper into a subset of these tasks, so I am not familiar with tools like Luigi, Airflow, Kubernetes, Deeper understanding of AWS services, writing python like they do in Open Source libraries, Hadoop, Spark, etc.  My question to you guys is, what role/title should I pursue and what among the stuff above should I focus less? I want to stay in the AI domain and build large-scale systems around an AI core. I absolutely love designing and developing modular systems.",self.dataengineering
469,pce5yi,CapitalistZ,https://www.reddit.com/r/dataengineering/comments/pce5yi/check_for_records_or_download_everything/,Check for records or download everything?,"Hi, this is my first ETL project and my first major project in Python. Everything is going great. I am connecting to an API and uploading the data to an MS SQL server, although this is a general question and doesn't have to be specific to my project.  Should I write my request functions to check the latest record in the data I've previously downloaded and only request data from that record onwards? Or should I just request everything and UPSERT the table back in to the server?  I imagine when you're dealing with big data it's not ideal to download all the data each time you want to update your data warehouse and I'd like to get this right from the start.",self.dataengineering
470,pcbds8,the_travelo_,https://www.reddit.com/r/dataengineering/comments/pcbds8/what_exactly_does_schema_on_read_mean_in_a_data/,"What exactly does ""Schema on Read"" mean in a Data Lake?","I've seen a lot of Data Lakes that enforce schemas after the landing zone.  This means that once the data source arrives to (let's say) S3, they have automated parametrized scripts that enforce schemas, data types, etc.  Seems like the data lake is more of a data warehouse in these cases, since they follow the traditional approach, with the only difference being that they use S3 instead of Oracle or Teradata or any other of those systems.  I fail to see what a true data lake with the promise of fast ingestion, flexibility and such looks like.  Can anyone explain how this can be done in a ""modern"" approach?   How does the data flow from landing to ""curated"" and how is the schema treated?",self.dataengineering
471,pcbc1n,Successful-Aide3077,https://youtube.com/watch?v=-UseEukEoCQ&amp;feature=share,Microsoft Azure Data Engineer Certification DP 203,,youtube.com
472,pcav10,Integrator_Eye,https://www.reddit.com/r/dataengineering/comments/pcav10/changing_column_data_type_on_large_hive_tables/,Changing column data type on large hive tables,"Hi Folks, I have requirement and I am looking for solution. We have on perm hadoop data lake. There are more than 250+ tables defined using parquet format and having 1000+ partitions. We come to know few columns containing date value are defined as BIGINT. We want to change schema of these tables and convert column data type from BIGINT to Datetime. Also, update data in table to have date value. Any suggestion, what should be strategy to perform this huge task with minimum downtime.",self.dataengineering
473,pc9q71,Trumty,https://www.reddit.com/r/dataengineering/comments/pc9q71/joining_a_data_engineering_bi_team/,Joining a Data Engineering &amp; BI Team,"Joining a large team of data engineers and BI engineers to build out the Analytics arm.  Previously I led an analytics team in a more decentralized model, reporting into the business side.  What’s your advice?",self.dataengineering
474,pc9ddk,Aaron-SWE,https://v.redd.it/nqu748covrj71,It's not sexy. It's necessary.,,/r/dataengineering/comments/pc9ddk/its_not_sexy_its_necessary/
475,pc8xtx,metaplane_hq,https://www.metaplane.dev/blog/inside-data-with-ben-cohen-spoton,"Learnings from a data engineering leader at SpotOn (ex-Cameo, Braintree, Cars.com) - ""Focus on the most important company and team goals, and forget about imposter syndrome.""",,metaplane.dev
476,pc8xeq,Bobbing4horseradish,https://www.reddit.com/r/dataengineering/comments/pc8xeq/how_often_do_you_pushpull_to_a_data_lake_what_are/,How often do you push/pull to a data lake? What are the main factors to consider?,PM here trying to understand technical considerations.  We have a DB ~10GB in size.  Would like to have as close to real time access in a BI tool accessing the data lake.,self.dataengineering
477,pc6w34,Aaron-SWE,https://i.redd.it/dtke6l9w9rj71.jpg,"The ""struggle"" is real",,i.redd.it
478,pc6jwg,eddygaras,https://www.reddit.com/r/dataengineering/comments/pc6jwg/moving_away_from_ssis/,Moving away from SSIS,"Wanted to get everyone’s opinion on what would be the best tech stack to use in Azure environment in order to move away from SSIS.   In our company we have migrated a lot of MS SQL servers holding various ERP systems data into Azure VM’s. Reporting warehouses are now in Azure VM’s, as well. We use SSIS to collect data from servers holding ERP data into reporting warehouse(s). There are costs involved for professional Visual Studio licenses and a 3rd party Redgate toolbox to be able to do SQL Change Automation nicely with git and Azure DevOps pipelines. SSIS projects are done properly, runs data streams in parallel, with error handling, defensive programming, monitoring, logging and notifications, packages can run anytime without messing anything up and can pick up where it left off after a failure. It would be hard to find any other company with SSIS implemented that well like we have here.   We have a pretty knowledgeable team, good on data modelling and architecture, everyone knows Kimball, Inmon methodology etc., can do basic python/c#/vb programming.   I am worried that we are wasting that much talent by staying on soon to be legacy tech and not trying to move forward with the new tech. I feel like we should have at least tried to do some of our SSIS workloads in Azure Data Factory… Anyways, there is a lot more work to come and we would do everything in SSIS. What would you use instead of SSIS? Would it be Azure Data Factory? Or using python and frameworks like Airflow and Spark or some others?  (Sorry for a long post)",self.dataengineering
479,pc4mdk,Adventurous_Ebb_9392,https://www.reddit.com/r/dataengineering/comments/pc4mdk/python_low_code_tools/,Python Low Code Tools?,[removed],self.dataengineering
480,pc4b9b,saik2363,https://www.dasca.org/world-of-big-data/article/why-every-data-scientist-wants-a-data-engineer,Why Every Data Scientist Wants a Data Engineer,,dasca.org
481,pc2yne,stackedhats,https://www.reddit.com/r/dataengineering/comments/pc2yne/if_you_could_go_back_in_time_and_tell_yourself/,"If you could go back in time and tell yourself one thing before building your first pipeline, what would that be?","I'm about to build my first pipeline and warehouse, the ETL is pretty darn straightforward in this case... rip the data out of the shitty accounting platform architecture into something reasonably sane.  I plan to treat this as a learning exercise because I'll be responsible for building almost all the infrastructure in this firm for the foreseeable future.  I was hired as an analyst, when the firm had no DE or infrastructure to speak of, so I've been learning how to wear that hat and enjoying it a lot actually. I'm working my way through Kleppman's DDIA.  I don't need someone to hold my hand but I am curious, what sorts of things would you have kept in mind or done differently if you could do your first pipeline over again?  Note that I am the sole IC on this project so tips on making it maintainable are highly relevant.",self.dataengineering
482,pc1bww,instamarq,https://www.reddit.com/r/dataengineering/comments/pc1bww/transitioning_from_bi/,Transitioning from BI,"Hello everyone! This is my first post here and on Reddit in general. To try and keep things concise, I'll start with the question:  ## TLDR  What's the best bet for breaking into a data engineering role for someone with only an associate's degree, Lambda School Data Science program completion and \~1yr of experience as a BI consultant building Power BI dashboards for a small company?  ## A Bit More Background  I have a very strong grasp of python, decent (albeit rusty) SQL skills, good working knowledge of analytics/ML libraries in the language, decent understanding of computer science topics. I've worked on more than a couple of intermediate data science (analytics &amp; ML) projects as part of the Lambda School curriculum as well as proof-of-concept and production data pipelines in my job.  I have some knowledge gaps when it comes to scheduling and parallel processing tools but I'm working on filling those in.  Initially, I wanted to land a data science role straight out of Lambda school, but I quickly came to the realization that my hyper-expensive bootcamp had not been enough to get me a job in the kind of role I wanted. I applied to over 100 companies without a single interview. Eventually I found a job at a small company building Power BI dashboards thanks to an old friend. Unfortunately, I'm just kind of unhappy there, mostly because of feeling like I don't have the personality for a consultant role and generally disliking the Power BI stack.   Ultimately, all I wanted since I started this journey almost 4yrs ago (that's when I decided to go all in on learning python) was to get my foot in the door somewhere that would allow me to build something useful via programming and pivot towards a fulfilling career (I was a professional musician before I decided to make a change).  Thanks in advance for any tips/advice, even if it seems discouraging.",self.dataengineering
483,pc1ad8,Vorskl,https://www.reddit.com/r/dataengineering/comments/pc1ad8/dataquest_data_engineering_path/,Dataquest Data Engineering path,"Hi, has anyone taken or have an educated opinion about the Dataquest data engineering path?  Also, are their projects good enough for one's portfolio?",self.dataengineering
484,pc0asf,Professional-Bid9676,https://www.reddit.com/r/dataengineering/comments/pc0asf/singaporean_data_engineers/,Singaporean Data Engineers?,Any Singaporean DEs out here? Curious about how has your job searching experience has been and how your job scope matches your expectations here in SG.,self.dataengineering
485,pbzyc1,kristiclimbs,https://www.reddit.com/r/dataengineering/comments/pbzyc1/airflow_subdag_tasks_backfill_in_cli_is_creating/,Airflow Subdag tasks backfill in CLI is creating new tasks instead of rerunning the failed tasks and only running 1 day not range,"I'm trying to do the backfill in the command line, I'm launching the backfill command on ECS Fargate and instead of rerunning the tasks for the subdag for 2021-01-03 its creating new tasks for 2021-01-02 and running it, and if I try to do a range 2021-01-03 to 2021-01-05 it only runs for one date         ./launch-ecs-backfill.sh -v -w -t dev 'instagram.build_media -s 2021-01-03  -e 2021-01-05 --rerun-failed-tasks'",self.dataengineering
486,pbxn53,howMuchCheeseIs2Much,https://github.com/mike-paper/pulse,Pulse – An open-source analytics app built with dbt,,github.com
487,pbx7ki,Original_Bend,https://www.reddit.com/r/dataengineering/comments/pbx7ki/how_to_deal_with_source_database_changes/,How to deal with source database changes?,"Hi,  I'm currently facing a problem. Some of my pipelines are using two tables from an OLTP database. There is only one instance of this database. The DBA notified me that some tables were going to change (schema change...).   I understand that there are some tools for database schema evolution (Liquibase, Sqitch...). But I would like to be able to have three environments separated : dev, staging, prod. My pipelines are using these three environments, but the source database has always been the same for all three envs.  I don't think that asking the DBA to manage three distincts copies of the DB for each env would be practical, but I don't want a schema change on the source impacting my pipelines in production before I could do the evolutions on the dev environment.  What would you do? Thank you",self.dataengineering
488,pbu2ue,nanderovski,https://dev.to/nazliander/about-the-google-cloud-platform-operators-in-airflow-3ap7,Using Google Cloud Platform Operators in Airflow,,dev.to
489,pbu2ez,nanderovski,https://www.reddit.com/r/dataengineering/comments/pbu2ez/using_google_cloud_platform_operators_in_airflow/,Using Google Cloud Platform Operators in Airflow,[removed],self.dataengineering
490,pbshco,GreekYogurtt,https://www.reddit.com/r/dataengineering/comments/pbshco/brand_image_walmart_vs_amex/,Brand Image Walmart vs Amex," I only have 1 Yr Exp as a Data Engineer, looking to work with good brands.   Which ones of these is more impressive ? My Job Profile is similar at both.",self.dataengineering
491,pbscdv,citizenofacceptance,https://www.reddit.com/r/dataengineering/comments/pbscdv/how_much_longer_do_we_think_the_job_market_will/,How much longer do we think the job market will stay this hot ?,anyone else feel like this summer has been heavy for DE recruiting and some salary gain opportunities? Curious about how we think this will last to balance waiting out a change or jumping,self.dataengineering
492,pbrmno,RoundAlternative1106,https://www.reddit.com/r/dataengineering/comments/pbrmno/i_have_completed_my_computer_engineering_degree/,"I have completed my Computer engineering Degree, now I want to become a data engineer .","Question: To become a Data engineer What should learn (i.e. deep learning of that subject)?  I have learn SQL,NOsql,python(and how to use it with SQL/Nosql).   Should I also learn Data Structure and Algorithum(Both in depth)?",self.dataengineering
493,pbqiza,falconike62,https://www.reddit.com/r/dataengineering/comments/pbqiza/new_data_engineer_here_question/,New Data Engineer here - question,"About 6 months into a new engineering position, moved from an analyst position. Current debate is circulating my engineering team and wanted an outside opinion. We have 3 parts to our current pipeline, each operated by three seperate python wrappers. A few members of our team are advocating to move all three parts under the same wrapper, while others advocate keeping them seperate. Pro seems to be more ability for interdependency as data flows through the pipeline, but seems to add a layer of complexity all under the same ""hood"".  What's has been your experience? Simple is better? Or more control as data flows?",self.dataengineering
494,pbq301,justwantstocode,https://www.reddit.com/r/dataengineering/comments/pbq301/ugh_made_a_mistake_on_an_assessment/,Ugh made a mistake on an assessment,Completed an assessment I spent around 5-6 hours on creating the scripts to generate data and then build a hypothetical system while answering some questions.  I realized I had made a  mistake in a calculation when creating the dummy data set...  So frustrating since it's such a stupid mistake in retrospect.  Should I be worried? They didn't flat out reject me instead giving me the opportunity to identify the mistake.,self.dataengineering
495,pbpsgh,Reasonable_Banana297,https://www.reddit.com/r/dataengineering/comments/pbpsgh/adobe_vs_grubhub/,Adobe vs GrubHub,I have below offers for Senior data engineer role for Utah location  Adobe: Base 150k Stock 88k over 4 years Sign on bonus 15k Annual bonus 15%  Grubhub: Base 160k Stock 300k over 4 years No bonus  Current TC: 🥜   #tech #h1b #adobe #grubhub  Adobe vs GrubHub ?,self.dataengineering
496,pbojfj,gloverb2016,https://www.reddit.com/r/dataengineering/comments/pbojfj/nervous_about_de_interviews/,Nervous about DE interviews,"I finally decided to start applying for DE positions. I am a bit nervous about on the spot coding questions as I am self taught and don't feel super comfortable coding on the spot. I am good at Python and understand CS fundamentals but don't feel super confident. I will admit I am more of a look it up online and fit to my use and refactor to get better. How serious are these ""Whiteboard"" questions? Should I just invest in those online coding challenge questions?   As a background I am a Data Analyst. I build pipelines from transactional systems and create reports for the whole company. I mainly use SQL and Access. I create scorecard reports as well as power bi reports.   Thanks",self.dataengineering
497,pblsea,zeckk89,https://www.reddit.com/r/dataengineering/comments/pblsea/how_long_on_average_do_you_wait_from_hr_on_a/,How long on average do you wait from HR on a position you interviewed for?,I interviewed for a sr. Data engineering role last week and Thursday will be 1 week sense my last interview. Should I count this as a loss or think I still have a chance?,self.dataengineering
498,pblh3i,superconductiveKyle,https://www.reddit.com/r/dataengineering/comments/pblh3i/maximizing_productivity_of_analytics_teams_part_1/,Maximizing Productivity of Analytics Teams Part 1: Triaging Broken Dashboards.,"Solid blog about how pipelines grow in scope and complexity, which is completely normal. Data changes and issues will arise, so building pipelines with continued maintenance in mind is crucial to a growing analytics team.  [https://greatexpectations.io/blog/maximizing-productivity-of-analytics-teams-pt1/](https://greatexpectations.io/blog/maximizing-productivity-of-analytics-teams-pt1/)  &amp;#x200B;  Written by [Sarah Krasnik](https://twitter.com/sarahmk125)",self.dataengineering
499,pbks3q,vietlinh12hoa,https://www.reddit.com/r/dataengineering/comments/pbks3q/read_sql_file_in_jupyter_notebook/,Read .sql file in jupyter notebook,"I'm the newbie in .sql. I have a .sql file, but not sure how to read as a table (with pandas) in jupyter notebook.",self.dataengineering
