KEYWORD_SENTENCE_ID,POST_ID,AUTHOR,URL,TITLE,KEYWORD,KEYWORD_CATEGORY,SENTENCE_TEXT,MATCH_SUBSTRING,START_POSITION,END_POSITION
5cd63505df6b3f40a72da25091932e2c,ps0h44,scraper01,https://www.reddit.com/r/dataengineering/comments/ps0h44/is_anyone_here_still_using_apache_storm/,Is anyone here still using Apache Storm?,APACHE_FLINK,analytics,Wondering if anyone here prefers Storm above other stream frameworks like Flink and why.,Flink,75,80
197fcc949ab81f4e7ba40737280c95b3,pqty5b,scraper01,https://www.reddit.com/r/dataengineering/comments/pqty5b/stream_processing_tool_to_pair_with_kafka/,Stream processing tool to pair with Kafka,APACHE_FLINK,analytics,"Between spark streaming, apache flink and apache storm, what do you think sinergizes the best with Kafka",apache flink,26,38
7aee742060ef26a075cc0f30437ea4c5,pt90kg,ArunMu,https://www.reddit.com/r/dataengineering/comments/pt90kg/clickhouse_and_apache_pinot/,ClickHouse and Apache Pinot,APACHE_PINOT,datastores,I had posted my findings about both ClickHouse and Apache Pinot at [https://www.reddit.com/r/bigdata/comments/pse4gb/clickhouse\_and\_apache\_pinot/](https://www.reddit.com/r/bigdata/comments/pse4gb/clickhouse_and_apache_pinot/,Apache Pinot,52,64
2fb599e9c1c74e61888eaee72d28b65c,pryp6w,KimStacks,https://www.reddit.com/r/dataengineering/comments/pryp6w/less_than_1tb_of_data_what_tools_should_i_get/,Less than 1TB of data what tools should I get better at?,POSTGRES,datastores,"I read https://docs.dask.org/en/latest/spark.html and in the very last line it said and I quote  &gt; you are looking to manage a terabyte or less of tabular CSV or JSON data, then you should forget both Spark and Dask and use Postgres or MongoDB",Postgres,228,236
528da3019020b4da152cfea8c9a7aab0,pryp6w,KimStacks,https://www.reddit.com/r/dataengineering/comments/pryp6w/less_than_1tb_of_data_what_tools_should_i_get/,Less than 1TB of data what tools should I get better at?,POSTGRES,datastores, I’m a software developer using mostly django and Postgres  And wants to slowly improve my data engineering skills in tandem with my day to day,Postgres,51,59
a1a37fb930e19a9e9f2c0bbf1d4765b5,pr6xg5,onelostsoul115,https://www.reddit.com/r/dataengineering/comments/pr6xg5/python_code_test_interview_general_career_advice/,Python Code Test (Interview) + general career advice,POSTGRES,datastores," I've evolved from \~5 years of hardcore database work (Oracle, Postgres",Postgres,65,73
66b323852aaee672194f17311f678df0,ponhwq,ciskoh3,https://www.reddit.com/r/dataengineering/comments/ponhwq/advice_on_db_for_ml_project/,Advice on DB for ML project,POSTGRES,datastores, Which solution would you pick:  \- Local Postgres SQL (if the whole thing stays below 600 GB,Postgres,43,51
9426974b217465e41f3d02e7ec50db52,ponhwq,ciskoh3,https://www.reddit.com/r/dataengineering/comments/ponhwq/advice_on_db_for_ml_project/,Advice on DB for ML project,POSTGRES,datastores, \- Google cloud SQL with PostGREs server  \-Google BigQuery (not sure if this is needed,PostGREs,27,35
a3668f5afd3fa6e161e12bbd080a7101,pomqcg,dadadawe,https://www.reddit.com/r/dataengineering/comments/pomqcg/dbt_looker_whats_it_all_about/,"DBT, Looker, ... what's it all about?",POSTGRES,datastores," Aside from flashy interfaces, what would be the difference between me generating a view in PostgreSQL or using a tool like Denodo to virtualize my star schema on top of the storage layer",PostgreS,93,101
662ed0e6036fb804306e7ab55c288b60,po5kgw,marcosmarxm,https://www.reddit.com/r/dataengineering/comments/po5kgw/presentation_airbyte_airflow_on_gcp/,Presentation: Airbyte + Airflow on GCP,POSTGRES,datastores, Tomorrow I'll go over how to set up Airbyte + Google Composer on GCP and send data from a Postgres Database to BigQuery! If you're curious about how to leverage Airbyte with Airflow this is your chance,Postgres,92,100
0cd833fe4755f6fb0539b9df3dc0b145,pktcp3,noNSFWcontent,https://www.reddit.com/r/dataengineering/comments/pktcp3/can_someone_help_me_understand_the_difference/,Can someone help me understand the difference between the the docker-compose files?,POSTGRES,datastores,"   **The first docker compose file is  -**           version: '3.9'               #asking for a postgres database     services:       postgres:         image: postgres:9.6         environment:           - POSTGRES_USER=airflow           - POSTGRES_PASSWORD=airflow           - POSTGRES_DB=airflow         logging:           options:             max-size: 10m             max-file: 3               #asking for a webserver       webserver:         build: ./dockerfiles         restart: always         depends_on:           - postgres         environment:           - LOAD_EX=n           - EXECUTOR=Local         logging:           options:             max-size: 10m             max-file: 3         volumes:           - ./dags:/usr/local/airflow/dags           # - ./plugins:/usr/local/airflow/plugins         ports:           - 8080:8080         command: webserver         healthcheck:           test: [CMD-SHELL, [ -f /usr/local/airflow/airflow-webserver.pid ]]           interval: 30s           timeout: 30s           retries: 3   For the above docker compose file, the dockerfiles folder had Dockerfile with the following text -       FROM puckel/docker-airflow          RUN pip install requests     RUN pip install pandas  **The second docker compose file is -**         version: '3'     services:       postgres:         image: postgres:9.6         environment:           - POSTGRES_USER=airflow           - POSTGRES_PASSWORD=airflow           - POSTGRES_DB=airflow         ports:           - 5432:5432            webserver:         image: puckel/docker-airflow:1.10.1         build:           context: https://github.com/puckel/docker-airflow.git#1.10.1           dockerfile: Dockerfile           args:             AIRFLOW_DEPS: gcp_api,s3             PYTHON_DEPS: sqlalchemy==1.2.0         restart: always         depends_on:           - postgres         environment:           - LOAD_EX=n           - EXECUTOR=Local           - FERNET_KEY=jsDPRErfv8Z_eVTnGfF8ywd19j4pyqE3NpdUBA_oRTo=         volumes:           - ./examples/intro-example/dags:/usr/local/airflow/dags           # Uncomment to include custom plugins           # - ./plugins:/usr/local/airflow/plugins         ports:           - 8080:8080         command: webserver         healthcheck:           test: [CMD-SHELL, [ -f /usr/local/airflow/airflow-webserver.pid ]]           interval: 30s           timeout: 30s           retries: 3       When the second docker compose was stuck in a dependency hell, I noticed both were pulling the same Airflow puckel image",postgres,97,105
992d1b6ba23026d18a12219e440fe7ec,pknfs8,fatredbeaver,https://www.reddit.com/r/dataengineering/comments/pknfs8/advice_on_reflect_changes_to_contents_of_database/,Advice on reflect changes to contents of database based on refreshed scraped data,POSTGRES,datastores,"The data scraped is cleaned and inserted into a Postgres database using SQLAlchemy as a basic interface (nothing too complicated, similar to what you can find in tutorials",Postgres,49,57
c182c8778d44b5239f8799d1287ebb2a,pj6hoa,Away-Suggestion-5845,https://www.reddit.com/r/dataengineering/comments/pj6hoa/a_case_study_example_for_an_interview/,A Case Study Example for an Interview,POSTGRES,datastores,  **Case Study:**  The event management company provides the platform for their customers to publish their events and manages all transactional processes. The ‘shop’ platform stores all orders(included historical datas) of a customer in a relational database and the DWH imports all those orders daily into their own PostgreSQL DB,PostgreS,318,326
18d35628866513a6c405cbae07bbbeb0,pin0kq,priyasweety1,https://www.reddit.com/r/dataengineering/comments/pin0kq/different_approaches_to_do_incremental_delta_load/,Different approaches to do incremental delta load in S3,POSTGRES,datastores,I have historical data as well as incremental data in s3 what’s the best efficient way to load and transform only incremental and load incremental data in Postgres or Redshift,Postgres,156,164
dc1190a9bb8bd529ebb2bffb1480dbfb,pgm25e,ApocalypseAce,https://www.reddit.com/r/dataengineering/comments/pgm25e/how_to_deal_with_db_performance/,How to deal with DB performance?,POSTGRES,datastores,"I'm using Postgresql as a datawarehouse for a small business, with relatively low data volumes",Postgres,11,19
aef6b2f1e9f41ed8885c2f9364e13658,pgm25e,ApocalypseAce,https://www.reddit.com/r/dataengineering/comments/pgm25e/how_to_deal_with_db_performance/,How to deal with DB performance?,POSTGRES,datastores,"Hardware is decent, running at least 4-6 cores at minimum at any one time, with up to 8GB+ of RAM available on demand for Postgres",Postgres,123,131
f9a2778edd82edb4c86be6f0fc99bed7,pgm25e,ApocalypseAce,https://www.reddit.com/r/dataengineering/comments/pgm25e/how_to_deal_with_db_performance/,How to deal with DB performance?,POSTGRES,datastores,Postgres won't cut it?,Postgres,1,9
2199ee7ab19f790dc2f373b6c69d0296,pgcq36,vinsanity1603,https://www.reddit.com/r/dataengineering/comments/pgcq36/aws_setup_recommendation_help/,AWS Setup Recommendation HELP,POSTGRES,datastores,"But my question is, can AWS Glue handle incremental load to a database like PostgreSQL or Redshift, as well as data modelling",PostgreS,77,85
d9e4da0e1788e0fc0392604f353a9377,pg8guw,vinsanity1603,https://www.reddit.com/r/dataengineering/comments/pg8guw/aws_setup_recommendation/,AWS Setup Recommendation,POSTGRES,datastores,"But my question is, can AWS Glue handle incremental load to a database like PostgreSQL or Redshift, as well as data modelling",PostgreS,77,85
94bc45d1cc2b7eaac585acb45169c0ad,pg8guw,vinsanity1603,https://www.reddit.com/r/dataengineering/comments/pg8guw/aws_setup_recommendation/,AWS Setup Recommendation,POSTGRES,datastores,"    For the OLAP System, I was thinking of just using an AWS RDS PostgreSQL for it",PostgreS,66,74
3c8e736f7b1c950f44fdf0907fd41dee,pg3d2h,MOONSfan,https://www.reddit.com/r/dataengineering/comments/pg3d2h/need_help_with_group_by_cube_function_in_redshift/,Need help with Group by Cube function in Redshift.,POSTGRES,datastores,"Hi all, I need help in doing postgres equivalent of group by cube in Redshift",postgres,30,38
65498d5ad5b53dee9b990117f75c77b9,pg3d2h,MOONSfan,https://www.reddit.com/r/dataengineering/comments/pg3d2h/need_help_with_group_by_cube_function_in_redshift/,Need help with Group by Cube function in Redshift.,POSTGRES,datastores,I can do this in postgres but the data size is too huge,postgres,18,26
807e428cad6b28788811caf037d80c57,pf86jp,OkieDaddy,https://www.reddit.com/r/dataengineering/comments/pf86jp/need_help_with_redshift/,Need help with Redshift pivoting/unpivoting/functions,POSTGRES,datastores,"In Postgres I can do it, albeit slowly, with a function that passes in the code value, and returns the code text",Postgres,4,12
55491f0c41127089ca939971718288cc,pskxx5,mdl003,https://www.reddit.com/r/dataengineering/comments/pskxx5/best_tools_for_metadata_capture_indexing_for/,Best tools for metadata capture + indexing for delta/parquet tables?,AMUNDSEN,data governance and registries,I've checked out [Amundsen.io](https://Amundsen.io,Amundsen,19,27
07bd6f06472cbb7396998d37c036c410,pryban,regreddit,https://www.reddit.com/r/dataengineering/comments/pryban/metadata_dictionary_software_for_small_teams/,Metadata Dictionary software for small teams?,AMUNDSEN,data governance and registries,"I've installed and semi-configured LinkedIn's Datahub and Lyfts Amundsen, and both were pretty complex to setup, and data discovery and ingestion is a very involved process in Amundsen, and only slightly easier in datahub",Amundsen,65,73
4c164e0066a46f58fcedc1390636769c,pryban,regreddit,https://www.reddit.com/r/dataengineering/comments/pryban/metadata_dictionary_software_for_small_teams/,Metadata Dictionary software for small teams?,AMUNDSEN,data governance and registries,"Amundsen literally requires almost a dedicated team to keep it running and any ingestion of metadata requires deep knowledge of the platform, which is very sparsely documented",Amundsen,1,9
f79a01983e004b156934e1719e5e7188,ppjtk8,Cloakie,https://www.reddit.com/r/dataengineering/comments/ppjtk8/amundsen_vs_datahub_vs/,Amundsen vs DataHub vs ???,AMUNDSEN,data governance and registries,"    I see that Amundsen has a long laundry list of adopters, but I find their documentation to be a little spotty and hard to read-through in comparison to DataHub",Amundsen,16,24
fa11f84b33318e4e7abfbce8429d7100,peap7h,CoMatrix1234,https://www.reddit.com/r/dataengineering/comments/peap7h/a_self_governing_data_gateway/,A self governing data gateway,AMUNDSEN,data governance and registries," Our ecosystem: AWS, Okta, S3, terraform, looking at Amundsen, Spark, cloud monitoring tools",Amundsen,54,62
06a83a0f7aea6c6d59d57678fa3ae80d,pryban,regreddit,https://www.reddit.com/r/dataengineering/comments/pryban/metadata_dictionary_software_for_small_teams/,Metadata Dictionary software for small teams?,DATAHUB,data governance and registries,"I've installed and semi-configured LinkedIn's Datahub and Lyfts Amundsen, and both were pretty complex to setup, and data discovery and ingestion is a very involved process in Amundsen, and only slightly easier in datahub",Datahub,47,54
ac5e434c7810a7e406f5f8c34bcb4fcd,pryban,regreddit,https://www.reddit.com/r/dataengineering/comments/pryban/metadata_dictionary_software_for_small_teams/,Metadata Dictionary software for small teams?,DATAHUB,data governance and registries,"Datahub would be perfect except it leaves a lot of components to the end user, like auth.",Datahub,1,8
7602d317c37e4232fb0a0f39f6b98a34,ppjtk8,Cloakie,https://www.reddit.com/r/dataengineering/comments/ppjtk8/amundsen_vs_datahub_vs/,Amundsen vs DataHub vs ???,DATAHUB,data governance and registries,"    I see that Amundsen has a long laundry list of adopters, but I find their documentation to be a little spotty and hard to read-through in comparison to DataHub",DataHub,157,164
069c60d121b57ae9cd7c26c903e0f00e,pli0og,MediumZealousideal29,https://www.reddit.com/r/dataengineering/comments/pli0og/skills_required/,Skills required,HDFS,file system,"I know SQL, hdfs, hive, control M..I also learned airflow, sqoop and basics of spark streaming, Kafka and AWS",hdfs,13,17
2926da14c5f8e008f59803f578d7744e,ph4fvj,WalrusWhich202,https://www.reddit.com/r/dataengineering/comments/ph4fvj/how_to_querying_data_faster/,How to Querying data faster,HDFS,file system," I've looked into spark, hadoop, cassandra, hdfs, and others",hdfs,45,49
39e1ba4ab3b3cc39890c186b5fa3c9b1,pgq50c,king_booker,https://www.reddit.com/r/dataengineering/comments/pgq50c/processing_different_file_format_data/,processing different file format data,HDFS,file system," It places the files in an hdfs location, which kafka picks up and does some processing over it and pushes it into hdfs in an RC file format",hdfs,28,32
66f98b18213081330f10581ed1c5c680,pgq50c,king_booker,https://www.reddit.com/r/dataengineering/comments/pgq50c/processing_different_file_format_data/,processing different file format data,HDFS,file system,"Is it better to read the json into a kafka topic, push it to hdfs as a json, create a hive external table with json serde and then convert it to parquet in other staging tables",hdfs,62,66
88d58d31f4a2f72e72589499c9c6b988,pgq50c,king_booker,https://www.reddit.com/r/dataengineering/comments/pgq50c/processing_different_file_format_data/,processing different file format data,HDFS,file system, Or should I push the data in some other file format in hdfs?,hdfs,57,61
bc4239e55b1d54d5e8519bf5d8b60824,pskxx5,mdl003,https://www.reddit.com/r/dataengineering/comments/pskxx5/best_tools_for_metadata_capture_indexing_for/,Best tools for metadata capture + indexing for delta/parquet tables?,AWS_S3,file system,Raw data is passed by the application to various Kafka topics where it's streamed into s3 buckets,s3,88,90
6ab75988d36472150a0a4a21a20755ce,psh63n,pineapplesoda1e-3,https://www.reddit.com/r/dataengineering/comments/psh63n/parquet_files/,Parquet files,AWS_S3,file system,What is the fastest way to read parquet file form S3 Bucket using Python?Should i stream file?Should i just use pandas?,S3,51,53
d82df81b0759494fa26a82febd36d131,pr6xg5,onelostsoul115,https://www.reddit.com/r/dataengineering/comments/pr6xg5/python_code_test_interview_general_career_advice/,Python Code Test (Interview) + general career advice,AWS_S3,file system,"file ingestion from S3 via Glue with some very basic spark transformations, very basic lamdbas etc",S3,21,23
f30aa20df02013798e9a5690335c123b,pq9f8b,noNSFWcontent,https://www.reddit.com/r/dataengineering/comments/pq9f8b/i_just_had_an_interview_with_a_senior_data/,"I just had an interview with a Senior Data Engineer and the head of the Data Science department of a company and they said, in the next interview they'd see how I think.",AWS_S3,file system,"We had a good conversation about my work experience which was not quite of a data engineer and my personal experience working with AWS tools such as EC2, Kinesis, Lambda, DynamoDB and S3",S3,185,187
44a85a4ecdfba80bc73264f864e7f347,pq4dma,britishbanana,https://www.reddit.com/r/dataengineering/comments/pq4dma/how_do_you_handle_data_versioning_for/,How do you handle data versioning for reproducibility?,AWS_S3,file system,"  We now have a need to start caching results from analyses on our analytical datasets in order to reduce the amount of some more expensive operations, but in order to reliably read from a cache (by cache I just mean a Delta table on S3",S3,235,237
b505ee7c11e0c0ef128ded1247cd6ae1,pq4dma,britishbanana,https://www.reddit.com/r/dataengineering/comments/pq4dma/how_do_you_handle_data_versioning_for/,How do you handle data versioning for reproducibility?,AWS_S3,file system," Alternatively I've thought about augmenting the Delta versioning / metadata with an external metadata store of some kind (another S3 file, or some persistent database), although the lack of transaction control across the metadata store and the data itself would make me sad",S3,132,134
11484114eb14735e28b6a0b59883b2f2,ppmc48,getafterit123,https://www.reddit.com/r/dataengineering/comments/ppmc48/spark_and_s3_integration/,Spark and S3 integration,AWS_S3,file system,Has anyone been able to connect spark 3.0+ to S3 object store using s3 connector or only s3a,S3,47,49
40e82418c67f1310032cd02fac4dd91f,ppmc48,getafterit123,https://www.reddit.com/r/dataengineering/comments/ppmc48/spark_and_s3_integration/,Spark and S3 integration,AWS_S3,file system,I heard somewhere that s3 connector was being depreciated in favor of s3a only in 3.0 and beyond due to s3a being more performant and supporting larger object sizes,s3,24,26
9343e0793a5e9eae86427c4e6a4f9f0c,ppmc48,getafterit123,https://www.reddit.com/r/dataengineering/comments/ppmc48/spark_and_s3_integration/,Spark and S3 integration,AWS_S3,file system,Would it be possible to used s3a on the Hadoop configuration on the client side but still connect to a s3://endpoint on the object storage side,s3,30,32
6fb4cb1e67522b2e672905c611cdf17d,plv523,Tropical_Wasp,https://www.reddit.com/r/dataengineering/comments/plv523/is_my_job_scope_too_broad_or_is_this_normal/,Is my job scope too broad? Or is this normal.,AWS_S3,file system, Over the last 18 months I have done the following:    Created a Python tool that auto uploads files from network drives and legacy on premise systems to AWS S3,AWS S3,155,161
1b2b8be8183ac4811883105f33d37ab4,plaxd7,77_65_61_73_65_6c,https://www.reddit.com/r/dataengineering/comments/plaxd7/what_it_skills_do_i_need_to_know_if_i_want_to_be/,What IT skills do I need to know if I want to be a data engineer?,AWS_S3,file system,"I have some experience with the basic services of AWS (EC2, S3, Batch, DynamoDB, etc",S3,61,63
b9abe0111b5a45c805633363edb38a0e,pl9n2x,DatKalvin,https://www.reddit.com/r/dataengineering/comments/pl9n2x/is_this_architecture_cost_effective_performant/,Is this Architecture cost effective &amp; performant? What's your suggestion.,AWS_S3,file system,stream to S3 in parquet format using AWS Database Migration Service as new records becomes available,S3,11,13
4c4fa70831d6e1658138a800cef0d795,pl9n2x,DatKalvin,https://www.reddit.com/r/dataengineering/comments/pl9n2x/is_this_architecture_cost_effective_performant/,Is this Architecture cost effective &amp; performant? What's your suggestion.,AWS_S3,file system,"Then Snowpipe picks up data from S3 based on event notification from S3, put in a queue and load to Snowflake data warehouse in a stream format",S3,34,36
9008558cc82c39132986a2f8989dea69,pktcp3,noNSFWcontent,https://www.reddit.com/r/dataengineering/comments/pktcp3/can_someone_help_me_understand_the_difference/,Can someone help me understand the difference between the the docker-compose files?,AWS_S3,file system,"   **The first docker compose file is  -**           version: '3.9'               #asking for a postgres database     services:       postgres:         image: postgres:9.6         environment:           - POSTGRES_USER=airflow           - POSTGRES_PASSWORD=airflow           - POSTGRES_DB=airflow         logging:           options:             max-size: 10m             max-file: 3               #asking for a webserver       webserver:         build: ./dockerfiles         restart: always         depends_on:           - postgres         environment:           - LOAD_EX=n           - EXECUTOR=Local         logging:           options:             max-size: 10m             max-file: 3         volumes:           - ./dags:/usr/local/airflow/dags           # - ./plugins:/usr/local/airflow/plugins         ports:           - 8080:8080         command: webserver         healthcheck:           test: [CMD-SHELL, [ -f /usr/local/airflow/airflow-webserver.pid ]]           interval: 30s           timeout: 30s           retries: 3   For the above docker compose file, the dockerfiles folder had Dockerfile with the following text -       FROM puckel/docker-airflow          RUN pip install requests     RUN pip install pandas  **The second docker compose file is -**         version: '3'     services:       postgres:         image: postgres:9.6         environment:           - POSTGRES_USER=airflow           - POSTGRES_PASSWORD=airflow           - POSTGRES_DB=airflow         ports:           - 5432:5432            webserver:         image: puckel/docker-airflow:1.10.1         build:           context: https://github.com/puckel/docker-airflow.git#1.10.1           dockerfile: Dockerfile           args:             AIRFLOW_DEPS: gcp_api,s3             PYTHON_DEPS: sqlalchemy==1.2.0         restart: always         depends_on:           - postgres         environment:           - LOAD_EX=n           - EXECUTOR=Local           - FERNET_KEY=jsDPRErfv8Z_eVTnGfF8ywd19j4pyqE3NpdUBA_oRTo=         volumes:           - ./examples/intro-example/dags:/usr/local/airflow/dags           # Uncomment to include custom plugins           # - ./plugins:/usr/local/airflow/plugins         ports:           - 8080:8080         command: webserver         healthcheck:           test: [CMD-SHELL, [ -f /usr/local/airflow/airflow-webserver.pid ]]           interval: 30s           timeout: 30s           retries: 3       When the second docker compose was stuck in a dependency hell, I noticed both were pulling the same Airflow puckel image",s3,1742,1744
3ab80c98be480b5f417f2f87ebecd6cb,pkmggc,datanoob2021,https://www.reddit.com/r/dataengineering/comments/pkmggc/streaming_pipeline_question/,Streaming Pipeline Question,AWS_S3,file system, I know AWS has tools like step functions but I was hoping to use some free tools like GreatExpectations and other libraries to check data before sending it over to Firehose/S3/Redshift.,S3,175,177
6fa85c5f5ddb0183b492544d6cbbddd3,pin0kq,priyasweety1,https://www.reddit.com/r/dataengineering/comments/pin0kq/different_approaches_to_do_incremental_delta_load/,Different approaches to do incremental delta load in S3,AWS_S3,file system,I have historical data as well as incremental data in s3 what’s the best efficient way to load and transform only incremental and load incremental data in Postgres or Redshift,s3,55,57
911f1648d736e0c9640f42a646a78eaa,pie34a,umarcja,https://www.reddit.com/r/dataengineering/comments/pie34a/storing_json_logs_in_aws_for_querying/,Storing JSON logs in AWS for querying,AWS_S3,file system, I'm using an S3 bucket to store the raw logs files at the moment and trying to figure out what's the best way to store those logs,S3,15,17
f02d31e77760bfe5084f912b95d24604,pi70aq,h2otoo_frog,https://www.reddit.com/r/dataengineering/comments/pi70aq/is_boomi_as_bad_as_it_seems/,Is Boomi as bad as it seems?,AWS_S3,file system," It just took me a week to write something that would make a basic API call, loop through the pages, save to AWS S3, then send a command to the database for loading",AWS S3,110,116
5766105910ca375637cfc554061d35eb,pgcq36,vinsanity1603,https://www.reddit.com/r/dataengineering/comments/pgcq36/aws_setup_recommendation_help/,AWS Setup Recommendation HELP,AWS_S3,file system,"Hi, I have an application that will dump json data to an object storage (probably S3",S3,83,85
1339ed1a942a3a9bf48af156fe57679a,pg8guw,vinsanity1603,https://www.reddit.com/r/dataengineering/comments/pg8guw/aws_setup_recommendation/,AWS Setup Recommendation,AWS_S3,file system,"Hi, I have an application that will dump json data to an object storage (probably S3",S3,83,85
a6b13e1e3fa84e7e5a3a4c4cd84a72d6,pfxupg,Initial_Squirrel2693,https://www.reddit.com/r/dataengineering/comments/pfxupg/what_does_aws_coding_round_consist_usually/,What does AWS coding round consist usually?,AWS_S3,file system," The services mentioned in my Resume are S3, Redshift, Lambda, ElasticSearch, EC2, etc",S3,42,44
e1beb76dd5f5442a49f5e9b40e6046d7,pfic3r,LinaVak,https://www.reddit.com/r/dataengineering/comments/pfic3r/newbie_question_on_s3/,Newbie question on s3,AWS_S3,file system,I have never used s3 before,s3,19,21
821f3985bec701dc0b4f4e7e6dbfccfb,peap7h,CoMatrix1234,https://www.reddit.com/r/dataengineering/comments/peap7h/a_self_governing_data_gateway/,A self governing data gateway,AWS_S3,file system," Our ecosystem: AWS, Okta, S3, terraform, looking at Amundsen, Spark, cloud monitoring tools",S3,28,30
e516b9bdb8856b359e35017fb738a001,pdflom,Ajudada,https://www.reddit.com/r/dataengineering/comments/pdflom/how_to_change_career_from_it_operations_to/,How to change career from IT operations to development field? (Please read description),AWS_S3,file system,"I have good programming skills and basic AWS skills (S3, Athena, EMR, Glue",S3,54,56
9543778450fdcd539c978176cea07e29,pcokrh,dathu9,https://www.reddit.com/r/dataengineering/comments/pcokrh/is_dbt_can_be_used_at_the_enterprise_level/,is dbt can be used at the enterprise level?,AWS_S3,file system,Can dbt read the data from s3 without EMR cluster (Spark cluster,s3,28,30
7322ac401128d6a9bd1a2a88ff3c6038,pcokrh,dathu9,https://www.reddit.com/r/dataengineering/comments/pcokrh/is_dbt_can_be_used_at_the_enterprise_level/,is dbt can be used at the enterprise level?,AWS_S3,file system,Can dbt store the output to s3 or onedrive storage?,s3,29,31
8816214a4005bd2ad72794eaadb8c29f,pceu26,skpfm_06,https://www.reddit.com/r/dataengineering/comments/pceu26/been_working_in_a_small_startup_for_almost_2/,"Been working in a small startup for almost 2 years doing pretty much everything on the engineering side, what should I focus on and pursue moving forward?",AWS_S3,file system,Design how things should be stored on S3,S3,39,41
d39c4ec215c8ce0514e59172db1f3188,pcbds8,the_travelo_,https://www.reddit.com/r/dataengineering/comments/pcbds8/what_exactly_does_schema_on_read_mean_in_a_data/,What exactly does ""Schema on Read"" mean in a Data Lake?,AWS_S3,file system,"S3, they have automated parametrized scripts that enforce schemas, data types, etc",S3,1,3
1840f0da99557c63c9ca7e7db2bbdbd9,pcbds8,the_travelo_,https://www.reddit.com/r/dataengineering/comments/pcbds8/what_exactly_does_schema_on_read_mean_in_a_data/,What exactly does ""Schema on Read"" mean in a Data Lake?,AWS_S3,file system," Seems like the data lake is more of a data warehouse in these cases, since they follow the traditional approach, with the only difference being that they use S3 instead of Oracle or Teradata or any other of those systems",S3,160,162
e391394d54efa84f0ba390553458c92e,pdt2va,boggle_thy_mind,https://www.reddit.com/r/dataengineering/comments/pdt2va/ingesting_google_ads/,Ingesting Google Ads,MELTANO,data ingestion,"**Google Ads** data (for the first time, never done it before), so far I've looked into Fivetran, Stitch, Singer, Meltano and Airbyte - all of them indicate as having google-ads as a source",Meltano,115,122
27991cbfd332da9036faa7dc7179916b,pdt2va,boggle_thy_mind,https://www.reddit.com/r/dataengineering/comments/pdt2va/ingesting_google_ads/,Ingesting Google Ads,MELTANO,data ingestion,Because Singer is having these issues I assume Meltano will as well,Meltano,48,55
e187c4bcd4af3bc78731f38618a54898,prdgxd,Relevant_take_2,https://www.reddit.com/r/dataengineering/comments/prdgxd/managing_a_data_warehouse_project_as_a_nonde_and/,"Managing a data warehouse project as a non-DE, and non-PM",APACHE_NIFI,workflow management,"But if it does go well, there's significant room for career (and CV",nifi,36,40
5466a06213d6c3488bc9d6f542801bf9,pnol2m,brownstrom,https://www.reddit.com/r/dataengineering/comments/pnol2m/data_profiling_for_a_large_dataset/,Data Profiling for a large dataset,APACHE_NIFI,workflow management,  How do I do data profiling on something so large to come up with a significant analysis of the data which is useful?,nifi,73,77
fbfbf4715e4a990d51425d3cf6f6f0df,pn2tc5,MaxGanzII,https://www.reddit.com/r/dataengineering/comments/pn2tc5/amazon_redshift_research_project_minimum_and/,"Amazon Redshift Research Project : Minimum and Maximum Values by Data Type (white paper, PDF)",APACHE_NIFI,workflow management,"During investigation, it also became clear that the mechanisms used to connect to Redshift - `psycopg2`, `psql`, `ODBC`, etc - all seems to be performing significant data processing, and at times are getting it wrong, leading to behaviour such as the silent modification of inserted values and incorrect values being presented from `SELECT`",nifi,158,162
826ac19c36aa953f8dc94baebb8b6205,pkxfd4,secodaHQ,https://www.reddit.com/r/dataengineering/comments/pkxfd4/rethinking_the_data_catalog/,Rethinking the data catalog,APACHE_NIFI,workflow management,"The tool should focus on the business users because they are the source of many discovery questions and have few options to find the answer for themselves.   Since coming to this understanding, we’ve made some significant changes to our product",nifi,214,218
94f683c4569472ec7ba465f751a05f8d,pkxfd4,secodaHQ,https://www.reddit.com/r/dataengineering/comments/pkxfd4/rethinking_the_data_catalog/,Rethinking the data catalog,APACHE_NIFI,workflow management,The first significant change is that we don't think of Secoda as a data catalog,nifi,14,18
22f87e9e6ce962616829da568f2fd58b,pdka0i,HighlyIllogicall,https://www.reddit.com/r/dataengineering/comments/pdka0i/creating_analytical_layer_using_spark_on_top_of/,Creating analytical layer using Spark on top of Parquet files,APACHE_NIFI,workflow management," so, the source system is a SQL server system:   \- I am thinking of pulling all the data in a parquet format using Nifi    \- Using spark I am gonna create tables/views out of the parquet files  \- connect Power BI and other visualization tools to Spark views/tables  However my concern is:  \- How to update parquet files whenever there are changes in the source",Nifi,117,121
cf06303daa264a9f8c0a7ac1b6ae9bc8,pdanrj,Born-Comment3359,https://www.reddit.com/r/dataengineering/comments/pdanrj/are_andy_pavlos_lectures_good_to_prepare_for_a/,Are Andy Pavlo's lectures good to prepare for a FAANG data engineer interview?,APACHE_NIFI,workflow management,"I couldn't find any good unified resources to prepare for a FAANG DE interview, and a lot of people adviced me Andy Pavlo's lectures on Youtube",nifi,27,31
87a8eeba16956920d90a84336404a671,ptdg0g,diegoelmestre,https://www.reddit.com/r/dataengineering/comments/ptdg0g/improvements_to_my_first_de_project_streaming/,Improvements to my first DE (?) project (Streaming Datawarehouse). What's next?,APACHE_SPARK,analytics,"Be free to suggest new techs (Sparks, Airflows, or even other GCP components (dataflow p.e.), I'm willing to learn it  &amp;#x200B;  &amp;#x200B;  https://preview.redd.it/lyap4qggf3p71.png?width=735&amp;format=png&amp;auto=webp&amp;s=fedf64446f22a27f195f172e653c7160a3295da6  Feel free to ask for more information",Spark,31,36
c753a3bd2ef6269aec8253ebb31fb968,ptcnp5,ZookeepergamePure467,https://www.reddit.com/r/dataengineering/comments/ptcnp5/need_help_to_advance_in_career/,Need help to advance in career,APACHE_SPARK,analytics,"I have mostly worked on SQL and spark and have a pretty good knowledge of Azure technologies like Databricks, data factory, data lake storage, etc",spark,33,38
fb1f0adae9d5413630ade1dd73c01d55,pt4t5v,Delicious_Attempt_99,https://www.reddit.com/r/dataengineering/comments/pt4t5v/scala_or_python_for_data_engineer/,Scala or Python for Data engineer?,APACHE_SPARK,analytics,"Hi All,  I’m a scala developer, not to a pro level, but I know scala with spark very well",spark,75,80
f1020de8d924f7e3e002565e5cd2deb9,psv1wj,Vorskl,https://www.reddit.com/r/dataengineering/comments/psv1wj/any_opinions_on_akretzs_learndataengineeringcom/,Any opinions on A.Kretz's learndataengineering.com ?,APACHE_SPARK,analytics,"  The courses look interesting, but I have some reservations that for example, Spark is covered in only 3 hrs ([https://learndataengineering.com/p/learning-apache-spark-fundamentals](https://learndataengineering.com/p/learning-apache-spark-fundamentals))",Spark,80,85
e9078956bddc952eb4f11cd3219741ee,pskxx5,mdl003,https://www.reddit.com/r/dataengineering/comments/pskxx5/best_tools_for_metadata_capture_indexing_for/,Best tools for metadata capture + indexing for delta/parquet tables?,APACHE_SPARK,analytics,Details of our team's current ETL process and our homegrown metadata capture/indexing solution below if that helps spark discussion,spark,116,121
fbbff5eeecea63afab562c4411324b96,pscnk0,raghukveer,https://www.reddit.com/r/dataengineering/comments/pscnk0/university_of_helsinki_is_providing_big_data/,University of Helsinki is providing Big Data Platforms Mooc (https://big-data-platforms-21.mooc.fi/),APACHE_SPARK,analytics,"Main topics are:  * distributed computing, * Warehouse-Scale Computers, * fault tolerance in distributed systems, * distributed file systems, * distributed batch processing with the MapReduce and the Apache Spark (PySpark",Apache Spark,201,213
bd15edd4d0681bc5e40a26e1baa2ea89,pryp6w,KimStacks,https://www.reddit.com/r/dataengineering/comments/pryp6w/less_than_1tb_of_data_what_tools_should_i_get/,Less than 1TB of data what tools should I get better at?,APACHE_SPARK,analytics,"I read https://docs.dask.org/en/latest/spark.html and in the very last line it said and I quote  &gt; you are looking to manage a terabyte or less of tabular CSV or JSON data, then you should forget both Spark and Dask and use Postgres or MongoDB",spark,40,45
81df88d751f02d47c5b36ccdf8d12140,pr96sa,vinsanity1603,https://www.reddit.com/r/dataengineering/comments/pr96sa/pyspark_etl_aws_glue_joining_on_nearest_timestamp/,Pyspark ETL (AWS Glue) Joining on Nearest Timestamp,APACHE_SPARK,analytics,How can I join the every-5-min data to the nearest timestamp in the every-1-min data using Pyspark,spark,94,99
a2c847ec53a7440d069568bc9c1155a6,pr6xg5,onelostsoul115,https://www.reddit.com/r/dataengineering/comments/pr6xg5/python_code_test_interview_general_career_advice/,Python Code Test (Interview) + general career advice,APACHE_SPARK,analytics,"Their tech stack is as follows:  SQL, Python, Clojure, Scala, AWS:Glue, Athena, Redshift, Apache Airflow, Kafka, Spark, Terraform  I'm a bit worried about the Python code test as my day to day and expertise is still mainly in pl/sql and pl/pgsql, so I need to brush up on my Python",Spark,114,119
bf472f124a313bbdfd8c31bbd9f5bbb1,pr6xg5,onelostsoul115,https://www.reddit.com/r/dataengineering/comments/pr6xg5/python_code_test_interview_general_career_advice/,Python Code Test (Interview) + general career advice,APACHE_SPARK,analytics,"file ingestion from S3 via Glue with some very basic spark transformations, very basic lamdbas etc",spark,54,59
59f1b13b465156d6955b8ada4e6551ce,pqty5b,scraper01,https://www.reddit.com/r/dataengineering/comments/pqty5b/stream_processing_tool_to_pair_with_kafka/,Stream processing tool to pair with Kafka,APACHE_SPARK,analytics,"So far i've learnt Spark for big data ETL, a bit of Big Query for ELT, Apache Airflow for small data ETL,  and at last Kafka as a message passing queue",Spark,20,25
b4fc3dfab19ccf775dc235df10ee3051,pqty5b,scraper01,https://www.reddit.com/r/dataengineering/comments/pqty5b/stream_processing_tool_to_pair_with_kafka/,Stream processing tool to pair with Kafka,APACHE_SPARK,analytics,"Between spark streaming, apache flink and apache storm, what do you think sinergizes the best with Kafka",spark,9,14
9aa431a4bdb8023c41c543d20cc20951,pq4dma,britishbanana,https://www.reddit.com/r/dataengineering/comments/pq4dma/how_do_you_handle_data_versioning_for/,How do you handle data versioning for reproducibility?,APACHE_SPARK,analytics," We are heavily based on Spark and we only recalculate our datasets a few times a year as new data comes in (lambda architecture), so this versioning strategy has been fine if suboptimal",Spark,26,31
77433b6e732eaba38da33de2c13d4985,pq4dma,britishbanana,https://www.reddit.com/r/dataengineering/comments/pq4dma/how_do_you_handle_data_versioning_for/,How do you handle data versioning for reproducibility?,APACHE_SPARK,analytics," I've seen some tools such as Pachyderm that attempt to address this, but have no idea how well they work in practice - and haven't seen anything built more for a Spark context",Spark,164,169
ad3776a65766e3ed3e562aba440f7280,ppuh5c,hatchikyu,https://www.reddit.com/r/dataengineering/comments/ppuh5c/if_i_had_to_explain_data_engineering_work_to_a/,If I had to explain data engineering work to a business colleague...,APACHE_SPARK,analytics," **Supports the following kind of work (that business people understand):**  * Machine learning * Data Science * Deep Learning  **Most common jargon**  ETL  - extract-transform-load — tooling to move data from place to place  **Scope of work**  * Enormous amounts of data means heavy lifting around **scalability** * Responsible for deciding what data is useful and what is superfluous * Today’s data engineers are essentially specialist software engineers  **Capability areas**  * SDLC best practice - version, control, release management, automated pipelines, implementing open-source tools like Spark and Tensorflow * InfoSec - cloud security best practice, data handling, data privacy, GDPR, OSS security etc",Spark,599,604
2dfd2c0fa1999eaf51876cd53a62f526,ppmc48,getafterit123,https://www.reddit.com/r/dataengineering/comments/ppmc48/spark_and_s3_integration/,Spark and S3 integration,APACHE_SPARK,analytics,Has anyone been able to connect spark 3.0+ to S3 object store using s3 connector or only s3a,spark,33,38
814a9dff5f49ec057b148a1565f13915,poimtb,brownstrom,https://www.reddit.com/r/dataengineering/comments/poimtb/data_profiling_reviews_on_dbtprofiler_or_any/,Data Profiling: Reviews on dbt-profiler or any similar tool?,APACHE_SPARK,analytics,  Don't like pandas profiler as it doesn't use Spark so it would be very expensive to convert spark df to pandas for data profiling,Spark,48,53
d58041c1498e8aab2fabba3409794f0d,poi4im,gloriarachel77,https://www.reddit.com/r/dataengineering/comments/poi4im/how_can_i_become_a_data_engineer/,How can I become a data engineer?,APACHE_SPARK,analytics,"I took several courses regarding Hadoop and PySpark, but still feel a little confused",Spark,47,52
00f8064f44fb54e9352cae7840de4378,po9ru6,theant97,https://www.reddit.com/r/dataengineering/comments/po9ru6/mind_fish_confused/,Mind fish : confused,APACHE_SPARK,analytics,"Complete python, sql and spark courses and become a de  2",spark,26,31
baf2388097fbe6ccb0f9dbe1e14e8d95,po8sm8,monistordualist,https://www.reddit.com/r/dataengineering/comments/po8sm8/pyspark_on_kubernetes_a_production_boilerplate/,PySpark on Kubernetes: A Production Boilerplate Framework,APACHE_SPARK,analytics,"I spent some time looking for an all-in-one template/demo for how to run PySpark jobs on Kubernetes--and couldn't quite find something fully fledged, so I made one myself specifically with data scientists and engineers in mind",Spark,76,81
8bd85f9c6d09e6fb2ff7829381921831,po8sm8,monistordualist,https://www.reddit.com/r/dataengineering/comments/po8sm8/pyspark_on_kubernetes_a_production_boilerplate/,PySpark on Kubernetes: A Production Boilerplate Framework,APACHE_SPARK,analytics, Here's the repo: [https://github.com/Albell-Cloud-Labs/pyspark-k8s-boilerplate](https://github.com/Albell-Cloud-Labs/pyspark-k8s-boilerplate,spark,59,64
9b4a2ca67e8f8c2ef433186c0bf2641b,po1np1,FeelsToWaltz,https://www.reddit.com/r/dataengineering/comments/po1np1/starting_my_first_de_role_good_resources_for/,Starting my first DE role - good resources for learning Scala and Spark?,APACHE_SPARK,analytics,I'm transitioning into a role as a Data Engineer at a company that primarily uses Scala as well as Spark and Hadoop,Spark,100,105
108b2282d036d196a3ab6928008c12a3,pny53c,sgtbrecht,https://www.reddit.com/r/dataengineering/comments/pny53c/an_alternative_to_aws_solutions_architect_course/,An alternative to AWS Solutions Architect course on Udemy?,APACHE_SPARK,analytics,"I then watched a bunch of videos on youtube and udemy on Python, Pandas API and Spark",Spark,81,86
ac80d2f8a8cd489a8c401441533c27b6,pnmvkm,treacherous_tim,https://www.reddit.com/r/dataengineering/comments/pnmvkm/upskilling_in_api_service_development_as_a_data/,Upskilling in API / Service development as a data engineer,APACHE_SPARK,analytics,"I've worked for a while as a data engineer building out pipelines using Airflow, Spark, etc",Spark,82,87
84fcb4ca41ec5823ce88fa2bfc9222b0,pmeio7,O_its_that_guy_again,https://www.reddit.com/r/dataengineering/comments/pmeio7/im_looking_for_advice_on_a_beginner_data/,I'm looking for advice on a beginner data engineering project.,APACHE_SPARK,analytics," &amp;#x200B;  We are going through SQL and Spark over the course, so I'll be using those to the extent that I can, along with python",Spark,45,50
a5be56d26aed6356527f5870cd1cabf5,pmeg4k,O_its_that_guy_again,https://www.reddit.com/r/dataengineering/comments/pmeg4k/advice/,Advice,APACHE_SPARK,analytics," &amp;#x200B;  We are going through SQL and Spark over the course, so I'll be using those to the extent that I can, along with python",Spark,45,50
9e3ae665f98bdcf1668a2ae1ddcdcd9b,pljfsl,ckdatanerd,https://www.reddit.com/r/dataengineering/comments/pljfsl/best_ways_to_learn_advanced_databricks_pyspark/,Best Ways To Learn Advanced Databricks (Pyspark),APACHE_SPARK,analytics,"I have a lot of opportunity to work on what I want to, and some of the cooler projects I have gotten involved with use Databricks (pyspark — ETL pipelines, optimizing ML notebooks for production, optimizing cluster usage",spark,134,139
d3e1e6dfe9179146135317b9fbdbb191,pljfsl,ckdatanerd,https://www.reddit.com/r/dataengineering/comments/pljfsl/best_ways_to_learn_advanced_databricks_pyspark/,Best Ways To Learn Advanced Databricks (Pyspark),APACHE_SPARK,analytics,"I have found various sources online on my own (reviewed documentation, read the book Learning Spark 2.0), but I have been having trouble finding a book that covers more advanced pyspark utilization and dives into how to optimize different parameters",Spark,95,100
126448015e7a8b7a8a4b9ad7dd3024a9,pljfsl,ckdatanerd,https://www.reddit.com/r/dataengineering/comments/pljfsl/best_ways_to_learn_advanced_databricks_pyspark/,Best Ways To Learn Advanced Databricks (Pyspark),APACHE_SPARK,analytics,"The book Learning Spark 2.0 has been immensely helpful for me so far, and I am in hope of finding an “advanced” version of it or other useful resources",Spark,19,24
fec62082dc18d9e609ae85d298816e56,pli0og,MediumZealousideal29,https://www.reddit.com/r/dataengineering/comments/pli0og/skills_required/,Skills required,APACHE_SPARK,analytics,I am very good at spark,spark,19,24
f2527b3a20b4e9e5f845e7772464b8f8,pli0og,MediumZealousideal29,https://www.reddit.com/r/dataengineering/comments/pli0og/skills_required/,Skills required,APACHE_SPARK,analytics,"I know SQL, hdfs, hive, control M..I also learned airflow, sqoop and basics of spark streaming, Kafka and AWS",spark,80,85
b1b13215a4d87565f5ceff73622724f1,pkd8uk,deveid,https://www.reddit.com/r/dataengineering/comments/pkd8uk/how_to_validate_a_table_refresh_in_hive/,How to validate a table refresh in hive,APACHE_SPARK,analytics,"Using pyspark, but not sure what cases I can check for",spark,9,14
f16aebd0e03607a37eaa63006417c5e9,pk78fp,ClumsyRooster,https://www.reddit.com/r/dataengineering/comments/pk78fp/airflow_spark_other_tool/,"Airflow, Spark, other tool ?",APACHE_SPARK,analytics,So I took the time to use Airflow + Spark,Spark,37,42
6a2ce30e804d76ded146aab9a0da4feb,pk78fp,ClumsyRooster,https://www.reddit.com/r/dataengineering/comments/pk78fp/airflow_spark_other_tool/,"Airflow, Spark, other tool ?",APACHE_SPARK,analytics, What's the advantage of using Spark instead of Airflow for small processing ,Spark,32,37
ec259b4fc419903ecee1106579d07ae0,pjt56n,gamecockguy2003,https://www.reddit.com/r/dataengineering/comments/pjt56n/best_way_to_get_snapshotfull_dataset_from_changes/,Best way to get snapshot/full dataset from changes,APACHE_SPARK,analytics, I've been trying to do this with Spark but am struggling with doing this performantly given the changes are spread throughout the dataset,Spark,35,40
b011b64a7c8837835b4ef63b75b5c42a,pjsuu8,Irmodude2003,https://www.reddit.com/r/dataengineering/comments/pjsuu8/best_way_to_get_snapshotfull_dataset_from_changes/,Best way to get snapshot/full dataset from changes,APACHE_SPARK,analytics, I've been trying to do this with Spark but am struggling with doing this performantly given the changes are spread throughout the dataset,Spark,35,40
33f4ed8040ec5b504b769382fca4b636,pjf1ui,Gawgba,https://www.reddit.com/r/dataengineering/comments/pjf1ui/taming_big_data_with_apache_spark_and_python/,Taming Big Data with Apache Spark and Python - Hands On! question,APACHE_SPARK,analytics,"the course uses Spark 2 when the most recent version is 3, and seems to have a very long section on RDDs but I thought I'd heard that RDDs were no longer in favor and dataframes are where it's at",Spark,17,22
02c52cb8a82a8b628412e41284d35522,pii29n,dolphinday2,https://www.reddit.com/r/dataengineering/comments/pii29n/programming_etl_vs_sql_etl/,Programming ETL vs SQL ETL,APACHE_SPARK,analytics,"Because for Scala for example, you would use Spark SQL, which is really just SQL at the end of the day",Spark,46,51
8a74eb98301eb20e8bf45c92ab6c3b8c,ph4fvj,WalrusWhich202,https://www.reddit.com/r/dataengineering/comments/ph4fvj/how_to_querying_data_faster/,How to Querying data faster,APACHE_SPARK,analytics," I've looked into spark, hadoop, cassandra, hdfs, and others",spark,19,24
3e308f0ca528af7224220d58ae302889,ph17wi,Efficient_Camel6175,https://www.reddit.com/r/dataengineering/comments/ph17wi/data_engineering_project/,Data Engineering Project,APACHE_SPARK,analytics,Can someone provide Simple Data Engineering Project using Hadoop and Spark for reference?,Spark,70,75
6d442c51c3d7745569c132eea199bc7b,ph15au,SukalpVashishtha,https://www.reddit.com/r/dataengineering/comments/ph15au/microservice_shared_database_antipattern_question/,Microservice shared database antipattern question,APACHE_SPARK,analytics,  Is it a fair design pattern that a stream processing framework (spark/kafka streams etc,spark,67,72
24c382cd02ab3744d2a503d7bd3e1dbf,pgs1z3,Kassme,https://www.reddit.com/r/dataengineering/comments/pgs1z3/spark_dfs_vs_pandas_dfs/,Spark DF’s vs Pandas DF’s,APACHE_SPARK,analytics,"Hello fellow DE’s, I’m curious for those of you that use notebooks to interface with spark (I’m working with PySpark notebooks in Azure Synapse",spark,86,91
c6c87007dbc8e1516a9785fb55cfb925,pgs1z3,Kassme,https://www.reddit.com/r/dataengineering/comments/pgs1z3/spark_dfs_vs_pandas_dfs/,Spark DF’s vs Pandas DF’s,APACHE_SPARK,analytics,- do you think it’s worth learning how to use native spark DF’s,spark,54,59
80e47af4a60224101f0708ed932f5bd3,pgs1z3,Kassme,https://www.reddit.com/r/dataengineering/comments/pgs1z3/spark_dfs_vs_pandas_dfs/,Spark DF’s vs Pandas DF’s,APACHE_SPARK,analytics,Currently I have been told to just use pandas because it’s what I’m familiar with but I wonder if it’s worth the effort to learn to use the DF methods that are native to spark,spark,171,176
d17048c1cfeec3709f23778d157ee17c,pgs1z3,Kassme,https://www.reddit.com/r/dataengineering/comments/pgs1z3/spark_dfs_vs_pandas_dfs/,Spark DF’s vs Pandas DF’s,APACHE_SPARK,analytics,"  PS pls refrain from recommending an option C in this case, I don’t get to choose the stack I have to work with in this project, it’s either pandas or PySpark df’s",Spark,155,160
ef45cc18015748936f30d29e983a30ce,pggz3t,pknpkn21,https://www.reddit.com/r/dataengineering/comments/pggz3t/cicd_for_de_pipeline/,CI/CD for DE Pipeline,APACHE_SPARK,analytics,"Hi,  We are currently in the process of setting up a data processing pipeline for few of our data sources and the processing logic is designed in Pyspark with Airflow for orchestration in AWS environment",spark,149,154
1e6b8253d491acf5937fc91b97f0c3e4,pfv18d,Godmons,https://www.reddit.com/r/dataengineering/comments/pfv18d/spark_jobs_should_we_define_schema_on_read/,Spark Jobs : Should we define schema on read ?,APACHE_SPARK,analytics,"Hi guys,     When importing a dataframe from parquet or other file format in spark, we can either infer or specify the schema (data types",spark,78,83
38374f3592840b0a5418a857cf1227ea,pfv18d,Godmons,https://www.reddit.com/r/dataengineering/comments/pfv18d/spark_jobs_should_we_define_schema_on_read/,Spark Jobs : Should we define schema on read ?,APACHE_SPARK,analytics," On my previous company, we were using spark dataframe &amp; INFERING before processing.On my current company, we are specifying the schema on each read",spark,40,45
1548b0bbff225d6661dfe10cbddc4925,pfemin,Zorkol02,https://www.reddit.com/r/dataengineering/comments/pfemin/cloud_certification_help/,Cloud Certification Help,APACHE_SPARK,analytics,"I know python, sql, bash, pyspark and airflow, but not an expert yet just in case",spark,29,34
60dc2c6406d7eb66ccea866b2a51ae94,peyiec,lurakona,https://www.reddit.com/r/dataengineering/comments/peyiec/thinking_about_pivoting_from_swe/,Thinking about Pivoting from SWE,APACHE_SPARK,analytics,I spoke with a Data Engineer and this is what originally sparked my interest,spark,58,63
b14acd91fd441bc90cfd8d85aa09dbeb,peap7h,CoMatrix1234,https://www.reddit.com/r/dataengineering/comments/peap7h/a_self_governing_data_gateway/,A self governing data gateway,APACHE_SPARK,analytics," Our ecosystem: AWS, Okta, S3, terraform, looking at Amundsen, Spark, cloud monitoring tools",Spark,64,69
8cb1d551d2af8b60a68bd73ef94ee936,pe3i1b,OkDetective1169,https://www.reddit.com/r/dataengineering/comments/pe3i1b/data_science_domain_job_title_and_description/,""Data Science"" Domain: Job Title and Description confusion,APACHE_SPARK,analytics, I really love the engineering aspect of helping the data scientists get what they need through pipelines using AWS/SQL/PySpark/Python and sometimes Tableau,Spark,123,128
264aba353d9cbe63ba7dfee0002a809c,pdni20,sedition8,https://www.reddit.com/r/dataengineering/comments/pdni20/spark_design_patterns/,Spark Design Patterns,APACHE_SPARK,analytics,I know that the company I'm applying to uses spark for stream processing and asks questions based on their current infra,spark,46,51
331d47df667fc8d9c36fa86069f6ac91,pdni20,sedition8,https://www.reddit.com/r/dataengineering/comments/pdni20/spark_design_patterns/,Spark Design Patterns,APACHE_SPARK,analytics,   I'm looking for resources that show how Spark would get integrated with an end to end system,Spark,44,49
a7cf2d02457b94ea922262f474e92d52,pdni20,sedition8,https://www.reddit.com/r/dataengineering/comments/pdni20/spark_design_patterns/,Spark Design Patterns,APACHE_SPARK,analytics,Spark receives data from Kafka queues 2,Spark,1,6
2a5ef48dccb479d2ccdcb943b1ff8b3d,pdni20,sedition8,https://www.reddit.com/r/dataengineering/comments/pdni20/spark_design_patterns/,Spark Design Patterns,APACHE_SPARK,analytics,Spark aggregates the data,Spark,1,6
880bbc1c6869ed33835734cefc688e85,pdka0i,HighlyIllogicall,https://www.reddit.com/r/dataengineering/comments/pdka0i/creating_analytical_layer_using_spark_on_top_of/,Creating analytical layer using Spark on top of Parquet files,APACHE_SPARK,analytics,"Hello,  I am trying to build an analytical layer using Spark I am still in the planning stage, the idea is we want to try new technologies within my team to see the potentials of such technology",Spark,56,61
27f002a4786e7bcb734fc10264daa6c9,pdka0i,HighlyIllogicall,https://www.reddit.com/r/dataengineering/comments/pdka0i/creating_analytical_layer_using_spark_on_top_of/,Creating analytical layer using Spark on top of Parquet files,APACHE_SPARK,analytics," so, the source system is a SQL server system:   \- I am thinking of pulling all the data in a parquet format using Nifi    \- Using spark I am gonna create tables/views out of the parquet files  \- connect Power BI and other visualization tools to Spark views/tables  However my concern is:  \- How to update parquet files whenever there are changes in the source",spark,134,139
dd4a47c7b76802e9a39b9f7b4080c305,pdka0i,HighlyIllogicall,https://www.reddit.com/r/dataengineering/comments/pdka0i/creating_analytical_layer_using_spark_on_top_of/,Creating analytical layer using Spark on top of Parquet files,APACHE_SPARK,analytics, \- will it be better to read directly from SQL tables and update the spark fact/dim tables,spark,71,76
24d7075b8bbc2d461e5651461b254214,pdka0i,HighlyIllogicall,https://www.reddit.com/r/dataengineering/comments/pdka0i/creating_analytical_layer_using_spark_on_top_of/,Creating analytical layer using Spark on top of Parquet files,APACHE_SPARK,analytics,what made me consider parquet format is the fact that it is recommended by spark to be used.,spark,76,81
689acd5f4852679dbec4b922aa61facd,pdejm8,Simonaque,https://www.reddit.com/r/dataengineering/comments/pdejm8/career_advice_unsure_if_i_should_pursue_a_masters/,Career Advice: Unsure if I should pursue a Master's of Computer Science since I have a non-technical Undergraduate Degree.,APACHE_SPARK,analytics,"Business Analyst - Certificates: AWS CCP, AWS DAS, DB Spark Developer, MCAF, Gitlab Assoc",Spark,55,60
1abe416961a88d382a20443774902dd9,pctg6h,statistically_broke,https://www.reddit.com/r/dataengineering/comments/pctg6h/parquet_row_group_indexing/,Parquet Row Group ""Indexing"",APACHE_SPARK,analytics," For reference, using primarily Pyspark on Azure Databricks and processing from a Data Lake to a Delta Lake",spark,35,40
582adeb5f1e3527204623cdf85efeec6,pcrlzi,Professional_Crazy49,https://www.reddit.com/r/dataengineering/comments/pcrlzi/confused_about_de_vs_sde/,Confused about DE vs SDE,APACHE_SPARK,analytics," The Confusion:  So one of my friends works as a software engineer 1 and he has worked with kafka, docker, aws, prometheus, spark",spark,125,130
f4fd84c11f304686c426e598b263575d,pcokrh,dathu9,https://www.reddit.com/r/dataengineering/comments/pcokrh/is_dbt_can_be_used_at_the_enterprise_level/,is dbt can be used at the enterprise level?,APACHE_SPARK,analytics,Can dbt read the data from s3 without EMR cluster (Spark cluster,Spark,52,57
ea0add59ccfb43574845e497f16099e3,pceu26,skpfm_06,https://www.reddit.com/r/dataengineering/comments/pceu26/been_working_in_a_small_startup_for_almost_2/,"Been working in a small startup for almost 2 years doing pretty much everything on the engineering side, what should I focus on and pursue moving forward?",APACHE_SPARK,analytics,"Because I was doing most of these solo for so long, I haven't had the chance to delve deeper into a subset of these tasks, so I am not familiar with tools like Luigi, Airflow, Kubernetes, Deeper understanding of AWS services, writing python like they do in Open Source libraries, Hadoop, Spark, etc",Spark,289,294
a0215b3f431955ee4b8b2eb06d153278,pc6jwg,eddygaras,https://www.reddit.com/r/dataengineering/comments/pc6jwg/moving_away_from_ssis/,Moving away from SSIS,APACHE_SPARK,analytics,Or using python and frameworks like Airflow and Spark or some others,Spark,49,54
ed2e4e77668c194572e90adc5c6a4b3d,pt0vhk,pineapplesoda1e-3,https://www.reddit.com/r/dataengineering/comments/pt0vhk/parquet_files/,parquet files,APACHE_PARQUET,formats,"Hey, i have parquet files where each column is a pixel value of a image,i want to feed my pytorch model,should i directly extract each row as image from parquet file?I wanted to make each row a pickle file which i will feed to model",parquet,13,20
34893799c2bdf6405d7608b959c28ee4,pskxx5,mdl003,https://www.reddit.com/r/dataengineering/comments/pskxx5/best_tools_for_metadata_capture_indexing_for/,Best tools for metadata capture + indexing for delta/parquet tables?,APACHE_PARQUET,formats,"What tools are out there that can automate column value testing, metadata capture, and indexing at scale for columns in delta/parquet tables",parquet,127,134
4275b1ee07697db8434886655122cf10,psh63n,pineapplesoda1e-3,https://www.reddit.com/r/dataengineering/comments/psh63n/parquet_files/,Parquet files,APACHE_PARQUET,formats,What is the fastest way to read parquet file form S3 Bucket using Python?Should i stream file?Should i just use pandas?,parquet,33,40
761c501a1d17de479a1bac3d4691e555,pl9n2x,DatKalvin,https://www.reddit.com/r/dataengineering/comments/pl9n2x/is_this_architecture_cost_effective_performant/,Is this Architecture cost effective &amp; performant? What's your suggestion.,APACHE_PARQUET,formats,stream to S3 in parquet format using AWS Database Migration Service as new records becomes available,parquet,17,24
f0d8448c6677c98d8a758252f6ec2ef9,pjt56n,gamecockguy2003,https://www.reddit.com/r/dataengineering/comments/pjt56n/best_way_to_get_snapshotfull_dataset_from_changes/,Best way to get snapshot/full dataset from changes,APACHE_PARQUET,formats,"My goal is to create a representation of the full set of that domain data based on those changes, with the output being something like a parquet file (though this is flexible",parquet,138,145
2e4fd53802e70d6308a5d274cf355533,pjsuu8,Irmodude2003,https://www.reddit.com/r/dataengineering/comments/pjsuu8/best_way_to_get_snapshotfull_dataset_from_changes/,Best way to get snapshot/full dataset from changes,APACHE_PARQUET,formats,"My goal is to create a representation of the full set of that domain data based on those changes, with the output being something like a parquet file (though this is flexible",parquet,138,145
3716524982a15076e0cd57515a85d730,pie34a,umarcja,https://www.reddit.com/r/dataengineering/comments/pie34a/storing_json_logs_in_aws_for_querying/,Storing JSON logs in AWS for querying,APACHE_PARQUET,formats," I thought of using the Parquet format as it's small and powerful, and this format will allow me to filter base on one column easily, but the problem is that I need to provide the raw file in the end",Parquet,25,32
c8bdb03b0453d9f9abc89cc828da75d7,pie34a,umarcja,https://www.reddit.com/r/dataengineering/comments/pie34a/storing_json_logs_in_aws_for_querying/,Storing JSON logs in AWS for querying,APACHE_PARQUET,formats,Is it completely wrong to use Parquet and have a column that stores raw JSON logs,Parquet,31,38
90f96572421e44310e2b25cc2f111588,pie34a,umarcja,https://www.reddit.com/r/dataengineering/comments/pie34a/storing_json_logs_in_aws_for_querying/,Storing JSON logs in AWS for querying,APACHE_PARQUET,formats, I thought of Athena to query Parquet files or dump raw logs in Elasticsearch,Parquet,31,38
9d3c296f12c0f2198affa3cfb514873e,pgq50c,king_booker,https://www.reddit.com/r/dataengineering/comments/pgq50c/processing_different_file_format_data/,processing different file format data,APACHE_PARQUET,formats,Which is later converted into parquet tables in hive,parquet,31,38
7297d1ca5b3226a4fa0e6b33c410b020,pgq50c,king_booker,https://www.reddit.com/r/dataengineering/comments/pgq50c/processing_different_file_format_data/,processing different file format data,APACHE_PARQUET,formats,"Is it better to read the json into a kafka topic, push it to hdfs as a json, create a hive external table with json serde and then convert it to parquet in other staging tables",parquet,146,153
3a6eff8d81295cd9659124c425950f6e,pfv18d,Godmons,https://www.reddit.com/r/dataengineering/comments/pfv18d/spark_jobs_should_we_define_schema_on_read/,Spark Jobs : Should we define schema on read ?,APACHE_PARQUET,formats,"Hi guys,     When importing a dataframe from parquet or other file format in spark, we can either infer or specify the schema (data types",parquet,46,53
3be2f85162c11c2b7d5ffc0412cffec4,peieca,Ok-Sentence-8542,https://www.reddit.com/r/dataengineering/comments/peieca/how_do_you_guys_validate_your_data_how_does_your/,"How do you guys validate your data? How does your process look like and which tools are you using? Our main Stack: Databricks, Azure Datafactory, Data Lake",APACHE_PARQUET,formats,We are using databricks and azure data lake and would like to test our parquet files against expectations such as:  &amp;#x200B;      Pipeline fails if:      - customerId null or customerId is not unique     -  productPrice is smaller or equal to 0     - ,parquet,72,79
dce554c7f5fb8e093aba3dd6ba1cf460,pe9f42,TiDuNguyen,https://www.reddit.com/r/dataengineering/comments/pe9f42/purely_columnar_database_suggestion/,Purely columnar database suggestion,APACHE_PARQUET,formats,"My usecase requires it to  * optimize on read/add/remove columns * scale well with column number  * ease of use/maintenance, something as easy to read by columns as a `parquet` file but also support updating by columns",parquet,169,176
9355734ccfe8fa0fecc800041ebbc660,pdka0i,HighlyIllogicall,https://www.reddit.com/r/dataengineering/comments/pdka0i/creating_analytical_layer_using_spark_on_top_of/,Creating analytical layer using Spark on top of Parquet files,APACHE_PARQUET,formats," so, the source system is a SQL server system:   \- I am thinking of pulling all the data in a parquet format using Nifi    \- Using spark I am gonna create tables/views out of the parquet files  \- connect Power BI and other visualization tools to Spark views/tables  However my concern is:  \- How to update parquet files whenever there are changes in the source",parquet,96,103
ef37ed5046791441bd8952fd73843da0,pdka0i,HighlyIllogicall,https://www.reddit.com/r/dataengineering/comments/pdka0i/creating_analytical_layer_using_spark_on_top_of/,Creating analytical layer using Spark on top of Parquet files,APACHE_PARQUET,formats,what made me consider parquet format is the fact that it is recommended by spark to be used.,parquet,23,30
c76b749ac79790fb56463f0ecbb5849e,pctg6h,statistically_broke,https://www.reddit.com/r/dataengineering/comments/pctg6h/parquet_row_group_indexing/,Parquet Row Group ""Indexing"",APACHE_PARQUET,formats,"Posted this in another subreddit as well, but figured would cast my net wider since this is a bit niche:   I'm not what you would call *the best* or *most knowledgeable* on Parquet files, but I recently came across their file structure and was wondering",Parquet,174,181
e3a16198ca00323db4b4af3dc1bc8535,pctg6h,statistically_broke,https://www.reddit.com/r/dataengineering/comments/pctg6h/parquet_row_group_indexing/,Parquet Row Group ""Indexing"",APACHE_PARQUET,formats, Desired use case: filter out already processed row groups from a parquet file to limit the amount of data kept for downstream merging with existing tables,parquet,67,74
50db3bb9ba798866f79a3ac7cc48589b,pctg6h,statistically_broke,https://www.reddit.com/r/dataengineering/comments/pctg6h/parquet_row_group_indexing/,Parquet Row Group ""Indexing"",APACHE_PARQUET,formats,Grab IngestionParquetFile from Data Lake,Parquet,15,22
8f40ed33896438612b33533a642f9ea1,pctg6h,statistically_broke,https://www.reddit.com/r/dataengineering/comments/pctg6h/parquet_row_group_indexing/,Parquet Row Group ""Indexing"",APACHE_PARQUET,formats,Save Filtered new Row Groups as UpdatesParquetFile,Parquet,40,47
40b160ec20a67e14bd19a212c8d5106d,pctg6h,statistically_broke,https://www.reddit.com/r/dataengineering/comments/pctg6h/parquet_row_group_indexing/,Parquet Row Group ""Indexing"",APACHE_PARQUET,formats,Save the list of Row Group Checksums in the UpdatesParquetFile to keep track of processing them to the database,Parquet,52,59
873fab636746bb580e99249b4da604cc,pctg6h,statistically_broke,https://www.reddit.com/r/dataengineering/comments/pctg6h/parquet_row_group_indexing/,Parquet Row Group ""Indexing"",APACHE_PARQUET,formats,downstream processing of the much smaller UpdatesParquetFile.,Parquet,50,57
2419defc2c6b244ab297a4d757ded594,pcav10,Integrator_Eye,https://www.reddit.com/r/dataengineering/comments/pcav10/changing_column_data_type_on_large_hive_tables/,Changing column data type on large hive tables,APACHE_PARQUET,formats,There are more than 250+ tables defined using parquet format and having 1000+ partitions,parquet,47,54
04fb7da8391d0cdaf4e7f70ff7b488b4,ptdg0g,diegoelmestre,https://www.reddit.com/r/dataengineering/comments/ptdg0g/improvements_to_my_first_de_project_streaming/,Improvements to my first DE (?) project (Streaming Datawarehouse). What's next?,KAFKA_CONNECT,integration,    1 - We have a Kafka Connect running that grabs the changes from our MongoDB collections,Kafka Connect,19,32
a67d7ec97f42fbc3cf9b03295f7912ee,ph15au,SukalpVashishtha,https://www.reddit.com/r/dataengineering/comments/ph15au/microservice_shared_database_antipattern_question/,Microservice shared database antipattern question,APACHE_KAFKA_STREAMS,stream processing,  Is it a fair design pattern that a stream processing framework (spark/kafka streams etc,kafka streams,73,86
3e3731ca66080001e5a1b0bb2497b048,ps0h44,scraper01,https://www.reddit.com/r/dataengineering/comments/ps0h44/is_anyone_here_still_using_apache_storm/,Is anyone here still using Apache Storm?,APACHE_STORM,stream processing,Wondering if anyone here prefers Storm above other stream frameworks like Flink and why.,Storm,34,39
8dcb3c13ee75324208f0234a963930df,pqty5b,scraper01,https://www.reddit.com/r/dataengineering/comments/pqty5b/stream_processing_tool_to_pair_with_kafka/,Stream processing tool to pair with Kafka,APACHE_STORM,stream processing,"Between spark streaming, apache flink and apache storm, what do you think sinergizes the best with Kafka",apache storm,43,55
57720113fc51d4c1da327528d5ce7ed1,po6zh0,Nervous-Chain-5301,https://www.reddit.com/r/dataengineering/comments/po6zh0/aws_managed_airflow_or_aws_sam_for_simple_etls/,AWS Managed Airflow or AWS SAM for Simple ETLs,APACHE_STORM,stream processing,"We are planning on ingesting data sources that do not have a native Fivetran connector, and I am brainstorming the best approach",storm,103,108
7f79c7a4beb0b5630ea5736f1bcb824a,po5kgw,marcosmarxm,https://www.reddit.com/r/dataengineering/comments/po5kgw/presentation_airbyte_airflow_on_gcp/,Presentation: Airbyte + Airflow on GCP,APACHE_STORM,stream processing,You can register for the Livestorm session here: [https://airbyte.io/weekly-office-hours](https://airbyte.io/weekly-office-hours?utm_content=buffer52018&amp;utm_medium=social&amp;utm_source=linkedin.com&amp;utm_campaign=buffer,storm,30,35
cff8496d9cd496d7625e7757ed718ca7,ptfb2d,redditthrowaway0315,https://www.reddit.com/r/dataengineering/comments/ptfb2d/does_airflow_provide_a_functionality_to_remove/,Does Airflow provide a functionality to remove historical data created by previous runs of a DAG?,APACHE_AIRFLOW,workflow management,I can clear previous runs and Airflow will schedule re-runs but the re-runs do NOT remove historical data,Airflow,31,38
d1dbdd0b6b486283e4c46fcd9f3cccaf,ptfb2d,redditthrowaway0315,https://www.reddit.com/r/dataengineering/comments/ptfb2d/does_airflow_provide_a_functionality_to_remove/,Does Airflow provide a functionality to remove historical data created by previous runs of a DAG?,APACHE_AIRFLOW,workflow management,It looks like the only way to do it properly is to:  * Pause DAG in Airflow UI * Manually DELETE all data of previous 60 days * Manually run the new script and INSERT data for the past 60 days * Unpause the DAG  However this is error-prone and can be tricky to implement if say I want to re-run the previous 48 hour data for an hourly process (so that I really need to be careful where to cut the data,Airflow,69,76
54836da2a7e20d4a49ffd68ce1b5e6a7,ptfb2d,redditthrowaway0315,https://www.reddit.com/r/dataengineering/comments/ptfb2d/does_airflow_provide_a_functionality_to_remove/,Does Airflow provide a functionality to remove historical data created by previous runs of a DAG?,APACHE_AIRFLOW,workflow management,"If everytime I cleared a run, Airflow can run a script in background (e.g",Airflow,31,38
123dc34b6c4eea8cceba647de9965df5,ptfb2d,redditthrowaway0315,https://www.reddit.com/r/dataengineering/comments/ptfb2d/does_airflow_provide_a_functionality_to_remove/,Does Airflow provide a functionality to remove historical data created by previous runs of a DAG?,APACHE_AIRFLOW,workflow management,  Is there a way to do this in Airflow?,Airflow,32,39
4f66054164634198657e94aab6b5b252,ptdg0g,diegoelmestre,https://www.reddit.com/r/dataengineering/comments/ptdg0g/improvements_to_my_first_de_project_streaming/,Improvements to my first DE (?) project (Streaming Datawarehouse). What's next?,APACHE_AIRFLOW,workflow management,"Be free to suggest new techs (Sparks, Airflows, or even other GCP components (dataflow p.e.), I'm willing to learn it  &amp;#x200B;  &amp;#x200B;  https://preview.redd.it/lyap4qggf3p71.png?width=735&amp;format=png&amp;auto=webp&amp;s=fedf64446f22a27f195f172e653c7160a3295da6  Feel free to ask for more information",Airflow,39,46
18fda7878779b857a0b7599bed3caeba,ptceyv,kristiclimbs,https://www.reddit.com/r/dataengineering/comments/ptceyv/airflow_externaltasksensor_proper_way_to_use_for/,Airflow - ExternalTaskSensor-&gt; proper way to use for subdag task,APACHE_AIRFLOW,workflow management,"Hello,  I'm having difficulty figuring out how to use the ExternalTaskSensor for Airflow",Airflow,82,89
a3655d2212e7e138fba003f8389fce76,ptceyv,kristiclimbs,https://www.reddit.com/r/dataengineering/comments/ptceyv/airflow_externaltasksensor_proper_way_to_use_for/,Airflow - ExternalTaskSensor-&gt; proper way to use for subdag task,APACHE_AIRFLOW,workflow management,and the Airflow slack channel but I haven't received a response,Airflow,9,16
0c958b08d788892b3a3d756c015a753a,psryjw,Assi6,https://www.reddit.com/r/dataengineering/comments/psryjw/tips_and_useful_websitecourses_to_get_into_data/,Tips and useful website/courses to get into data engineering?,APACHE_AIRFLOW,workflow management,I know a bit of Airflow but zero knowledge about cloud things (AWS for example,Airflow,17,24
a20a54a63c56023395c06b23e8251418,psntnd,honorchan1,https://www.reddit.com/r/dataengineering/comments/psntnd/tutorial_monitoring_airflow_with_prometheus/,"Tutorial: Monitoring Airflow with Prometheus, StatsD and Grafana",APACHE_AIRFLOW,workflow management,"In this post, Vova guides you through how to build an open-source Airflow monitoring solution that helps you visualize Airflow cluster metrics",Airflow,67,74
956b054151cc6ea680bc7bf0c36f864b,psntnd,honorchan1,https://www.reddit.com/r/dataengineering/comments/psntnd/tutorial_monitoring_airflow_with_prometheus/,"Tutorial: Monitoring Airflow with Prometheus, StatsD and Grafana",APACHE_AIRFLOW,workflow management,"If you've ever wanted to configure an [**open source data observability dashboard**](https://databand.ai/blog/everyday-data-engineering-monitoring-airflow-with-prometheus-statsd-and-grafana/?utm_source=forum&amp;utm_medium=r&amp;utm_group=de), we think you'll find this useful",airflow,148,155
50236673b76367095ffdeebfe83e627f,pskxx5,mdl003,https://www.reddit.com/r/dataengineering/comments/pskxx5/best_tools_for_metadata_capture_indexing_for/,Best tools for metadata capture + indexing for delta/parquet tables?,APACHE_AIRFLOW,workflow management,From there we transform the raw data and insert it into delta tables using Databricks jobs orchestrated by Airflow,Airflow,108,115
7373c4c2b7f1939fe45fbfe8e9f7eb10,prp2nd,Ok-Message1053,https://www.reddit.com/r/dataengineering/comments/prp2nd/how_can_i_write_my_pipeline_to_execute_only/,how can I write my pipeline to execute only certain tasks?,APACHE_AIRFLOW,workflow management,"I know Airflow exists, but I'm not sure if that would work for me because I am only creating a VM and starting my container on command, so I don't have an environment for Airflow to constantly be running in",Airflow,8,15
e47e4876f198a58e334750a0dfa265f4,preqiw,m_usamahameed,https://www.reddit.com/r/dataengineering/comments/preqiw/cdc_implementation_in_airflow/,CDC implementation in Airflow.,APACHE_AIRFLOW,workflow management,How can we implement CDC in Airflow using Mysql or Python Operator,Airflow,29,36
9e8ef2ad957e683eaef3f885c668443c,pr6xg5,onelostsoul115,https://www.reddit.com/r/dataengineering/comments/pr6xg5/python_code_test_interview_general_career_advice/,Python Code Test (Interview) + general career advice,APACHE_AIRFLOW,workflow management,"Their tech stack is as follows:  SQL, Python, Clojure, Scala, AWS:Glue, Athena, Redshift, Apache Airflow, Kafka, Spark, Terraform  I'm a bit worried about the Python code test as my day to day and expertise is still mainly in pl/sql and pl/pgsql, so I need to brush up on my Python",Apache Airflow,91,105
4e9b3a9f3edb5f0e534301723e626e80,pqty5b,scraper01,https://www.reddit.com/r/dataengineering/comments/pqty5b/stream_processing_tool_to_pair_with_kafka/,Stream processing tool to pair with Kafka,APACHE_AIRFLOW,workflow management,"So far i've learnt Spark for big data ETL, a bit of Big Query for ELT, Apache Airflow for small data ETL,  and at last Kafka as a message passing queue",Apache Airflow,72,86
609387401a00e721110601e570f4d8bd,pqeipn,QuaternionHam,https://www.reddit.com/r/dataengineering/comments/pqeipn/standardbest_ways_of_moving_data_between_databases/,Standard/Best ways of moving data between Databases,APACHE_AIRFLOW,workflow management,All of this using Airflow as orchestrator,Airflow,19,26
f687ad7d0e28ed7a227fbdf06bf41588,pqeipn,QuaternionHam,https://www.reddit.com/r/dataengineering/comments/pqeipn/standardbest_ways_of_moving_data_between_databases/,Standard/Best ways of moving data between Databases,APACHE_AIRFLOW,workflow management, &amp;#x200B;  I thought of creating a custom python function that gets data from Postgre (using the correspondent airflow hook,airflow,116,123
36001bd7cf8f468b8969d807b8a0fb34,pphbrg,NotACardassian,https://www.reddit.com/r/dataengineering/comments/pphbrg/docker_environments_and_best_practices_in/,Docker environments and best practices in Industry question,APACHE_AIRFLOW,workflow management,I’m really struggling to wrap my head around industry best practice for an airflow workflow,airflow,76,83
280d97fc6051c8ab24c36c94f40dddc6,pphbrg,NotACardassian,https://www.reddit.com/r/dataengineering/comments/pphbrg/docker_environments_and_best_practices_in/,Docker environments and best practices in Industry question,APACHE_AIRFLOW,workflow management,  So say I have a docker container that runs airflow,airflow,46,53
111cbbc123987cc76771740c57fd1254,pphbrg,NotACardassian,https://www.reddit.com/r/dataengineering/comments/pphbrg/docker_environments_and_best_practices_in/,Docker environments and best practices in Industry question,APACHE_AIRFLOW,workflow management,Should any additional packages I might use in my airflow script be installed in the python virtual environment inside the container,airflow,50,57
2832c94c708bf83bd17630fbb7929569,pphbrg,NotACardassian,https://www.reddit.com/r/dataengineering/comments/pphbrg/docker_environments_and_best_practices_in/,Docker environments and best practices in Industry question,APACHE_AIRFLOW,workflow management,Or would I setup a second virtual environment inside or outside the docker container so airflow dependencies don’t mess with etl script dependencies where I may want something like the latest version of pandas,airflow,89,96
168252622ab6767802ae1308d52492c1,po8y8l,honorchan1,https://www.reddit.com/r/dataengineering/comments/po8y8l/airflows_best_kept_secrets_how_to_track_metadata/,Airflow's Best Kept Secrets: How To Track Metadata With Airflow Cluster Policies &amp; Task Callbacks,APACHE_AIRFLOW,workflow management,"Databand.ai Senior Software Engineer, Jonathan Barda, recently penned a [**tutorial on how to track metadata using Airflow's cluster policies and task callbacks**](https://databand.ai/blog/how-to-track-metadata-with-airflow-cluster-policies-task-callbacks/?utm_source=forum&amp;utm_medium=r&amp;utm_group=de",Airflow,116,123
254b90c8618e50c596934852769229f0,po6zh0,Nervous-Chain-5301,https://www.reddit.com/r/dataengineering/comments/po6zh0/aws_managed_airflow_or_aws_sam_for_simple_etls/,AWS Managed Airflow or AWS SAM for Simple ETLs,APACHE_AIRFLOW,workflow management, &amp;#x200B;  My initial thought was to use AWS managed Airflow...and write simple DAGs to load raw data to Snowflake,Airflow,58,65
b12685aedbec2070546be70d16a36baa,po6jpv,SingingNumbers,https://www.reddit.com/r/dataengineering/comments/po6jpv/how_do_i_improve_my_data_pipeline/,How do I improve my data pipeline?,APACHE_AIRFLOW,workflow management,* **Should I use Airflow?**    * Would Airflow be overkill if none of my Excel workbooks are bigger than 100MB ,Airflow,18,25
3b6c3df3535c6a1d69c69bfb2181ed16,po6jpv,SingingNumbers,https://www.reddit.com/r/dataengineering/comments/po6jpv/how_do_i_improve_my_data_pipeline/,How do I improve my data pipeline?,APACHE_AIRFLOW,workflow management,   * I can mentally keep track of my monthly process right now.......so do I need Airflow?,Airflow,83,90
f9e9d6bd105f688f516b6e87a12cfba1,po5kgw,marcosmarxm,https://www.reddit.com/r/dataengineering/comments/po5kgw/presentation_airbyte_airflow_on_gcp/,Presentation: Airbyte + Airflow on GCP,APACHE_AIRFLOW,workflow management, Tomorrow I'll go over how to set up Airbyte + Google Composer on GCP and send data from a Postgres Database to BigQuery! If you're curious about how to leverage Airbyte with Airflow this is your chance,Airflow,176,183
36e9db96d1659141e7d48f23840f0813,po5kgw,marcosmarxm,https://www.reddit.com/r/dataengineering/comments/po5kgw/presentation_airbyte_airflow_on_gcp/,Presentation: Airbyte + Airflow on GCP,APACHE_AIRFLOW,workflow management," I'll discuss when to use Airbyte and Airflow, deploy both in GCP (hope everything works",Airflow,39,46
6fa405415792ca07e76d6bc012263eec,pny53c,sgtbrecht,https://www.reddit.com/r/dataengineering/comments/pny53c/an_alternative_to_aws_solutions_architect_course/,An alternative to AWS Solutions Architect course on Udemy?,APACHE_AIRFLOW,workflow management,"I still want to learn stuff like Airflow, Kafka, advanced SQL, advanced DWH...etc but I'm not sure what to focus on at this point",Airflow,34,41
24722a16a8cd3ede3f6e7fcae9f4bda0,pnvfue,Ok-Message1053,https://www.reddit.com/r/dataengineering/comments/pnvfue/why_use_airflow/,why use airflow,APACHE_AIRFLOW,workflow management,"I hear about Airflow all the time so I finally decided to watch a couple tutorials on it, but I'm not really sure I understand why it's so popular",Airflow,14,21
78d92ff34b4749e8afef084d57f89449,pnvfue,Ok-Message1053,https://www.reddit.com/r/dataengineering/comments/pnvfue/why_use_airflow/,why use airflow,APACHE_AIRFLOW,workflow management,Can anyone give some thoughts on why Airflow might be preferable to my current method,Airflow,38,45
876dd78dcb7020358395e053ebef4629,pnvfue,Ok-Message1053,https://www.reddit.com/r/dataengineering/comments/pnvfue/why_use_airflow/,why use airflow,APACHE_AIRFLOW,workflow management,"I'm not sure if Airflow would be replacing these tools, or if it should be used in addition to what I'm using",Airflow,17,24
011244d5bc99e762a1852a7b1d3a7575,pnvfig,NotACardassian,https://www.reddit.com/r/dataengineering/comments/pnvfig/a_question_on_setting_up_airflow_docker_and_python/,"A question on setting up airflow, docker, and python",APACHE_AIRFLOW,workflow management,I decided to teach myself airflow and am trying to get a simple etl environment setup on digital ocean,airflow,27,34
aa483476a9c1a6b0ef28b151ca99267e,pnvfig,NotACardassian,https://www.reddit.com/r/dataengineering/comments/pnvfig/a_question_on_setting_up_airflow_docker_and_python/,"A question on setting up airflow, docker, and python",APACHE_AIRFLOW,workflow management,I have installed airflow with docker using the install instructions in their documentation,airflow,18,25
2e9a7e369e49d92404892dfe532f9233,pnvfig,NotACardassian,https://www.reddit.com/r/dataengineering/comments/pnvfig/a_question_on_setting_up_airflow_docker_and_python/,"A question on setting up airflow, docker, and python",APACHE_AIRFLOW,workflow management,Is there a more common practice for setting up your scripting environment and airflow environment,airflow,79,86
435ba12fe4f08763b0d385d165e96e26,pnmvkm,treacherous_tim,https://www.reddit.com/r/dataengineering/comments/pnmvkm/upskilling_in_api_service_development_as_a_data/,Upskilling in API / Service development as a data engineer,APACHE_AIRFLOW,workflow management,"I've worked for a while as a data engineer building out pipelines using Airflow, Spark, etc",Airflow,73,80
c6d8b69ed99a4f675cb56df3fbcc2f0f,pnd1wk,killer_unkill,https://www.reddit.com/r/dataengineering/comments/pnd1wk/de_interview_in_amazon_europe/,DE interview in Amazon Europe,APACHE_AIRFLOW,workflow management,and currently working with Airflow and Snowflake,Airflow,28,35
3d5f9fa24ba95a665915390bcaf65a5e,pn4c08,Playba1133,https://www.reddit.com/r/dataengineering/comments/pn4c08/currently_an_analyst_looking_to_pivot_to_an/,"Currently, an Analyst Looking to Pivot to an Engineer Position",APACHE_AIRFLOW,workflow management," Skills: Able to use SQL, Python, and R fairly well and have recently started a project at work that leverages Airflow",Airflow,112,119
e72e8dba3da8b0b410db9a5cea9a61f7,plulrs,IamWarmduscher,https://www.reddit.com/r/dataengineering/comments/plulrs/how_do_i_launch_the_airflow_ui_with_airflow_on_an/,How do I launch the Airflow UI with Airflow on an EC2?,APACHE_AIRFLOW,workflow management,I have Airflow on an EC2 instance and I launched the webserver and scheduler,Airflow,8,15
3ef63474be4bf7be518fc24ab5fca635,plmjrm,aCoolGuy12,https://www.reddit.com/r/dataengineering/comments/plmjrm/what_tools_can_replace_talend/,What tools can replace Talend?,APACHE_AIRFLOW,workflow management," I thought of Airflow+python scripts, but because there are not enough software engineers in the company, I am also required to think of an UI-based / more-friendly alternative to just python scripts",Airflow,15,22
707fa590e75d3dd92690b6f408683326,pli0og,MediumZealousideal29,https://www.reddit.com/r/dataengineering/comments/pli0og/skills_required/,Skills required,APACHE_AIRFLOW,workflow management,"I know SQL, hdfs, hive, control M..I also learned airflow, sqoop and basics of spark streaming, Kafka and AWS",airflow,51,58
c91ed1daa3827b7bdf2e97e7230c4d2f,plg9j3,RstarPhoneix,https://www.reddit.com/r/dataengineering/comments/plg9j3/how_to_master_apache_airflow/,How to master apache airflow?,APACHE_AIRFLOW,workflow management,I am interested in learning apache airflow,apache airflow,29,43
de042a63949001626d5ecc5cf0b94d5e,plg9j3,RstarPhoneix,https://www.reddit.com/r/dataengineering/comments/plg9j3/how_to_master_apache_airflow/,How to master apache airflow?,APACHE_AIRFLOW,workflow management,Can anyone recommend me any complete  tutorial series/YouTube  etc related to apache airflow,apache airflow,79,93
5a7e506c229f01b5ddcc45a121258e97,pl3pos,TheLoveBoat,https://www.reddit.com/r/dataengineering/comments/pl3pos/im_a_data_engineer_how_do_i_become_a_better/,I'm a Data Engineer. How do I become a better Software Engineer?,APACHE_AIRFLOW,workflow management,"I work with the modern data stack, so I'm comfortable building data pipelines using tools like Airflow and dbt",Airflow,96,103
0156127024c833f64bff3747a4718b40,pktcp3,noNSFWcontent,https://www.reddit.com/r/dataengineering/comments/pktcp3/can_someone_help_me_understand_the_difference/,Can someone help me understand the difference between the the docker-compose files?,APACHE_AIRFLOW,workflow management,So I was doing a few follow along tutorial for Apache Airflow with Docker,Apache Airflow,48,62
d880e1d3133903209b08043b9b96674a,pktcp3,noNSFWcontent,https://www.reddit.com/r/dataengineering/comments/pktcp3/can_someone_help_me_understand_the_difference/,Can someone help me understand the difference between the the docker-compose files?,APACHE_AIRFLOW,workflow management,  The main difference I faced between the first and the second tutorial is that the first docker compose file just fired up instantly and Airflow was up and running,Airflow,139,146
a6d8ad09924066a29c0b88eac1a17d21,pktcp3,noNSFWcontent,https://www.reddit.com/r/dataengineering/comments/pktcp3/can_someone_help_me_understand_the_difference/,Can someone help me understand the difference between the the docker-compose files?,APACHE_AIRFLOW,workflow management,"   **The first docker compose file is  -**           version: '3.9'               #asking for a postgres database     services:       postgres:         image: postgres:9.6         environment:           - POSTGRES_USER=airflow           - POSTGRES_PASSWORD=airflow           - POSTGRES_DB=airflow         logging:           options:             max-size: 10m             max-file: 3               #asking for a webserver       webserver:         build: ./dockerfiles         restart: always         depends_on:           - postgres         environment:           - LOAD_EX=n           - EXECUTOR=Local         logging:           options:             max-size: 10m             max-file: 3         volumes:           - ./dags:/usr/local/airflow/dags           # - ./plugins:/usr/local/airflow/plugins         ports:           - 8080:8080         command: webserver         healthcheck:           test: [CMD-SHELL, [ -f /usr/local/airflow/airflow-webserver.pid ]]           interval: 30s           timeout: 30s           retries: 3   For the above docker compose file, the dockerfiles folder had Dockerfile with the following text -       FROM puckel/docker-airflow          RUN pip install requests     RUN pip install pandas  **The second docker compose file is -**         version: '3'     services:       postgres:         image: postgres:9.6         environment:           - POSTGRES_USER=airflow           - POSTGRES_PASSWORD=airflow           - POSTGRES_DB=airflow         ports:           - 5432:5432            webserver:         image: puckel/docker-airflow:1.10.1         build:           context: https://github.com/puckel/docker-airflow.git#1.10.1           dockerfile: Dockerfile           args:             AIRFLOW_DEPS: gcp_api,s3             PYTHON_DEPS: sqlalchemy==1.2.0         restart: always         depends_on:           - postgres         environment:           - LOAD_EX=n           - EXECUTOR=Local           - FERNET_KEY=jsDPRErfv8Z_eVTnGfF8ywd19j4pyqE3NpdUBA_oRTo=         volumes:           - ./examples/intro-example/dags:/usr/local/airflow/dags           # Uncomment to include custom plugins           # - ./plugins:/usr/local/airflow/plugins         ports:           - 8080:8080         command: webserver         healthcheck:           test: [CMD-SHELL, [ -f /usr/local/airflow/airflow-webserver.pid ]]           interval: 30s           timeout: 30s           retries: 3       When the second docker compose was stuck in a dependency hell, I noticed both were pulling the same Airflow puckel image",airflow,220,227
668ecc35c9c12fbb9601dd2184f394cc,pkmggc,datanoob2021,https://www.reddit.com/r/dataengineering/comments/pkmggc/streaming_pipeline_question/,Streaming Pipeline Question,APACHE_AIRFLOW,workflow management,I have built batched pipelines in the past and the invoking of the process was usually triggered by AirFlow or ECS,AirFlow,101,108
bbdfdd56620b07882bcf38405521a594,pkmggc,datanoob2021,https://www.reddit.com/r/dataengineering/comments/pkmggc/streaming_pipeline_question/,Streaming Pipeline Question,APACHE_AIRFLOW,workflow management,Do I just have Airflow constantly run it over and over,Airflow,16,23
c5246aeeb912518a9f64021b12d1e703,pk78fp,ClumsyRooster,https://www.reddit.com/r/dataengineering/comments/pk78fp/airflow_spark_other_tool/,"Airflow, Spark, other tool ?",APACHE_AIRFLOW,workflow management,"  What I did first, is simply use Apache Airflow v2 because it looked like I could orchestrate some scripts (I mean DAGs, but they looks like scripts to me, I'm probably doing something off",Apache Airflow,35,49
a7e3196504bb96151c30f13744406db2,pk78fp,ClumsyRooster,https://www.reddit.com/r/dataengineering/comments/pk78fp/airflow_spark_other_tool/,"Airflow, Spark, other tool ?",APACHE_AIRFLOW,workflow management,"After more research, it looks like using only Airflow for ETL is wrong and I shouldn't do that (why though ",Airflow,47,54
e316d8b844ea6d79bf924287a0b54372,pk78fp,ClumsyRooster,https://www.reddit.com/r/dataengineering/comments/pk78fp/airflow_spark_other_tool/,"Airflow, Spark, other tool ?",APACHE_AIRFLOW,workflow management,So I took the time to use Airflow + Spark,Airflow,27,34
b6a6ba18f64746916f4bab2bbccbab77,pk78fp,ClumsyRooster,https://www.reddit.com/r/dataengineering/comments/pk78fp/airflow_spark_other_tool/,"Airflow, Spark, other tool ?",APACHE_AIRFLOW,workflow management,Is it wrong to only use Airflow (2.1.3,Airflow,25,32
c4c7752ab9699beba10d3bafe5076849,pk78fp,ClumsyRooster,https://www.reddit.com/r/dataengineering/comments/pk78fp/airflow_spark_other_tool/,"Airflow, Spark, other tool ?",APACHE_AIRFLOW,workflow management, What's the advantage of using Spark instead of Airflow for small processing ,Airflow,49,56
6b6585376de80fd6b740b0471eabe4b5,pjtz2s,stackedhats,https://www.reddit.com/r/dataengineering/comments/pjtz2s/will_i_regret_it_if_i_start_using_informatica/,Will I regret it if I start using Informatica?,APACHE_AIRFLOW,workflow management, I would have to maintain Airflow/Luigi by myself and have zero experience with them so I don't really want to go that route right now,Airflow,27,34
2998b9fab1efa8a5a265c330dfacde99,pgtfxn,Vorskl,https://www.reddit.com/r/dataengineering/comments/pgtfxn/any_deds_udemy_courses_worth_stocking/,Any DE/DS Udemy courses worth stocking?,APACHE_AIRFLOW,workflow management," I'm eyeing Airflow,  BigQuery DE  and Lazy Programmer DS courses.",Airflow,13,20
dbd710c21ec03b20cfe82c6270099922,pggz3t,pknpkn21,https://www.reddit.com/r/dataengineering/comments/pggz3t/cicd_for_de_pipeline/,CI/CD for DE Pipeline,APACHE_AIRFLOW,workflow management,"Hi,  We are currently in the process of setting up a data processing pipeline for few of our data sources and the processing logic is designed in Pyspark with Airflow for orchestration in AWS environment",Airflow,160,167
222853c2a650d00f3090c6f6df05ca33,pg19m4,YellowPride95,https://www.reddit.com/r/dataengineering/comments/pg19m4/have_been_promoted_to_a_position_at_my_work_where/,"Have been promoted to a position at my work where I will be establishing the first official data warehouse for the company (&lt;50 People). No prior experience, but my boss and management trust me I can do it. Still in undergrad part time for mathematics, want to be quant in future. How do I do it?",APACHE_AIRFLOW,workflow management,Utilize Apache Airflow  3,Apache Airflow,9,23
9ce7719a39e553cf7dbdbd23fc25440b,pftkri,Fatal_Conceit,https://www.reddit.com/r/dataengineering/comments/pftkri/as_a_new_de_1yoe_how_should_i_approach_the_job/,"As a new DE &lt; 1YOE, how should I approach the job market to get good training?",APACHE_AIRFLOW,workflow management,We don’t have access to tools like airflow and the basic tech stacks needed for the role,airflow,36,43
64edb31d5c4f6fd068c86b252ee6ec8f,pfhxyi,IllustriousBalance50,https://www.reddit.com/r/dataengineering/comments/pfhxyi/airflow_and_macbook_m1/,Airflow and Macbook M1,APACHE_AIRFLOW,workflow management,I'm trying to learn airflow but I'm having a lot of problems and getting build errors  with the tutorials I'm following,airflow,21,28
c85b7db6eaf96d17222f5407f7957d71,pfemin,Zorkol02,https://www.reddit.com/r/dataengineering/comments/pfemin/cloud_certification_help/,Cloud Certification Help,APACHE_AIRFLOW,workflow management,"I know python, sql, bash, pyspark and airflow, but not an expert yet just in case",airflow,39,46
b67b3063c19c251882de02067db70f4d,pekrif,hairbear1234,https://www.reddit.com/r/dataengineering/comments/pekrif/de_contractor_at_faang_advice_for_quitting_taking/,"DE contractor at FAANG - Advice for quitting, taking some time off, and then jumping back in.",APACHE_AIRFLOW,workflow management,"I'm 4 months in and they want to extend my contract and have me interview for FT.   I have realized that Im not super excited for the work - SQL shop vs my last job of building cloud infra, airflow, python",airflow,191,198
0ed70efa005d18d70ac145638ad63141,pceu26,skpfm_06,https://www.reddit.com/r/dataengineering/comments/pceu26/been_working_in_a_small_startup_for_almost_2/,"Been working in a small startup for almost 2 years doing pretty much everything on the engineering side, what should I focus on and pursue moving forward?",APACHE_AIRFLOW,workflow management,"Because I was doing most of these solo for so long, I haven't had the chance to delve deeper into a subset of these tasks, so I am not familiar with tools like Luigi, Airflow, Kubernetes, Deeper understanding of AWS services, writing python like they do in Open Source libraries, Hadoop, Spark, etc",Airflow,168,175
be9c9dcead22c4cf8e000804893f2e62,pc6jwg,eddygaras,https://www.reddit.com/r/dataengineering/comments/pc6jwg/moving_away_from_ssis/,Moving away from SSIS,APACHE_AIRFLOW,workflow management,Or using python and frameworks like Airflow and Spark or some others,Airflow,37,44
bf826077ea6fdc03a4cbf0c4be71a687,ppuh5c,hatchikyu,https://www.reddit.com/r/dataengineering/comments/ppuh5c/if_i_had_to_explain_data_engineering_work_to_a/,If I had to explain data engineering work to a business colleague...,ARGO,workflow management," **Supports the following kind of work (that business people understand):**  * Machine learning * Data Science * Deep Learning  **Most common jargon**  ETL  - extract-transform-load — tooling to move data from place to place  **Scope of work**  * Enormous amounts of data means heavy lifting around **scalability** * Responsible for deciding what data is useful and what is superfluous * Today’s data engineers are essentially specialist software engineers  **Capability areas**  * SDLC best practice - version, control, release management, automated pipelines, implementing open-source tools like Spark and Tensorflow * InfoSec - cloud security best practice, data handling, data privacy, GDPR, OSS security etc",argo,144,148
05d73f7e2f56a2284f4346758a631ce0,prp2nd,Ok-Message1053,https://www.reddit.com/r/dataengineering/comments/prp2nd/how_can_i_write_my_pipeline_to_execute_only/,how can I write my pipeline to execute only certain tasks?,DOCKER,containerization,"I currently have a python script, [main.py](https://main.py), as my entry point to my docker container",docker,87,93
5d722650bd5550d3665f00a7f2707ddd,prp2nd,Ok-Message1053,https://www.reddit.com/r/dataengineering/comments/prp2nd/how_can_i_write_my_pipeline_to_execute_only/,how can I write my pipeline to execute only certain tasks?,DOCKER,containerization,I'm sure there is a better way to design my pipeline so that I can manually run my docker container and input something to say 'only run task x this time',docker,84,90
08e3305319f8ebc0aa23edec106a0f36,pqadz5,DrSnakee95,https://www.reddit.com/r/dataengineering/comments/pqadz5/interview_assignment_too_big/,Interview assignment too big ?,DOCKER,containerization,- Include testing and error handling - Contenerize the code I have written in a docker containter  This feels a bit overboard doesn't it ?,docker,81,87
d8edec4ea2c1841795d13285fc20db4c,pphbrg,NotACardassian,https://www.reddit.com/r/dataengineering/comments/pphbrg/docker_environments_and_best_practices_in/,Docker environments and best practices in Industry question,DOCKER,containerization,  So say I have a docker container that runs airflow,docker,19,25
945152455ef59e11265611e0d15256d3,pphbrg,NotACardassian,https://www.reddit.com/r/dataengineering/comments/pphbrg/docker_environments_and_best_practices_in/,Docker environments and best practices in Industry question,DOCKER,containerization,  Is the docker container my “whole” environment or just my scheduler,docker,10,16
f2d4a2ff7e2ebbac98544b5a315878cd,pphbrg,NotACardassian,https://www.reddit.com/r/dataengineering/comments/pphbrg/docker_environments_and_best_practices_in/,Docker environments and best practices in Industry question,DOCKER,containerization,Or would I setup a second virtual environment inside or outside the docker container so airflow dependencies don’t mess with etl script dependencies where I may want something like the latest version of pandas,docker,69,75
0d6eea81d8cc93d6a313fb6209a46efe,pphbrg,NotACardassian,https://www.reddit.com/r/dataengineering/comments/pphbrg/docker_environments_and_best_practices_in/,Docker environments and best practices in Industry question,DOCKER,containerization,"I was thinking a second docker container running a dash server, again, so dependencies don’t get mixed up but is this the correct approach",docker,25,31
f95e6eea6c5d4e60b8f57f62e6f6a534,pnvfue,Ok-Message1053,https://www.reddit.com/r/dataengineering/comments/pnvfue/why_use_airflow/,why use airflow,DOCKER,containerization,"I currently use Google Scheduler to trigger a GH workflow, which starts my Docker container on GCE, which I really like",Docker,76,82
15daec97de77b8d450d24afc926c14a9,pnvfig,NotACardassian,https://www.reddit.com/r/dataengineering/comments/pnvfig/a_question_on_setting_up_airflow_docker_and_python/,"A question on setting up airflow, docker, and python",DOCKER,containerization,I have installed airflow with docker using the install instructions in their documentation,docker,31,37
8a22781f8790ce70367925ddd343827c,pnvfig,NotACardassian,https://www.reddit.com/r/dataengineering/comments/pnvfig/a_question_on_setting_up_airflow_docker_and_python/,"A question on setting up airflow, docker, and python",DOCKER,containerization,Then I created a new venv so that I don’t mess with the docker environment with what would be my etl scripts,docker,57,63
3a3cba0567b6c6e7681b07b151f3c851,pnvfig,NotACardassian,https://www.reddit.com/r/dataengineering/comments/pnvfig/a_question_on_setting_up_airflow_docker_and_python/,"A question on setting up airflow, docker, and python",DOCKER,containerization,Should I be doing everything in the docker venv?,docker,37,43
687664e0dec6c4721ecd0b8a08e405a4,pmafnw,ezio20,https://www.reddit.com/r/dataengineering/comments/pmafnw/building_data_pipelines_using_docker_and_skaffold/,Building data pipelines using Docker and Skaffold,DOCKER,containerization,"Hi Guys, could you please suggest any resource / blog / Youtube video/ book that can give a simple tutorial in building data pipelines using Docker ans Skaffold?",Docker,142,148
2b6eecc7c8a139792223e203074b2da4,pktcp3,noNSFWcontent,https://www.reddit.com/r/dataengineering/comments/pktcp3/can_someone_help_me_understand_the_difference/,Can someone help me understand the difference between the the docker-compose files?,DOCKER,containerization,So I was doing a few follow along tutorial for Apache Airflow with Docker,Docker,68,74
f13b4dccd940ddf05baf9f3413a7aee0,pktcp3,noNSFWcontent,https://www.reddit.com/r/dataengineering/comments/pktcp3/can_someone_help_me_understand_the_difference/,Can someone help me understand the difference between the the docker-compose files?,DOCKER,containerization,  The main difference I faced between the first and the second tutorial is that the first docker compose file just fired up instantly and Airflow was up and running,docker,91,97
051b3531f102099c26645e74cf95688a,pktcp3,noNSFWcontent,https://www.reddit.com/r/dataengineering/comments/pktcp3/can_someone_help_me_understand_the_difference/,Can someone help me understand the difference between the the docker-compose files?,DOCKER,containerization,  The second docker compose file was stuck installing tons and tons of dependencies which never seemed to end,docker,14,20
b62b5773ab0479fddf94e9c598336fb4,pktcp3,noNSFWcontent,https://www.reddit.com/r/dataengineering/comments/pktcp3/can_someone_help_me_understand_the_difference/,Can someone help me understand the difference between the the docker-compose files?,DOCKER,containerization,"   **The first docker compose file is  -**           version: '3.9'               #asking for a postgres database     services:       postgres:         image: postgres:9.6         environment:           - POSTGRES_USER=airflow           - POSTGRES_PASSWORD=airflow           - POSTGRES_DB=airflow         logging:           options:             max-size: 10m             max-file: 3               #asking for a webserver       webserver:         build: ./dockerfiles         restart: always         depends_on:           - postgres         environment:           - LOAD_EX=n           - EXECUTOR=Local         logging:           options:             max-size: 10m             max-file: 3         volumes:           - ./dags:/usr/local/airflow/dags           # - ./plugins:/usr/local/airflow/plugins         ports:           - 8080:8080         command: webserver         healthcheck:           test: [CMD-SHELL, [ -f /usr/local/airflow/airflow-webserver.pid ]]           interval: 30s           timeout: 30s           retries: 3   For the above docker compose file, the dockerfiles folder had Dockerfile with the following text -       FROM puckel/docker-airflow          RUN pip install requests     RUN pip install pandas  **The second docker compose file is -**         version: '3'     services:       postgres:         image: postgres:9.6         environment:           - POSTGRES_USER=airflow           - POSTGRES_PASSWORD=airflow           - POSTGRES_DB=airflow         ports:           - 5432:5432            webserver:         image: puckel/docker-airflow:1.10.1         build:           context: https://github.com/puckel/docker-airflow.git#1.10.1           dockerfile: Dockerfile           args:             AIRFLOW_DEPS: gcp_api,s3             PYTHON_DEPS: sqlalchemy==1.2.0         restart: always         depends_on:           - postgres         environment:           - LOAD_EX=n           - EXECUTOR=Local           - FERNET_KEY=jsDPRErfv8Z_eVTnGfF8ywd19j4pyqE3NpdUBA_oRTo=         volumes:           - ./examples/intro-example/dags:/usr/local/airflow/dags           # Uncomment to include custom plugins           # - ./plugins:/usr/local/airflow/plugins         ports:           - 8080:8080         command: webserver         healthcheck:           test: [CMD-SHELL, [ -f /usr/local/airflow/airflow-webserver.pid ]]           interval: 30s           timeout: 30s           retries: 3       When the second docker compose was stuck in a dependency hell, I noticed both were pulling the same Airflow puckel image",docker,16,22
6c8d85fb0ef4762351aae4397c39c828,pktcp3,noNSFWcontent,https://www.reddit.com/r/dataengineering/comments/pktcp3/can_someone_help_me_understand_the_difference/,Can someone help me understand the difference between the the docker-compose files?,DOCKER,containerization,"  So, I tweaked the second docker compose to looked like the first one and it first right up",docker,28,34
9ca9a7005598c2994c7ef796d0b3acb2,pj99xo,noNSFWcontent,https://www.reddit.com/r/dataengineering/comments/pj99xo/please_help_me_understand_how_to_go_about_working/,Please help me understand how to go about working on a personal project.,DOCKER,containerization,             then display the prices on the front end    I am also in the process of looking for data science jobs so want to use tools like docker and a noSQL database like Mongodb to add to my resume,docker,142,148
72e1b928c4e111c54de6b51c0cfbde6c,pj99xo,noNSFWcontent,https://www.reddit.com/r/dataengineering/comments/pj99xo/please_help_me_understand_how_to_go_about_working/,Please help me understand how to go about working on a personal project.,DOCKER,containerization,  I know there are different backend server containers like FastAPI available in docker as well as noSQL databases like Mongodb,docker,82,88
565bab7715276abc707d251438d306f7,piu290,raghukveer,https://www.reddit.com/r/dataengineering/comments/piu290/is_it_mandatory_to_work_as_a_dbadata_analyst_to/,Is it mandatory to work as a DBA/Data Analyst to become a DE?,DOCKER,containerization," In past, I worked as an IT consultant for 2 small companies with Docker, Kubernetes, Linux, but have no experience with data-related works",Docker,67,73
586e20b1ba6a88e077adc98a62f6fa88,pcrlzi,Professional_Crazy49,https://www.reddit.com/r/dataengineering/comments/pcrlzi/confused_about_de_vs_sde/,Confused about DE vs SDE,DOCKER,containerization," The Confusion:  So one of my friends works as a software engineer 1 and he has worked with kafka, docker, aws, prometheus, spark",docker,100,106
fdca76c925bfb3e61f23401b80c5a904,pceu26,skpfm_06,https://www.reddit.com/r/dataengineering/comments/pceu26/been_working_in_a_small_startup_for_almost_2/,"Been working in a small startup for almost 2 years doing pretty much everything on the engineering side, what should I focus on and pursue moving forward?",DOCKER,containerization,Dockerise stuff when needed 9,Docker,1,7
10da833d9c7fdd4456576ba23744b63f,pli6w5,Material-Bridge,https://www.reddit.com/r/dataengineering/comments/pli6w5/creating_an_end_to_end_object_detection_pipeline/,Creating an end to end Object detection Pipeline,APACHE_CASSANDRA,datastores,  I was thinking about using a NOSQL db (Cassandra,Cassandra,42,51
f8f138fa31b8af022cf0d78c67ee8b6a,ph4fvj,WalrusWhich202,https://www.reddit.com/r/dataengineering/comments/ph4fvj/how_to_querying_data_faster/,How to Querying data faster,APACHE_CASSANDRA,datastores," I've looked into spark, hadoop, cassandra, hdfs, and others",cassandra,34,43
02660757dcf8c28faeea0b4944e72e71,pe9f42,TiDuNguyen,https://www.reddit.com/r/dataengineering/comments/pe9f42/purely_columnar_database_suggestion/,Purely columnar database suggestion,APACHE_CASSANDRA,datastores,or not *purely* columnar (e.g.: Cassandra actually store data in partitions of rows and columns rather than actual columns).,Cassandra,33,42
608c99783d495dce993cd3caca1b6789,pt90kg,ArunMu,https://www.reddit.com/r/dataengineering/comments/pt90kg/clickhouse_and_apache_pinot/,ClickHouse and Apache Pinot,CLICKHOUSE,datastores,I had posted my findings about both ClickHouse and Apache Pinot at [https://www.reddit.com/r/bigdata/comments/pse4gb/clickhouse\_and\_apache\_pinot/](https://www.reddit.com/r/bigdata/comments/pse4gb/clickhouse_and_apache_pinot/,ClickHouse,37,47
9efcc79631b9cacf0416b91236a026e6,pqeipn,QuaternionHam,https://www.reddit.com/r/dataengineering/comments/pqeipn/standardbest_ways_of_moving_data_between_databases/,Standard/Best ways of moving data between Databases,CLICKHOUSE,datastores,"(for example, Postgre to Clickhouse",Clickhouse,26,36
f3c316faf982279b45d566d4d56000e7,pqeipn,QuaternionHam,https://www.reddit.com/r/dataengineering/comments/pqeipn/standardbest_ways_of_moving_data_between_databases/,Standard/Best ways of moving data between Databases,CLICKHOUSE,datastores," &amp;#x200B;  We are currently just dumping data from Postgre to a CSV then copying that file using scp to the host where our Clickhouse instance is running (everything on-premises), and finally inserting from that file into our DW",Clickhouse,128,138
3205aa4629cb3ae3dd6e38ef7ed9ec05,pqeipn,QuaternionHam,https://www.reddit.com/r/dataengineering/comments/pqeipn/standardbest_ways_of_moving_data_between_databases/,Standard/Best ways of moving data between Databases,CLICKHOUSE,datastores,and then loading them into our DW using ClickHouse Hook,ClickHouse,41,51
cf0fdc94410b7dd0ded44032aececd54,pt88mq,vektor888,https://www.reddit.com/r/dataengineering/comments/pt88mq/any_des_working_in_a_company_implementing_the/,Any DEs working in a company implementing the Spotify model?,APACHE_ATLAS,data governance and registries,"Hi,   as the title might suggest, I've been working in a company implementing some flavour of the [Spotify model](https://www.atlassian.com/agile/agile-at-scale/spotify",atlas,127,132
f937b720497c1bff7947a9494158b298,pq9f8b,noNSFWcontent,https://www.reddit.com/r/dataengineering/comments/pq9f8b/i_just_had_an_interview_with_a_senior_data/,"I just had an interview with a Senior Data Engineer and the head of the Data Science department of a company and they said, in the next interview they'd see how I think.",KINESIS,data ingestion,"We had a good conversation about my work experience which was not quite of a data engineer and my personal experience working with AWS tools such as EC2, Kinesis, Lambda, DynamoDB and S3",Kinesis,155,162
96d43795b403a7cfa38bf2bda072a542,pl9n2x,DatKalvin,https://www.reddit.com/r/dataengineering/comments/pl9n2x/is_this_architecture_cost_effective_performant/,Is this Architecture cost effective &amp; performant? What's your suggestion.,KINESIS,data ingestion,We are also thinking of Kinesis Firehose replacing DMS but haven't figured out the possibility of this,Kinesis,25,32
c89af29128ba5f50ab1eaca5b7b05d4d,pli0og,MediumZealousideal29,https://www.reddit.com/r/dataengineering/comments/pli0og/skills_required/,Skills required,APACHE_SQOOP,data ingestion,"I know SQL, hdfs, hive, control M..I also learned airflow, sqoop and basics of spark streaming, Kafka and AWS",sqoop,60,65
8cc686ceaab6c0f1b0cb69dca0bde284,peieca,Ok-Sentence-8542,https://www.reddit.com/r/dataengineering/comments/peieca/how_do_you_guys_validate_your_data_how_does_your/,"How do you guys validate your data? How does your process look like and which tools are you using? Our main Stack: Databricks, Azure Datafactory, Data Lake",GREAT_EXPECTATIONS,testing,"Could we use databricks + Great Expectations + some Error Handling Logic (Logic App, etc",Great Expectations,27,45
b0173ed612c3bfceba3014705931f905,ptazl0,The_small_print,https://www.reddit.com/r/dataengineering/comments/ptazl0/corise_learning_platform_dbt_analytics/,Co:Rise Learning Platform &amp; dbt Analytics Engineering Course,DBT,data transformation, They recently announced an Analytics Engineering course that looks to use dbt for various projects over a few weeks,dbt,76,79
ec9ce319fe226418d1429a40e7290273,ptazl0,The_small_print,https://www.reddit.com/r/dataengineering/comments/ptazl0/corise_learning_platform_dbt_analytics/,Co:Rise Learning Platform &amp; dbt Analytics Engineering Course,DBT,data transformation, [Here's a link to the course in quesiton](https://corise.com/course/analytics-engineering-with-dbt?utm_source=emilyh),dbt,97,100
53dba0975047f7e47193cd55a33b9b93,pssfyi,sb2nov,https://www.reddit.com/r/dataengineering/comments/pssfyi/analytics_engineering_with_dbt_course_looking_for/,Analytics engineering with dbt Course - Looking for feedback,DBT,data transformation,"Emily Hawkins, Data Engineering Lead at Drizly/Uber and I are leading a live, cohort based course on dbt starting November 15th",dbt,102,105
d8bcd44b144f3908827c7aabbe9c7564,pssfyi,sb2nov,https://www.reddit.com/r/dataengineering/comments/pssfyi/analytics_engineering_with_dbt_course_looking_for/,Analytics engineering with dbt Course - Looking for feedback,DBT,data transformation,[https://corise.com/course/analytics-engineering-with-dbt](https://corise.com/course/analytics-engineering-with-dbt,dbt,55,58
baf94bb0fbc35877f7c1dd13c50bea38,pskxx5,mdl003,https://www.reddit.com/r/dataengineering/comments/pskxx5/best_tools_for_metadata_capture_indexing_for/,Best tools for metadata capture + indexing for delta/parquet tables?,DBT,data transformation,and dbt but doesn't look like either fits our use case very well on first read,dbt,5,8
57a68232d9d569a6ee92c4c6ed66b1be,ps702d,third_dude,https://www.reddit.com/r/dataengineering/comments/ps702d/should_you_flatten_json_from_apis_as_you_are/,Should you flatten json from apis as you are loading it into staging tables or afterwards with dbt?,DBT,data transformation,  If we are loading api results into snowflake for instance or bigquery is it better to just dump the thing in that nested structure and dbt everything out of it later,dbt,138,141
6f3feab860fa2fb772bf91e4d5d66b55,poqthi,Material_Cheetah934,https://www.reddit.com/r/dataengineering/comments/poqthi/documenting_by_parsing_sql_into_diagrams/,Documenting by parsing SQL into diagrams?,DBT,data transformation,"I have heard of DBT, but I have constraints that will prevent me from using it, either that or my understanding is limited",DBT,17,20
082e601709dc51e1f1ea04edc3604517,pomqcg,dadadawe,https://www.reddit.com/r/dataengineering/comments/pomqcg/dbt_looker_whats_it_all_about/,"DBT, Looker, ... what's it all about?",DBT,data transformation,but now I see more and more references to DBT &amp; Looker,DBT,43,46
d23a02194c5c93afa354fb01bed67b59,poimtb,brownstrom,https://www.reddit.com/r/dataengineering/comments/poimtb/data_profiling_reviews_on_dbtprofiler_or_any/,Data Profiling: Reviews on dbt-profiler or any similar tool?,DBT,data transformation,I came across **dbt**\-**profile** and also **pandas profiler**,dbt,17,20
41db9d07ebf15c054b161f44eed2dd99,poimtb,brownstrom,https://www.reddit.com/r/dataengineering/comments/poimtb/data_profiling_reviews_on_dbtprofiler_or_any/,Data Profiling: Reviews on dbt-profiler or any similar tool?,DBT,data transformation,  dbt-profiler is still in beta and kind off new to the market,dbt,3,6
0138279b85e4bfcde1f7a97312e8371f,po6zh0,Nervous-Chain-5301,https://www.reddit.com/r/dataengineering/comments/po6zh0/aws_managed_airflow_or_aws_sam_for_simple_etls/,AWS Managed Airflow or AWS SAM for Simple ETLs,DBT,data transformation,My company's core data stack is Fivetran + Snowflake + DBT + Looker,DBT,56,59
53f54e92c3bb94b606b19d22f0e7ea5d,po6zh0,Nervous-Chain-5301,https://www.reddit.com/r/dataengineering/comments/po6zh0/aws_managed_airflow_or_aws_sam_for_simple_etls/,AWS Managed Airflow or AWS SAM for Simple ETLs,DBT,data transformation,"Fivetran is our data loading tool, we load raw data into Snowflake, and process using DBT",DBT,87,90
d4cef9c41838912821817c64b320871a,pnixoi,CreativeAd6756,https://www.reddit.com/r/dataengineering/comments/pnixoi/soda_sql_vs_dbt_tests/,Soda SQL vs DBT tests,DBT,data transformation,We are already using dbt in our project so should we go with DBT tests,dbt,22,25
5212b99013a4d8f3098f67d59d8ca531,pnixoi,CreativeAd6756,https://www.reddit.com/r/dataengineering/comments/pnixoi/soda_sql_vs_dbt_tests/,Soda SQL vs DBT tests,DBT,data transformation, Or Soda sql have more advantages over DBT tests,DBT,40,43
4eb970d3f9bca5c834d3eca4d9d32542,pl3pos,TheLoveBoat,https://www.reddit.com/r/dataengineering/comments/pl3pos/im_a_data_engineer_how_do_i_become_a_better/,I'm a Data Engineer. How do I become a better Software Engineer?,DBT,data transformation,"I work with the modern data stack, so I'm comfortable building data pipelines using tools like Airflow and dbt",dbt,108,111
3423171da64d152c0ad89ce7fa76f383,pgv4vw,el_jeep0,https://www.reddit.com/r/dataengineering/comments/pgv4vw/considering_taking_analytics_engineering_role_for/,"Considering taking Analytics Engineering role for a while, can I go back to Data Engineering after, will it be harder?",DBT,data transformation,"customer-facing, it will deepen my SQL/DBT skills and allow me to maybe pick up some looker",DBT,40,43
55db54d869da9b8f7099f05aa77536d7,pfigsg,QuaternionHam,https://www.reddit.com/r/dataengineering/comments/pfigsg/freelance_de/,Freelance DE,DBT,data transformation,I have the opportunity to make some freelance DE jobs (dbt and cloud technologies mostly,dbt,56,59
36e97a460278ab8d8b8a33eb0dbaec8c,pcokrh,dathu9,https://www.reddit.com/r/dataengineering/comments/pcokrh/is_dbt_can_be_used_at_the_enterprise_level/,is dbt can be used at the enterprise level?,DBT,data transformation,I just start evaluating the dbt (data build tool,dbt,29,32
fe5d8c8c8645d1bec6416c840fc9a4ef,pcokrh,dathu9,https://www.reddit.com/r/dataengineering/comments/pcokrh/is_dbt_can_be_used_at_the_enterprise_level/,is dbt can be used at the enterprise level?,DBT,data transformation,product for our analytics team to perform analytical transformations with dbt,dbt,75,78
498589d09dc7d19a535477709cb542d9,pcokrh,dathu9,https://www.reddit.com/r/dataengineering/comments/pcokrh/is_dbt_can_be_used_at_the_enterprise_level/,is dbt can be used at the enterprise level?,DBT,data transformation,Can some please comment is below items achievable with dbt,dbt,56,59
f1989974cf757abc86105b787c5f53e3,pcokrh,dathu9,https://www.reddit.com/r/dataengineering/comments/pcokrh/is_dbt_can_be_used_at_the_enterprise_level/,is dbt can be used at the enterprise level?,DBT,data transformation,Can dbt support multi db connections in one model query,dbt,5,8
8f461bb5fc722a30efce7da13107f29e,pcokrh,dathu9,https://www.reddit.com/r/dataengineering/comments/pcokrh/is_dbt_can_be_used_at_the_enterprise_level/,is dbt can be used at the enterprise level?,DBT,data transformation,Can dbt read one snowflake and save the results into postgre,dbt,5,8
d89e46030c141e62b2ceba6cfb159e4c,pcokrh,dathu9,https://www.reddit.com/r/dataengineering/comments/pcokrh/is_dbt_can_be_used_at_the_enterprise_level/,is dbt can be used at the enterprise level?,DBT,data transformation,Can dbt read the data from s3 without EMR cluster (Spark cluster,dbt,5,8
bf739d67b7de6f1d62351cd74dd8847b,pcokrh,dathu9,https://www.reddit.com/r/dataengineering/comments/pcokrh/is_dbt_can_be_used_at_the_enterprise_level/,is dbt can be used at the enterprise level?,DBT,data transformation,Can dbt store the output to s3 or onedrive storage?,dbt,5,8
3d6a1cf7fe73413cfb8a59e6ff1d5730,pondby,nezamolmolk,https://www.reddit.com/r/dataengineering/comments/pondby/hierarchical_treemap_from_scratch_with_no/,Hierarchical Treemap from scratch with no aggregation,APACHE_DRILL,data virtualization, The idea is therefore to create a Treemap visualization with drill down / up functionality to get an idea of ​​the value of each level / each sub level,drill,63,68
0ea59d76c9265dc711f42d2dcc4ecaaf,ptdg0g,diegoelmestre,https://www.reddit.com/r/dataengineering/comments/ptdg0g/improvements_to_my_first_de_project_streaming/,Improvements to my first DE (?) project (Streaming Datawarehouse). What's next?,APACHE_KAFKA,data ingestion," We adopted the Apache Kafka ecosystem and Bigquery for DWH, MongoDB is our operational database",Apache Kafka,17,29
458eb55a53353688c048d3e7f36c8997,ptdg0g,diegoelmestre,https://www.reddit.com/r/dataengineering/comments/ptdg0g/improvements_to_my_first_de_project_streaming/,Improvements to my first DE (?) project (Streaming Datawarehouse). What's next?,APACHE_KAFKA,data ingestion,    1 - We have a Kafka Connect running that grabs the changes from our MongoDB collections,Kafka,19,24
3bfd6c879175e8b802decd5cc157cf77,ptdg0g,diegoelmestre,https://www.reddit.com/r/dataengineering/comments/ptdg0g/improvements_to_my_first_de_project_streaming/,Improvements to my first DE (?) project (Streaming Datawarehouse). What's next?,APACHE_KAFKA,data ingestion, 2 - The connector stores the data in a Kafka topic  3 - We are also running ksqlDB to make most of the transformations and a few aggregations,Kafka,41,46
6ed967d881d32e9f20f89016ec76c053,ptdg0g,diegoelmestre,https://www.reddit.com/r/dataengineering/comments/ptdg0g/improvements_to_my_first_de_project_streaming/,Improvements to my first DE (?) project (Streaming Datawarehouse). What's next?,APACHE_KAFKA,data ingestion,  4 - Transformed data is sent to the new Kafka Topic  \-- My biggest uncertainty about all systems starts here,Kafka,43,48
8eda24ffb994faecd73238cfdcb6f4d7,pskxx5,mdl003,https://www.reddit.com/r/dataengineering/comments/pskxx5/best_tools_for_metadata_capture_indexing_for/,Best tools for metadata capture + indexing for delta/parquet tables?,APACHE_KAFKA,data ingestion,Raw data is passed by the application to various Kafka topics where it's streamed into s3 buckets,Kafka,50,55
af451e7e3be97f0b66577fb05cf45a29,pr6xg5,onelostsoul115,https://www.reddit.com/r/dataengineering/comments/pr6xg5/python_code_test_interview_general_career_advice/,Python Code Test (Interview) + general career advice,APACHE_KAFKA,data ingestion,"Their tech stack is as follows:  SQL, Python, Clojure, Scala, AWS:Glue, Athena, Redshift, Apache Airflow, Kafka, Spark, Terraform  I'm a bit worried about the Python code test as my day to day and expertise is still mainly in pl/sql and pl/pgsql, so I need to brush up on my Python",Kafka,107,112
24bc2cbf7734f8d88488ce4557cd7f37,pqty5b,scraper01,https://www.reddit.com/r/dataengineering/comments/pqty5b/stream_processing_tool_to_pair_with_kafka/,Stream processing tool to pair with Kafka,APACHE_KAFKA,data ingestion,"So far i've learnt Spark for big data ETL, a bit of Big Query for ELT, Apache Airflow for small data ETL,  and at last Kafka as a message passing queue",Kafka,120,125
6c81a358cba4c9d9f42cd0cc2159330d,pqty5b,scraper01,https://www.reddit.com/r/dataengineering/comments/pqty5b/stream_processing_tool_to_pair_with_kafka/,Stream processing tool to pair with Kafka,APACHE_KAFKA,data ingestion,"Between spark streaming, apache flink and apache storm, what do you think sinergizes the best with Kafka",Kafka,100,105
c85b90fcb22d6a157bc1f106936bcaea,ppuh5c,hatchikyu,https://www.reddit.com/r/dataengineering/comments/ppuh5c/if_i_had_to_explain_data_engineering_work_to_a/,If I had to explain data engineering work to a business colleague...,APACHE_KAFKA,data ingestion,"* Data architecture - distributed databases, traceability, data model definition * Business domain knowledge - know-how of industry working within, communicate findings with the audience at their level and intent of understanding  **10 years ago vs now**  * Ten years ago, data engineering was data warehousing, business intelligence and ETL * There was limited development of pipelines for analytical models * Very fast space now with a lot of directions you can go within the same org * Throwing myself in as an example, I'm now going into my 8th year in data, but still lack knowledge in K8s, Kafka beyond the surface level, massive-scale migration projects  **End up working in orgs with varying levels of DE maturity**  1",Kafka,597,602
910bc175b7b9b7fd89f8ed8bcebec134,pny53c,sgtbrecht,https://www.reddit.com/r/dataengineering/comments/pny53c/an_alternative_to_aws_solutions_architect_course/,An alternative to AWS Solutions Architect course on Udemy?,APACHE_KAFKA,data ingestion,"I still want to learn stuff like Airflow, Kafka, advanced SQL, advanced DWH...etc but I'm not sure what to focus on at this point",Kafka,43,48
53d993abd99b8e214ba23b2d18478708,pnlgxj,Away-Suggestion-5845,https://www.reddit.com/r/dataengineering/comments/pnlgxj/importing_data_question_kafka_vs_dwh/,Importing data question Kafka vs DWH,APACHE_KAFKA,data ingestion,"Hi everyone, someone asked me a technical question about Kafka",Kafka,58,63
99e7389506812e1d63455b4d348311bc,pnlgxj,Away-Suggestion-5845,https://www.reddit.com/r/dataengineering/comments/pnlgxj/importing_data_question_kafka_vs_dwh/,Importing data question Kafka vs DWH,APACHE_KAFKA,data ingestion,   There is a platform which uses relational Database and also Kafka to communicate and receive order data(and updates,Kafka,64,69
d1d4832bd0e3ebd641a29eaf186e25b5,pnlgxj,Away-Suggestion-5845,https://www.reddit.com/r/dataengineering/comments/pnlgxj/importing_data_question_kafka_vs_dwh/,Importing data question Kafka vs DWH,APACHE_KAFKA,data ingestion,Which of these method is a better approach:  An incremental load from the DB or importing data via Kafka into the DWH,Kafka,100,105
08b5b6d942717f0d12e59a95b1d1c3bf,pnlgxj,Away-Suggestion-5845,https://www.reddit.com/r/dataengineering/comments/pnlgxj/importing_data_question_kafka_vs_dwh/,Importing data question Kafka vs DWH,APACHE_KAFKA,data ingestion,  As i don't have any technical basics about Kafka i couldn't understand the question and respond it,Kafka,46,51
43c63e1a509da998aec0cd715f8a7388,pli0og,MediumZealousideal29,https://www.reddit.com/r/dataengineering/comments/pli0og/skills_required/,Skills required,APACHE_KAFKA,data ingestion,"I know SQL, hdfs, hive, control M..I also learned airflow, sqoop and basics of spark streaming, Kafka and AWS",Kafka,97,102
a9cf1cfcb0f35662f7e117fb89647594,pl5blv,putinwhat,https://www.reddit.com/r/dataengineering/comments/pl5blv/ingesting_data_into_delta_lake/,Ingesting data into Delta Lake,APACHE_KAFKA,data ingestion,The data is a mix of batch and streaming at the moment coming from Kafka.,Kafka,68,73
be09ee814f37aa456294258f2c84657b,pj6hoa,Away-Suggestion-5845,https://www.reddit.com/r/dataengineering/comments/pj6hoa/a_case_study_example_for_an_interview/,A Case Study Example for an Interview,APACHE_KAFKA,data ingestion,"  **Task 2:** Besides storing all data in a relational database, the event platform already uses Kafka to communicate updates between its different subsystems, opening up a completely new way of getting order infos for the DWH",Kafka,98,103
8d599b21cbbc889494257c2034e1b895,pj6hoa,Away-Suggestion-5845,https://www.reddit.com/r/dataengineering/comments/pj6hoa/a_case_study_example_for_an_interview/,A Case Study Example for an Interview,APACHE_KAFKA,data ingestion,or would you prefer importing order data via Kafka into the DWH,Kafka,46,51
4a687a387185ec4b022d17132731b597,ph15au,SukalpVashishtha,https://www.reddit.com/r/dataengineering/comments/ph15au/microservice_shared_database_antipattern_question/,Microservice shared database antipattern question,APACHE_KAFKA,data ingestion,  Is it a fair design pattern that a stream processing framework (spark/kafka streams etc,kafka,73,78
11a6679fcfebc58c6011295e19766570,ph0uwb,chungmaster,https://www.reddit.com/r/dataengineering/comments/ph0uwb/keeping_track_of_a_large_number_of_kakfa_topics/,Keeping track of a large number of kakfa topics and how they are connected,APACHE_KAFKA,data ingestion,Totally new to this sphere but I just joined a pretty large org that has a pretty complicated web of services connected via a lot of kafka topics,kafka,134,139
8abf76679e17e7975387193252eb618c,pgq50c,king_booker,https://www.reddit.com/r/dataengineering/comments/pgq50c/processing_different_file_format_data/,processing different file format data,APACHE_KAFKA,data ingestion," It places the files in an hdfs location, which kafka picks up and does some processing over it and pushes it into hdfs in an RC file format",kafka,49,54
7bd0f6bee67f99f974ee6bf4fa31371e,pgq50c,king_booker,https://www.reddit.com/r/dataengineering/comments/pgq50c/processing_different_file_format_data/,processing different file format data,APACHE_KAFKA,data ingestion,"Is it better to read the json into a kafka topic, push it to hdfs as a json, create a hive external table with json serde and then convert it to parquet in other staging tables",kafka,38,43
37871f27b42f4144022c92ace419208f,peq4be,m_usamahameed,https://www.reddit.com/r/dataengineering/comments/peq4be/kafka_stream/,KAFKA STREAM,APACHE_KAFKA,data ingestion,Anyone experience with KAFKA STREAM with Scala language,KAFKA,24,29
1986b428c5dad9e6b1d02d3b32ea9313,peq4be,m_usamahameed,https://www.reddit.com/r/dataengineering/comments/peq4be/kafka_stream/,KAFKA STREAM,APACHE_KAFKA,data ingestion,"I'm a beginner in Kafka and currently, I'm taking help from Documentation but I completely lost (Due to heavy terms used in it",Kafka,19,24
e580671efa3d4d11b77377906b48f4d7,pe354d,syberman01,https://www.reddit.com/r/dataengineering/comments/pe354d/mainframe_offloading_is_db2_mirroring_a_good/,Mainframe offloading: Is DB2 mirroring a good tactic for making data available for modern apps?,APACHE_KAFKA,data ingestion," # Once a mirrored DB2-linux is available, many things like kafka streaming, or other CDC tools can be applied without any mainframe-politics",kafka,61,66
4fc6b73a085270f6e00b4e2bcd90992f,pdni20,sedition8,https://www.reddit.com/r/dataengineering/comments/pdni20/spark_design_patterns/,Spark Design Patterns,APACHE_KAFKA,data ingestion,Spark receives data from Kafka queues 2,Kafka,26,31
1090b066d31e812e00443a2e28893c9f,pcrlzi,Professional_Crazy49,https://www.reddit.com/r/dataengineering/comments/pcrlzi/confused_about_de_vs_sde/,Confused about DE vs SDE,APACHE_KAFKA,data ingestion," The Confusion:  So one of my friends works as a software engineer 1 and he has worked with kafka, docker, aws, prometheus, spark",kafka,93,98
8a7672594cf541ac775c2a57749db1e2,pceu26,skpfm_06,https://www.reddit.com/r/dataengineering/comments/pceu26/been_working_in_a_small_startup_for_almost_2/,"Been working in a small startup for almost 2 years doing pretty much everything on the engineering side, what should I focus on and pursue moving forward?",APACHE_KAFKA,data ingestion,(Kafka for streaming,Kafka,2,7
f6988d5dba02e2e5dc1ef3d80d72066f,pfemin,Zorkol02,https://www.reddit.com/r/dataengineering/comments/pfemin/cloud_certification_help/,Cloud Certification Help,FLOR,workflow management,"First thing is that I currently live in Spain, but will leave in a couple of months to Florida so I’m not sure if the cloud platform brand varies",Flor,88,92